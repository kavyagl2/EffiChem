{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clintox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clintox Molformer LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_clin=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/clintox_train.csv')\n",
    "val_clin=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/clintox_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clin_sub=train_clin.drop(['FDA_APPROVED','smiles'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1185, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_clin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>FDA_APPROVED</th>\n",
       "      <th>CT_TOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[C@@H]1([C@@H]([C@@H]([C@H]([C@@H]([C@@H]1Cl)C...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[H]/[NH+]=C(/C1=CC(=O)/C(=C\\C=c2ccc(=C([NH3+])...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[H]/[NH+]=C(\\N)/c1ccc(cc1)OCCCCCOc2ccc(cc2)/C(...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[N+](=O)([O-])[O-]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NH4][Pt]([NH4])(Cl)Cl</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles  FDA_APPROVED  CT_TOX\n",
       "0  [C@@H]1([C@@H]([C@@H]([C@H]([C@@H]([C@@H]1Cl)C...             1       0\n",
       "1  [H]/[NH+]=C(/C1=CC(=O)/C(=C\\C=c2ccc(=C([NH3+])...             1       0\n",
       "2  [H]/[NH+]=C(\\N)/c1ccc(cc1)OCCCCCOc2ccc(cc2)/C(...             1       0\n",
       "3                                 [N+](=O)([O-])[O-]             1       0\n",
       "4                             [NH4][Pt]([NH4])(Cl)Cl             1       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_clin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "smiles          0\n",
       "FDA_APPROVED    0\n",
       "CT_TOX          0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_clin.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tokenizer and Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_clin = AutoTokenizer.from_pretrained(\n",
    "    \"ibm/MoLFormer-XL-both-10pct\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load the model with a classification head\n",
    "model_clin = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"ibm/MoLFormer-XL-both-10pct\",\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 91 positive samples (7.68% of total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_260208/3779411929.py:13: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(f\"Label {i}: {label_counts[i]} positive samples ({freq:.2%} of total)\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_counts = clin_sub.sum(axis=0)  \n",
    "total_samples = clin_sub.shape[0]    \n",
    "\n",
    "\n",
    "label_distribution = label_counts / total_samples\n",
    "\n",
    "for i, freq in enumerate(label_distribution):\n",
    "    print(f\"Label {i}: {label_counts[i]} positive samples ({freq:.2%} of total)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_clin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training and Validation Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list_clin = train_clin['smiles'].tolist()\n",
    "smiles_val_clin=val_clin['smiles'].tolist()\n",
    "train_tokenized_clin=tokenizer_clin(smiles_list_clin)\n",
    "val_tokenized_clin=tokenizer_clin(smiles_val_clin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized_clin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 1185\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset_clin = Dataset.from_dict(train_tokenized_clin)\n",
    "val_dataset_clin = Dataset.from_dict(val_tokenized_clin)\n",
    "\n",
    "train_dataset_clin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_clin = train_clin['CT_TOX'].tolist() # Assuming tasks start from column 1\n",
    "val_labels_clin = val_clin['CT_TOX'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1185\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset_clin = train_dataset_clin.add_column(\"labels\", train_labels_clin)\n",
    "val_dataset_clin = val_dataset_clin.add_column(\"labels\", val_labels_clin)\n",
    "train_dataset_clin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying LoRA Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",  # Sequence classification task\n",
    "    r=64,  # Rank of LoRA matrices\n",
    "    lora_alpha=16,  # Scaling factor double of rank( from the rule of thumb)\n",
    "    target_modules='all-linear',\n",
    "    lora_dropout=0  # Dropout rate\n",
    "    #init_lora_weights=\"gaussian\"\n",
    ")\n",
    "\n",
    "model_train = get_peft_model(model_clin, lora_config)\n",
    "\n",
    "# change the target_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,506,498 || all params: 54,310,148 || trainable%: 15.6628\n"
     ]
    }
   ],
   "source": [
    "model_train.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_clin\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1.4628449108931757e-05,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=30,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_clin\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_mcc_metric\",\n",
    "    greater_is_better=True\n",
    "    #push_to_hub=True,  # Automatically push to Hugging Face Hub\n",
    "    #hub_model_id=\"HarshaH21/LoRA_Tox21\",  # Replace with your Hub model name\n",
    "     \n",
    ")\n",
    "#schedular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Computing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef\n",
    "\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "mcc_metric= load(\"matthews_correlation\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    probabilities = softmax(logits, axis=1)[:, 1]  # Get probabilities for class 1\n",
    "    predictions = np.argmax(logits, axis=1)  # Choose the most likely class\n",
    "    \n",
    "\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        \"eval_mcc_metric\": mcc,\n",
    "        \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"AUC-ROC\": roc_auc_score(labels, probabilities),  # AUC-ROC requires probabilities\n",
    "        \"Precision\": precision_score(labels, predictions),\n",
    "        \"Recall\": recall_score(labels, predictions),\n",
    "        \"F1-score\": f1_score(labels, predictions)\n",
    "    } \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0768, 0.9232], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class_weights= [1-(train_dataset_clin['labels'].count(0)/len(train_dataset_clin['labels'])),\n",
    "                           1-(train_dataset_clin['labels'].count(1)/len(train_dataset_clin['labels']))]\n",
    "\n",
    "class_weights = torch.from_numpy(np.array(class_weights)).float().to(\"cuda\")\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLossTrainer(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Extract labels\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # compute custom loss (suppose one has 2 labels with different weights)\n",
    "        loss_func = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_func(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_328447/793747337.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer= WeightedLossTrainer(\n",
    "    model=model_train,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_clin,\n",
    "    eval_dataset=val_dataset_clin,\n",
    "    tokenizer=tokenizer_clin,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='570' max='570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [570/570 04:59, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.642303</td>\n",
       "      <td>0.262208</td>\n",
       "      <td>0.689189</td>\n",
       "      <td>0.811607</td>\n",
       "      <td>0.134615</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.583292</td>\n",
       "      <td>0.342530</td>\n",
       "      <td>0.878378</td>\n",
       "      <td>0.805357</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.541144</td>\n",
       "      <td>0.368154</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.833036</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.439541</td>\n",
       "      <td>0.331143</td>\n",
       "      <td>0.905405</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.484964</td>\n",
       "      <td>0.414774</td>\n",
       "      <td>0.912162</td>\n",
       "      <td>0.816071</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>0.456614</td>\n",
       "      <td>0.530879</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.826786</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>0.456558</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.838393</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>0.475258</td>\n",
       "      <td>0.397920</td>\n",
       "      <td>0.905405</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>0.495006</td>\n",
       "      <td>0.564337</td>\n",
       "      <td>0.952703</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.463300</td>\n",
       "      <td>0.300211</td>\n",
       "      <td>0.366870</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.915179</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>0.455448</td>\n",
       "      <td>0.530879</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.831250</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>0.442535</td>\n",
       "      <td>0.556891</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.834821</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>0.272119</td>\n",
       "      <td>0.476340</td>\n",
       "      <td>0.932432</td>\n",
       "      <td>0.939286</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>0.289995</td>\n",
       "      <td>0.348075</td>\n",
       "      <td>0.912162</td>\n",
       "      <td>0.922321</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>0.263960</td>\n",
       "      <td>0.439305</td>\n",
       "      <td>0.939189</td>\n",
       "      <td>0.938393</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.341919</td>\n",
       "      <td>0.650562</td>\n",
       "      <td>0.966216</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.495739</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.233315</td>\n",
       "      <td>0.439305</td>\n",
       "      <td>0.939189</td>\n",
       "      <td>0.958036</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.340705</td>\n",
       "      <td>0.530879</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.908036</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.563841</td>\n",
       "      <td>0.439305</td>\n",
       "      <td>0.939189</td>\n",
       "      <td>0.835714</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.294576</td>\n",
       "      <td>0.387940</td>\n",
       "      <td>0.925676</td>\n",
       "      <td>0.933036</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>0.540840</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>0.571890</td>\n",
       "      <td>0.616846</td>\n",
       "      <td>0.966216</td>\n",
       "      <td>0.849107</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>0.296362</td>\n",
       "      <td>0.439305</td>\n",
       "      <td>0.939189</td>\n",
       "      <td>0.929464</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>0.440652</td>\n",
       "      <td>0.439305</td>\n",
       "      <td>0.939189</td>\n",
       "      <td>0.863393</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.086700</td>\n",
       "      <td>0.256729</td>\n",
       "      <td>0.509797</td>\n",
       "      <td>0.952703</td>\n",
       "      <td>0.949107</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.473465</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.858036</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.301133</td>\n",
       "      <td>0.439305</td>\n",
       "      <td>0.939189</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.245291</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.956250</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.338407</td>\n",
       "      <td>0.387940</td>\n",
       "      <td>0.925676</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model('./model2_clin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Using WanDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from evaluate import load\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mharodharsha21\u001b[0m (\u001b[33mharodharsha21-iit-ropar\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load():\n",
    "    train_clin=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/clintox_train.csv')\n",
    "    val_clin=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/clintox_test.csv')\n",
    "\n",
    "    return train_clin, val_clin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data_process,tokenizer_clin):\n",
    "\n",
    "    smiles_list_clin = data_process['smiles'].tolist()\n",
    "    tokenized_clin=tokenizer_clin(smiles_list_clin)\n",
    "    \n",
    "    \n",
    "    dataset_clin = Dataset.from_dict(tokenized_clin)\n",
    "    \n",
    "\n",
    "    labels_clin = data_process['CT_TOX'].tolist() # Assuming tasks start from column 1\n",
    "    \n",
    "    dataset_clin = dataset_clin.add_column(\"labels\", labels_clin)\n",
    "    \n",
    "\n",
    "    return dataset_clin\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "def lora_config(r, lora_alpha, dropout):\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",  # Sequence classification task\n",
    "        r=r,  # Rank of LoRA matrices\n",
    "        lora_alpha=lora_alpha,  # Scaling factor double of rank( from the rule of thumb)\n",
    "        target_modules='all-linear',\n",
    "        lora_dropout=dropout  # Dropout rate\n",
    "        #init_lora_weights=\"gaussian\"\n",
    "    )\n",
    "\n",
    "    return lora_config\n",
    "\n",
    "\n",
    "#model_train = get_peft_model(model_clin, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class_weights= [1-(train_dataset_clin['labels'].count(0)/len(train_dataset_clin['labels'])),\n",
    "                           1-(train_dataset_clin['labels'].count(1)/len(train_dataset_clin['labels']))]\n",
    "\n",
    "class_weights = torch.from_numpy(np.array(class_weights)).float().to(\"cuda\")\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Extract labels\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # compute custom loss (suppose one has 2 labels with different weights)\n",
    "        loss_func = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_func(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# initialize wandb with sweep \n",
    "def run_training():\n",
    "\n",
    "    run = wandb.init(project=\"Clintox Hyperparameter Tuning\")\n",
    "    config = run.config\n",
    "    config.batch_size = 128\n",
    "    config.num_epochs = 10\n",
    "\n",
    "    tokenizer_clin = AutoTokenizer.from_pretrained(\n",
    "    \"ibm/MoLFormer-XL-both-10pct\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "#Data\n",
    "\n",
    "    train_data, val_data=data_load()\n",
    "    training_data=data_prep(train_data,tokenizer_clin)\n",
    "    validation_data=data_prep(val_data,tokenizer_clin)    \n",
    "\n",
    "    \n",
    "\n",
    "# Load the model with a classification head\n",
    "    model_clin = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"ibm/MoLFormer-XL-both-10pct\",\n",
    "        num_labels=2,\n",
    "        problem_type=\"single_label_classification\",    \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    peft_config = lora_config(config.r, config.lora_alpha, config.dropout)\n",
    "    lora_model = get_peft_model(model_clin, peft_config)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=\"./results_clin_rerunmodel\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=config.lr,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_clin\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"wandb\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_mcc_metric\"\n",
    "    )\n",
    "\n",
    "\n",
    "    accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = softmax(logits, axis=1)[:, 1]  # Get probabilities for class 1\n",
    "        predictions = np.argmax(logits, axis=1)  # Choose the most likely class\n",
    "        mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "        return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities),  # AUC-ROC requires probabilities\n",
    "            \"Precision\": precision_score(labels, predictions),\n",
    "            \"Recall\": recall_score(labels, predictions),\n",
    "            \"F1-score\": f1_score(labels, predictions)\n",
    "        }\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    trainer= WeightedLossTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_data,\n",
    "    eval_dataset=validation_data,\n",
    "    tokenizer=tokenizer_clin,\n",
    "    compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    trainer.save_model(\"./best_clintox_model_rerunmodel\")\n",
    "    print(\"Best model saved to ./best_clintox_model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rerun model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define the sweep configuration\n",
    "def main():\n",
    "\n",
    "    sweep_config = {\n",
    "    \"name\": \"Clintox Hyperparameter Tuning\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"goal\": \"maximize\",\n",
    "        \"name\": \"eval/mcc_metric\"\n",
    "        },\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\n",
    "        \"distribution\": \"uniform\",\n",
    "                \"min\": 1e-5,\n",
    "                \"max\": 2e-5\n",
    "        },\n",
    "        \"r\": {\n",
    "            \"values\": [4, 8, 16, 32,64]\n",
    "        },\n",
    "        \"lora_alpha\": {\n",
    "            \"values\": [8, 16, 32, 64,128]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"values\": [0.0, 0.1, 0.2]\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"huggingface\")\n",
    "    wandb.agent(sweep_id, function=run_training, count=10)\n",
    "\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(f\"huggingface/{sweep_id}\")\n",
    "    print(sweep.runs[0].summary_metrics)\n",
    "\n",
    "    runs_with_eval_loss = [run for run in sweep.runs if 'eval/mcc_metric' in run.summary_metrics]\n",
    "\n",
    "    if runs_with_eval_loss:\n",
    "        best_run = sorted(runs_with_eval_loss, key=lambda run: run.summary_metrics['eval/mcc_metric'],reverse=False)[0]\n",
    "    else:\n",
    "        raise ValueError(\"No runs found with 'eval/mcc_metric' metric.\")\n",
    "\n",
    "    best_hyperparameters = best_run.config\n",
    "    print(best_hyperparameters)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: morh2os6\n",
      "Sweep URL: https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vtced8ur with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.968011583193597e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250308_092846-vtced8ur</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/vtced8ur' target=\"_blank\">youthful-sweep-1</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/vtced8ur' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/vtced8ur</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_332210/217523995.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 02:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.251840</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.997041</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.419800</td>\n",
       "      <td>0.137995</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.999408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.168100</td>\n",
       "      <td>0.187971</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.998225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.205897</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.271622</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.294050</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.290475</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>0.364801</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.130100</td>\n",
       "      <td>0.352266</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.093200</td>\n",
       "      <td>0.289030</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▆█▇▄▄▄▄▁▄▄</td></tr><tr><td>eval/Accuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/F1-score</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/Precision</td><td>▁█████████</td></tr><tr><td>eval/Recall</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>▅▁▃▃▅▆▆██▆</td></tr><tr><td>eval/mcc_metric</td><td>█▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▄▁▁▃▄▄█▂█</td></tr><tr><td>eval/samples_per_second</td><td>▆▄██▆▅▄▁▇▁</td></tr><tr><td>eval/steps_per_second</td><td>▆▄██▆▅▄▁▇▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>█▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▃▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99527</td></tr><tr><td>eval/Accuracy</td><td>0.98601</td></tr><tr><td>eval/F1-score</td><td>0.91667</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.28903</td></tr><tr><td>eval/mcc_metric</td><td>0.91287</td></tr><tr><td>eval/runtime</td><td>0.5948</td></tr><tr><td>eval/samples_per_second</td><td>240.433</td></tr><tr><td>eval/steps_per_second</td><td>15.132</td></tr><tr><td>total_flos</td><td>451446156716256.0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>750</td></tr><tr><td>train/grad_norm</td><td>0.00291</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0932</td></tr><tr><td>train_loss</td><td>0.1462</td></tr><tr><td>train_runtime</td><td>132.9927</td></tr><tr><td>train_samples_per_second</td><td>89.103</td></tr><tr><td>train_steps_per_second</td><td>5.639</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">youthful-sweep-1</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/vtced8ur' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/vtced8ur</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250308_092846-vtced8ur/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to ./best_clintox_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e4mkl3pg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.7590566832365665e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250308_093111-e4mkl3pg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/e4mkl3pg' target=\"_blank\">feasible-sweep-2</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/e4mkl3pg' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/e4mkl3pg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_332210/217523995.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 01:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.292653</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.987574</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.399700</td>\n",
       "      <td>0.199045</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.182100</td>\n",
       "      <td>0.266918</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.264425</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.358661</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>0.358785</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>0.406138</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.996450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.185400</td>\n",
       "      <td>0.473956</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.987574</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.185400</td>\n",
       "      <td>0.418338</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.141000</td>\n",
       "      <td>0.427414</td>\n",
       "      <td>0.867110</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▆▆▄▅▆█▁▇▇</td></tr><tr><td>eval/Accuracy</td><td>█████████▁</td></tr><tr><td>eval/F1-score</td><td>█████████▁</td></tr><tr><td>eval/Precision</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/Recall</td><td>█████████▁</td></tr><tr><td>eval/loss</td><td>▃▁▃▃▅▅▆█▇▇</td></tr><tr><td>eval/mcc_metric</td><td>█████████▁</td></tr><tr><td>eval/runtime</td><td>▅▂▁▃▁█▂▃▄▂</td></tr><tr><td>eval/samples_per_second</td><td>▃▇█▆█▁▆▆▅▇</td></tr><tr><td>eval/steps_per_second</td><td>▃▇█▆█▁▆▆▅▇</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>█▂▁▁▃▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▂▁▁▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99467</td></tr><tr><td>eval/Accuracy</td><td>0.97902</td></tr><tr><td>eval/F1-score</td><td>0.86957</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.76923</td></tr><tr><td>eval/loss</td><td>0.42741</td></tr><tr><td>eval/mcc_metric</td><td>0.86711</td></tr><tr><td>eval/runtime</td><td>0.4139</td></tr><tr><td>eval/samples_per_second</td><td>345.505</td></tr><tr><td>eval/steps_per_second</td><td>21.745</td></tr><tr><td>total_flos</td><td>390418522567776.0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>750</td></tr><tr><td>train/grad_norm</td><td>0.00203</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.141</td></tr><tr><td>train_loss</td><td>0.1796</td></tr><tr><td>train_runtime</td><td>114.6807</td></tr><tr><td>train_samples_per_second</td><td>103.33</td></tr><tr><td>train_steps_per_second</td><td>6.54</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feasible-sweep-2</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/e4mkl3pg' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/e4mkl3pg</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250308_093111-e4mkl3pg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to ./best_clintox_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xa7l15n9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.4255936086369527e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250308_093315-xa7l15n9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/xa7l15n9' target=\"_blank\">skilled-sweep-3</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/xa7l15n9' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/xa7l15n9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_332210/217523995.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 01:46, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.400473</td>\n",
       "      <td>0.781627</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.966272</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.472100</td>\n",
       "      <td>0.252105</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>0.181187</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.183903</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.175324</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.247986</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.130800</td>\n",
       "      <td>0.303584</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.989349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.159700</td>\n",
       "      <td>0.250188</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.159700</td>\n",
       "      <td>0.269858</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.274461</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅█▇██▇▇▇█</td></tr><tr><td>eval/Accuracy</td><td>▁█████████</td></tr><tr><td>eval/F1-score</td><td>▁█████████</td></tr><tr><td>eval/Precision</td><td>▁█████████</td></tr><tr><td>eval/Recall</td><td>▁█████████</td></tr><tr><td>eval/loss</td><td>█▃▁▁▁▃▅▃▄▄</td></tr><tr><td>eval/mcc_metric</td><td>▁█████████</td></tr><tr><td>eval/runtime</td><td>▁▂▁▂▁▁▂█▂▆</td></tr><tr><td>eval/samples_per_second</td><td>█▇█▆██▇▁▇▃</td></tr><tr><td>eval/steps_per_second</td><td>█▇█▆██▇▁▇▃</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>█▄▃▁▂▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▃▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99527</td></tr><tr><td>eval/Accuracy</td><td>0.98601</td></tr><tr><td>eval/F1-score</td><td>0.91667</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.27446</td></tr><tr><td>eval/mcc_metric</td><td>0.91287</td></tr><tr><td>eval/runtime</td><td>0.534</td></tr><tr><td>eval/samples_per_second</td><td>267.797</td></tr><tr><td>eval/steps_per_second</td><td>16.854</td></tr><tr><td>total_flos</td><td>390418522567776.0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>750</td></tr><tr><td>train/grad_norm</td><td>0.01935</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.134</td></tr><tr><td>train_loss</td><td>0.1924</td></tr><tr><td>train_runtime</td><td>107.0301</td></tr><tr><td>train_samples_per_second</td><td>110.717</td></tr><tr><td>train_steps_per_second</td><td>7.007</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">skilled-sweep-3</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/xa7l15n9' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/xa7l15n9</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250308_093315-xa7l15n9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to ./best_clintox_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ffuiqzlt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.0192515990124136e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250308_093514-ffuiqzlt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/ffuiqzlt' target=\"_blank\">copper-sweep-4</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/ffuiqzlt' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/ffuiqzlt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_332210/217523995.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 01:56, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.492122</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.930070</td>\n",
       "      <td>0.945562</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.535900</td>\n",
       "      <td>0.350833</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.986391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.332200</td>\n",
       "      <td>0.261779</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.989941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>0.220750</td>\n",
       "      <td>0.867110</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.989941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>0.188001</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.992899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.152300</td>\n",
       "      <td>0.185389</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.998817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.149600</td>\n",
       "      <td>0.173991</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.205593</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.149800</td>\n",
       "      <td>0.203931</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.992899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.128500</td>\n",
       "      <td>0.174991</td>\n",
       "      <td>0.867110</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.996450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▆▇▇▇█▇▇▇█</td></tr><tr><td>eval/Accuracy</td><td>▁██▇█████▇</td></tr><tr><td>eval/F1-score</td><td>▁██▇█████▇</td></tr><tr><td>eval/Precision</td><td>▁█████████</td></tr><tr><td>eval/Recall</td><td>▁██▆█████▆</td></tr><tr><td>eval/loss</td><td>█▅▃▂▁▁▁▂▂▁</td></tr><tr><td>eval/mcc_metric</td><td>▁██▇█████▇</td></tr><tr><td>eval/runtime</td><td>▁▁▃█▃▄▁▂▂▆</td></tr><tr><td>eval/samples_per_second</td><td>██▆▁▅▅█▇▇▃</td></tr><tr><td>eval/steps_per_second</td><td>██▆▁▅▅█▇▇▃</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>█▃▇▂▃▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99645</td></tr><tr><td>eval/Accuracy</td><td>0.97902</td></tr><tr><td>eval/F1-score</td><td>0.86957</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.76923</td></tr><tr><td>eval/loss</td><td>0.17499</td></tr><tr><td>eval/mcc_metric</td><td>0.86711</td></tr><tr><td>eval/runtime</td><td>0.4745</td></tr><tr><td>eval/samples_per_second</td><td>301.372</td></tr><tr><td>eval/steps_per_second</td><td>18.967</td></tr><tr><td>total_flos</td><td>418898085170400.0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>750</td></tr><tr><td>train/grad_norm</td><td>0.14761</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1285</td></tr><tr><td>train_loss</td><td>0.2305</td></tr><tr><td>train_runtime</td><td>116.455</td></tr><tr><td>train_samples_per_second</td><td>101.756</td></tr><tr><td>train_steps_per_second</td><td>6.44</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">copper-sweep-4</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/ffuiqzlt' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/ffuiqzlt</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250308_093514-ffuiqzlt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to ./best_clintox_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dyyaz919 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.097742889794737e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250308_093724-dyyaz919</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/dyyaz919' target=\"_blank\">distinctive-sweep-5</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/dyyaz919' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/dyyaz919</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_332210/217523995.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 01:59, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.492905</td>\n",
       "      <td>0.693894</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.929586</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.532600</td>\n",
       "      <td>0.366981</td>\n",
       "      <td>0.771728</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.976331</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.340800</td>\n",
       "      <td>0.267567</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.228000</td>\n",
       "      <td>0.222787</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.228000</td>\n",
       "      <td>0.194067</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995858</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.158200</td>\n",
       "      <td>0.195580</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.163600</td>\n",
       "      <td>0.215881</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.989349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.166100</td>\n",
       "      <td>0.208225</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.166100</td>\n",
       "      <td>0.198802</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.144300</td>\n",
       "      <td>0.201015</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▆████▇███</td></tr><tr><td>eval/Accuracy</td><td>▁▄▇▇██████</td></tr><tr><td>eval/F1-score</td><td>▁▃▇▇██████</td></tr><tr><td>eval/Precision</td><td>▁▅▆▆██████</td></tr><tr><td>eval/Recall</td><td>▁▁████████</td></tr><tr><td>eval/loss</td><td>█▅▃▂▁▁▂▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▃▇▇██████</td></tr><tr><td>eval/runtime</td><td>▁▄█▆▁▁▁▁▂▁</td></tr><tr><td>eval/samples_per_second</td><td>█▄▁▃█▇██▇▇</td></tr><tr><td>eval/steps_per_second</td><td>█▄▁▃█▇██▇▇</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>█▄▇▂▃▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▅▃▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99467</td></tr><tr><td>eval/Accuracy</td><td>0.98601</td></tr><tr><td>eval/F1-score</td><td>0.91667</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.20101</td></tr><tr><td>eval/mcc_metric</td><td>0.91287</td></tr><tr><td>eval/runtime</td><td>0.4254</td></tr><tr><td>eval/samples_per_second</td><td>336.16</td></tr><tr><td>eval/steps_per_second</td><td>21.157</td></tr><tr><td>total_flos</td><td>451446156716256.0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>750</td></tr><tr><td>train/grad_norm</td><td>0.20914</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1443</td></tr><tr><td>train_loss</td><td>0.24027</td></tr><tr><td>train_runtime</td><td>119.2326</td></tr><tr><td>train_samples_per_second</td><td>99.386</td></tr><tr><td>train_steps_per_second</td><td>6.29</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">distinctive-sweep-5</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/dyyaz919' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/dyyaz919</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250308_093724-dyyaz919/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to ./best_clintox_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k62x41nm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.1405499373113077e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250308_093933-k62x41nm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/k62x41nm' target=\"_blank\">different-sweep-6</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/k62x41nm' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/k62x41nm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_332210/217523995.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 01:55, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.484266</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.930070</td>\n",
       "      <td>0.951479</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.531000</td>\n",
       "      <td>0.345565</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.988757</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.327900</td>\n",
       "      <td>0.262900</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.990533</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.219500</td>\n",
       "      <td>0.226085</td>\n",
       "      <td>0.867110</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.988757</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.219500</td>\n",
       "      <td>0.192307</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.159800</td>\n",
       "      <td>0.189840</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.998817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.159600</td>\n",
       "      <td>0.178359</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995858</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.157100</td>\n",
       "      <td>0.208101</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.157100</td>\n",
       "      <td>0.208423</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.992899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.181416</td>\n",
       "      <td>0.867110</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.997041</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▇▇▇▇██▇▇█</td></tr><tr><td>eval/Accuracy</td><td>▁▇█▇█████▇</td></tr><tr><td>eval/F1-score</td><td>▁▇█▇█████▇</td></tr><tr><td>eval/Precision</td><td>▁▆████████</td></tr><tr><td>eval/Recall</td><td>▁██▆█████▆</td></tr><tr><td>eval/loss</td><td>█▅▃▂▁▁▁▂▂▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▇█▇█████▇</td></tr><tr><td>eval/runtime</td><td>▁▃▂█▂▂▂▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▇▅▇▁▇▇▆▇██</td></tr><tr><td>eval/steps_per_second</td><td>▇▅▇▁▇▇▆▇██</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>█▃▇▃▃▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99704</td></tr><tr><td>eval/Accuracy</td><td>0.97902</td></tr><tr><td>eval/F1-score</td><td>0.86957</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.76923</td></tr><tr><td>eval/loss</td><td>0.18142</td></tr><tr><td>eval/mcc_metric</td><td>0.86711</td></tr><tr><td>eval/runtime</td><td>0.4017</td></tr><tr><td>eval/samples_per_second</td><td>356.028</td></tr><tr><td>eval/steps_per_second</td><td>22.407</td></tr><tr><td>total_flos</td><td>402624049397472.0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>750</td></tr><tr><td>train/grad_norm</td><td>0.18194</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.136</td></tr><tr><td>train_loss</td><td>0.23467</td></tr><tr><td>train_runtime</td><td>115.4567</td></tr><tr><td>train_samples_per_second</td><td>102.636</td></tr><tr><td>train_steps_per_second</td><td>6.496</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">different-sweep-6</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/k62x41nm' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/k62x41nm</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250308_093933-k62x41nm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to ./best_clintox_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: otlhp516 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.84113390415324e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250308_094143-otlhp516</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/otlhp516' target=\"_blank\">frosty-sweep-7</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/otlhp516' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/otlhp516</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_332210/217523995.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 01:59, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.365252</td>\n",
       "      <td>0.693894</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.985799</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.451200</td>\n",
       "      <td>0.210670</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.204900</td>\n",
       "      <td>0.177729</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>0.170757</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>0.185229</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.108900</td>\n",
       "      <td>0.177198</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.998817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.112800</td>\n",
       "      <td>0.183044</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.133900</td>\n",
       "      <td>0.238558</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.133900</td>\n",
       "      <td>0.233902</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.992899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.182090</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.997041</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▄▅▄▆█▆▄▅▇</td></tr><tr><td>eval/Accuracy</td><td>▁█████████</td></tr><tr><td>eval/F1-score</td><td>▁█████████</td></tr><tr><td>eval/Precision</td><td>▁█████████</td></tr><tr><td>eval/Recall</td><td>▁█████████</td></tr><tr><td>eval/loss</td><td>█▂▁▁▂▁▁▃▃▁</td></tr><tr><td>eval/mcc_metric</td><td>▁█████████</td></tr><tr><td>eval/runtime</td><td>▃▁▂█▁▁▁▂▂▂</td></tr><tr><td>eval/samples_per_second</td><td>▆█▆▁███▇▇▇</td></tr><tr><td>eval/steps_per_second</td><td>▆█▆▁███▇▇▇</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>█▂▃▂▂▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▃▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99704</td></tr><tr><td>eval/Accuracy</td><td>0.98601</td></tr><tr><td>eval/F1-score</td><td>0.91667</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.18209</td></tr><tr><td>eval/mcc_metric</td><td>0.91287</td></tr><tr><td>eval/runtime</td><td>0.4323</td></tr><tr><td>eval/samples_per_second</td><td>330.825</td></tr><tr><td>eval/steps_per_second</td><td>20.821</td></tr><tr><td>total_flos</td><td>451446156716256.0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>750</td></tr><tr><td>train/grad_norm</td><td>0.02268</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1047</td></tr><tr><td>train_loss</td><td>0.17256</td></tr><tr><td>train_runtime</td><td>119.6002</td></tr><tr><td>train_samples_per_second</td><td>99.08</td></tr><tr><td>train_steps_per_second</td><td>6.271</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-sweep-7</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/otlhp516' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/otlhp516</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250308_094143-otlhp516/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to ./best_clintox_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zbhs9ags with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.77773179415408e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250308_094353-zbhs9ags</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/zbhs9ags' target=\"_blank\">splendid-sweep-8</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/zbhs9ags' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/zbhs9ags</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_332210/217523995.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 02:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.196581</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.332600</td>\n",
       "      <td>0.220922</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.161200</td>\n",
       "      <td>0.301411</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.404559</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.987574</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.459982</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.985799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.388149</td>\n",
       "      <td>0.867110</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.989941</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.074600</td>\n",
       "      <td>0.425321</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.987574</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.124200</td>\n",
       "      <td>0.506492</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.980473</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.124200</td>\n",
       "      <td>0.491953</td>\n",
       "      <td>0.867110</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.989349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.093700</td>\n",
       "      <td>0.413414</td>\n",
       "      <td>0.867110</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▇▆▇▅▄▆▅▁▆█</td></tr><tr><td>eval/Accuracy</td><td>█████▁██▁▁</td></tr><tr><td>eval/F1-score</td><td>█████▁██▁▁</td></tr><tr><td>eval/Precision</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/Recall</td><td>█████▁██▁▁</td></tr><tr><td>eval/loss</td><td>▁▂▃▆▇▅▆██▆</td></tr><tr><td>eval/mcc_metric</td><td>█████▁██▁▁</td></tr><tr><td>eval/runtime</td><td>▁█▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>█▁███▇████</td></tr><tr><td>eval/steps_per_second</td><td>█▁███▇████</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>█▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▃▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99408</td></tr><tr><td>eval/Accuracy</td><td>0.97902</td></tr><tr><td>eval/F1-score</td><td>0.86957</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.76923</td></tr><tr><td>eval/loss</td><td>0.41341</td></tr><tr><td>eval/mcc_metric</td><td>0.86711</td></tr><tr><td>eval/runtime</td><td>0.4176</td></tr><tr><td>eval/samples_per_second</td><td>342.451</td></tr><tr><td>eval/steps_per_second</td><td>21.553</td></tr><tr><td>total_flos</td><td>451446156716256.0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>750</td></tr><tr><td>train/grad_norm</td><td>0.00097</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0937</td></tr><tr><td>train_loss</td><td>0.13078</td></tr><tr><td>train_runtime</td><td>174.7028</td></tr><tr><td>train_samples_per_second</td><td>67.829</td></tr><tr><td>train_steps_per_second</td><td>4.293</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">splendid-sweep-8</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/zbhs9ags' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/zbhs9ags</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250308_094353-zbhs9ags/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to ./best_clintox_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cok8ufd6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.688687471220676e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250308_094656-cok8ufd6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/cok8ufd6' target=\"_blank\">morning-sweep-9</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/cok8ufd6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/cok8ufd6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_332210/217523995.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 01:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.389610</td>\n",
       "      <td>0.781627</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.248492</td>\n",
       "      <td>0.867110</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.988757</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.228400</td>\n",
       "      <td>0.170818</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995858</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.157725</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.154103</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.193525</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.114800</td>\n",
       "      <td>0.243710</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.138600</td>\n",
       "      <td>0.214898</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.990533</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.138600</td>\n",
       "      <td>0.217667</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.116000</td>\n",
       "      <td>0.227220</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▆█▇██▇▇▇█</td></tr><tr><td>eval/Accuracy</td><td>▁▆████████</td></tr><tr><td>eval/F1-score</td><td>▁▅████████</td></tr><tr><td>eval/Precision</td><td>▁█████████</td></tr><tr><td>eval/Recall</td><td>▁▁████████</td></tr><tr><td>eval/loss</td><td>█▄▁▁▁▂▄▃▃▃</td></tr><tr><td>eval/mcc_metric</td><td>▁▆████████</td></tr><tr><td>eval/runtime</td><td>▃▄▁▁█▁▂▃▃▄</td></tr><tr><td>eval/samples_per_second</td><td>▅▅▇█▁█▆▆▆▄</td></tr><tr><td>eval/steps_per_second</td><td>▅▅▇█▁█▆▆▆▄</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>█▃▃▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▃▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99408</td></tr><tr><td>eval/Accuracy</td><td>0.98601</td></tr><tr><td>eval/F1-score</td><td>0.91667</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.22722</td></tr><tr><td>eval/mcc_metric</td><td>0.91287</td></tr><tr><td>eval/runtime</td><td>0.4583</td></tr><tr><td>eval/samples_per_second</td><td>311.989</td></tr><tr><td>eval/steps_per_second</td><td>19.636</td></tr><tr><td>total_flos</td><td>451446156716256.0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>750</td></tr><tr><td>train/grad_norm</td><td>0.02125</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.116</td></tr><tr><td>train_loss</td><td>0.18083</td></tr><tr><td>train_runtime</td><td>115.0015</td></tr><tr><td>train_samples_per_second</td><td>103.042</td></tr><tr><td>train_steps_per_second</td><td>6.522</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">morning-sweep-9</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/cok8ufd6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/cok8ufd6</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250308_094656-cok8ufd6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to ./best_clintox_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j5clalsu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.6341311978149303e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250308_094906-j5clalsu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/j5clalsu' target=\"_blank\">revived-sweep-10</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/morh2os6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/j5clalsu' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/j5clalsu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_332210/217523995.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 01:57, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.312351</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.987574</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.414200</td>\n",
       "      <td>0.191448</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.183100</td>\n",
       "      <td>0.253990</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.259125</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.138200</td>\n",
       "      <td>0.342877</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>0.343051</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.129400</td>\n",
       "      <td>0.383682</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995858</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.185300</td>\n",
       "      <td>0.454561</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.987574</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.185300</td>\n",
       "      <td>0.398464</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.409227</td>\n",
       "      <td>0.867110</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▆▆▄▄▇█▁▇▇</td></tr><tr><td>eval/Accuracy</td><td>▁████████▄</td></tr><tr><td>eval/F1-score</td><td>▁████████▃</td></tr><tr><td>eval/Precision</td><td>▁█████████</td></tr><tr><td>eval/Recall</td><td>█████████▁</td></tr><tr><td>eval/loss</td><td>▄▁▃▃▅▅▆█▇▇</td></tr><tr><td>eval/mcc_metric</td><td>▁████████▄</td></tr><tr><td>eval/runtime</td><td>▁▁▂▃▁▁█▂▃▆</td></tr><tr><td>eval/samples_per_second</td><td>██▇▆██▁▆▅▃</td></tr><tr><td>eval/steps_per_second</td><td>██▇▆██▁▆▅▃</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>█▃▂▁▄▁▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▂▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99467</td></tr><tr><td>eval/Accuracy</td><td>0.97902</td></tr><tr><td>eval/F1-score</td><td>0.86957</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.76923</td></tr><tr><td>eval/loss</td><td>0.40923</td></tr><tr><td>eval/mcc_metric</td><td>0.86711</td></tr><tr><td>eval/runtime</td><td>0.5802</td></tr><tr><td>eval/samples_per_second</td><td>246.457</td></tr><tr><td>eval/steps_per_second</td><td>15.511</td></tr><tr><td>total_flos</td><td>390418522567776.0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>750</td></tr><tr><td>train/grad_norm</td><td>0.00274</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1425</td></tr><tr><td>train_loss</td><td>0.18314</td></tr><tr><td>train_runtime</td><td>117.4292</td></tr><tr><td>train_samples_per_second</td><td>100.912</td></tr><tr><td>train_steps_per_second</td><td>6.387</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">revived-sweep-10</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/j5clalsu' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/j5clalsu</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250308_094906-j5clalsu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to ./best_clintox_model\n",
      "{'_runtime': 122.387829053, '_step': 17, '_timestamp': 1741423869.0129378, '_wandb': {'runtime': 122}, 'eval/AUC-ROC': 0.9946745562130178, 'eval/Accuracy': 0.9790209790209792, 'eval/F1-score': 0.8695652173913043, 'eval/Precision': 1, 'eval/Recall': 0.7692307692307693, 'eval/loss': 0.4092266261577606, 'eval/mcc_metric': 0.86710996952412, 'eval/runtime': 0.5802, 'eval/samples_per_second': 246.457, 'eval/steps_per_second': 15.511, 'total_flos': 390418522567776, 'train/epoch': 10, 'train/global_step': 750, 'train/grad_norm': 0.0027433237992227077, 'train/learning_rate': 1.0894207985432869e-06, 'train/loss': 0.1425, 'train_loss': 0.18313566970825196, 'train_runtime': 117.4292, 'train_samples_per_second': 100.912, 'train_steps_per_second': 6.387}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No runs found with 'eval_mcc_metric' metric.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[22], line 47\u001b[0m\n",
      "\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;32m     46\u001b[0m    os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m---> 47\u001b[0m    \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[0;32mIn[22], line 40\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     38\u001b[0m     best_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(runs_with_eval_loss, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m run: run\u001b[38;5;241m.\u001b[39msummary_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_mcc_metric\u001b[39m\u001b[38;5;124m'\u001b[39m],reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo runs found with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_mcc_metric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m metric.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m     42\u001b[0m best_hyperparameters \u001b[38;5;241m=\u001b[39m best_run\u001b[38;5;241m.\u001b[39mconfig\n",
      "\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_hyperparameters)\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: No runs found with 'eval_mcc_metric' metric."
     ]
    }
   ],
   "source": [
    " # Define the sweep configuration\n",
    "def main():\n",
    "\n",
    "    sweep_config = {\n",
    "    \"name\": \"Clintox Hyperparameter Tuning\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"goal\": \"maximize\",\n",
    "        \"name\": \"eval_mcc_metric\"\n",
    "        },\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\n",
    "        \"distribution\": \"uniform\",\n",
    "                \"min\": 1e-5,\n",
    "                \"max\": 2e-5\n",
    "        },\n",
    "        \"r\": {\n",
    "            \"values\": [4, 8, 16, 32,64]\n",
    "        },\n",
    "        \"lora_alpha\": {\n",
    "            \"values\": [8, 16, 32, 64,128]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"values\": [0.0, 0.1, 0.2]\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"huggingface\")\n",
    "    wandb.agent(sweep_id, function=run_training, count=10)\n",
    "\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(f\"huggingface/{sweep_id}\")\n",
    "    print(sweep.runs[0].summary_metrics)\n",
    "\n",
    "    runs_with_eval_loss = [run for run in sweep.runs if 'eval_mcc_metric' in run.summary_metrics]\n",
    "\n",
    "    if runs_with_eval_loss:\n",
    "        best_run = sorted(runs_with_eval_loss, key=lambda run: run.summary_metrics['eval_mcc_metric'],reverse=False)[0]\n",
    "    else:\n",
    "        raise ValueError(\"No runs found with 'eval_mcc_metric' metric.\")\n",
    "\n",
    "    best_hyperparameters = best_run.config\n",
    "    print(best_hyperparameters)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model with a classification head\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"ibm/MoLFormer-XL-both-10pct\",\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True,\n",
    "    deterministic_eval=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "adapter_model = PeftModel.from_pretrained(base_model, \"/home/raghvendra2/Molformer_Finetuning/best_clintox_w__model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "adapter_model = PeftModel.from_pretrained(base_model, \"./best_clintox_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#peft_model = adapter_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data_clin=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/clintox_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_clin = AutoTokenizer.from_pretrained(\n",
    "    \"ibm/MoLFormer-XL-both-10pct\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_test_clin = test_data_clin['smiles'].tolist()\n",
    "\n",
    "test_tokenized_clin =tokenizer_clin(smiles_test_clin)\n",
    "\n",
    "test_dataset_clin = Dataset.from_dict(test_tokenized_clin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_clin = test_data_clin['CT_TOX'].tolist() # Assuming tasks start from column 1\n",
    "\n",
    "\n",
    "test_dataset_clin = test_dataset_clin.add_column(\"labels\", test_labels_clin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = softmax(logits, axis=1)[:, 1]  # Get probabilities for class 1\n",
    "        predictions = np.argmax(logits, axis=1)  # Choose the most likely class\n",
    "        mcc = matthews_corrcoef(labels, predictions)\n",
    "        \n",
    "        print(\"prob: \",probabilities)\n",
    "        print(\"Predictions:\", predictions)\n",
    "        print(\"Labels:\", labels)\n",
    "\n",
    "        return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities),  # AUC-ROC requires probabilities\n",
    "            \"Precision\": precision_score(labels, predictions),\n",
    "            \"Recall\": recall_score(labels, predictions),\n",
    "            \"F1-score\": f1_score(labels, predictions)\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_results_clintox2\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    report_to=\"none\",  # Disable logging to W&B for test\n",
    "    seed=42,  # Ensures reproducibility\n",
    "\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "\"ibm/MoLFormer-XL-both-10pct\",\n",
    "trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_540643/3462861890.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer= WeightedLossTrainer(\n",
    "    model=adapter_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset_clin,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob:  [0.1974876  0.18410206 0.04071056 0.06842607 0.05894638 0.19729373\n",
      " 0.07677974 0.04483588 0.40389663 0.03832499 0.11893113 0.1893536\n",
      " 0.06580044 0.06480221 0.03341505 0.8669206  0.17124312 0.10870148\n",
      " 0.05051511 0.08069278 0.10911671 0.14440681 0.4009542  0.06409924\n",
      " 0.05846755 0.1386373  0.03221663 0.98421544 0.09760652 0.1508786\n",
      " 0.19772144 0.05802347 0.14473975 0.1097756  0.12167212 0.0254125\n",
      " 0.3637201  0.16874947 0.1735283  0.09978637 0.16644715 0.28582984\n",
      " 0.388635   0.1016976  0.08798417 0.11798432 0.16091965 0.14002986\n",
      " 0.21268746 0.21100104 0.07887942 0.3085891  0.07098906 0.08430281\n",
      " 0.12454053 0.16771178 0.08288874 0.06965958 0.06861016 0.03258727\n",
      " 0.08207446 0.09404901 0.18429871 0.17011258 0.14170365 0.14707859\n",
      " 0.11183274 0.1188296  0.0769853  0.02821218 0.06315152 0.0673309\n",
      " 0.75138503 0.25719538 0.10843217 0.07390656 0.02522832 0.03075074\n",
      " 0.07938734 0.01532564 0.2014002  0.20899868 0.0554406  0.12642872\n",
      " 0.23929591 0.04706297 0.17177047 0.11767894 0.11745013 0.13522474\n",
      " 0.5475193  0.05543445 0.07483836 0.8570194  0.15613905 0.61811614\n",
      " 0.57932705 0.06496644 0.3026299  0.19159625 0.06120874 0.3515963\n",
      " 0.9636834  0.10353395 0.06845529 0.04569446 0.09869647 0.08030067\n",
      " 0.08122453 0.07382117 0.05419954 0.28298202 0.06900607 0.10636917\n",
      " 0.09793172 0.30044985 0.8872892  0.07636644 0.09070173 0.16965899\n",
      " 0.13039136 0.70760065 0.13714516 0.04898768 0.05870519 0.3034233\n",
      " 0.07010544 0.31264743 0.04659894 0.15969257 0.12012956 0.12209216\n",
      " 0.8732112  0.39053935 0.08281388 0.14462842 0.09980214 0.09547056\n",
      " 0.03526428 0.09946058 0.09197801 0.11149508 0.1507383 ]\n",
      "Predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Test Results for model2: {'eval_mcc_metric': 0.9128709291752768, 'eval_loss': 0.2628113627433777, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.9917159763313609, 'eval_Precision': 1.0, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 1.3055, 'eval_samples_per_second': 109.538, 'eval_steps_per_second': 2.298}\n"
     ]
    }
   ],
   "source": [
    "test_results_clin = trainer.evaluate()\n",
    "\n",
    "print(\"Test Results for model2:\", test_results_clin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob:  [0.29021806 0.35511708 0.26867062 0.48394194 0.32066518 0.41290104\n",
      " 0.2983156  0.25890774 0.57834363 0.28756273 0.35133228 0.3969228\n",
      " 0.1799421  0.41608563 0.19304872 0.80524325 0.34480733 0.28109017\n",
      " 0.13885628 0.22536659 0.25828618 0.2606503  0.42974025 0.24434689\n",
      " 0.18997893 0.24764091 0.2358779  0.96248376 0.22911075 0.26000497\n",
      " 0.31553936 0.17834651 0.332849   0.36092708 0.3036842  0.17140168\n",
      " 0.4110556  0.4459567  0.28366095 0.2572276  0.4148295  0.3456944\n",
      " 0.48130488 0.228825   0.25668377 0.4100538  0.2858609  0.27362218\n",
      " 0.3736393  0.38951382 0.25397232 0.41562295 0.21825364 0.32607055\n",
      " 0.30642816 0.21882752 0.2423174  0.20875436 0.19991313 0.12312841\n",
      " 0.21218301 0.25125518 0.2918531  0.28857717 0.37183434 0.23276204\n",
      " 0.25722945 0.20545036 0.19823198 0.12558277 0.20493305 0.21120866\n",
      " 0.69580644 0.38969347 0.31196836 0.21623373 0.14387546 0.18100762\n",
      " 0.22032158 0.15885828 0.33385623 0.31971753 0.17065942 0.18428741\n",
      " 0.42039642 0.16820471 0.3765629  0.29135376 0.33355084 0.30624187\n",
      " 0.56308496 0.1762217  0.29320678 0.7653114  0.3610442  0.7123204\n",
      " 0.63015586 0.2576416  0.51083004 0.4035495  0.22172156 0.4743605\n",
      " 0.94170064 0.26056904 0.21495062 0.23412782 0.28819638 0.22875476\n",
      " 0.25777423 0.25416285 0.18032815 0.37438104 0.20220943 0.29316264\n",
      " 0.17108682 0.4586902  0.87564635 0.22716664 0.22194245 0.3658342\n",
      " 0.2804949  0.7303649  0.1914698  0.20750561 0.22631456 0.57996124\n",
      " 0.19918925 0.5023268  0.2681465  0.32987615 0.2682084  0.35113183\n",
      " 0.8572055  0.53969055 0.30731097 0.24634854 0.26158926 0.24003759\n",
      " 0.13212085 0.21376291 0.22390379 0.26135406 0.43721706]\n",
      "Predictions: [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      "Labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "test_results_clin = trainer.evaluate()\n",
    "\n",
    "print(\"Test Results for model wandb:\", test_results_clin)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
