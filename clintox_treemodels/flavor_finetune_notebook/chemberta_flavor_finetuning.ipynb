{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Chemberta for FLAVOR Dataset Property Prediction Task using LoRA Technique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "import evaluate\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flav=pd.read_csv('./fart_train.csv')\n",
    "train_flav.reset_index(drop=True, inplace=True)\n",
    "val_flav=pd.read_csv('./fart_val.csv')\n",
    "val_flav.reset_index(drop=True, inplace=True)\n",
    "test_flav=pd.read_csv('./fart_test.csv')\n",
    "test_flav.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flav.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "val_flav.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "test_flav.drop('Unnamed: 0',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chemberta Model Sited in FART Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/SMILES_tokenized_PubChem_shard00_160k and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"seyonec/SMILES_tokenized_PubChem_shard00_160k\"  \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=5,\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Chemberta Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 77M MLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-77M-MLM\",\n",
    "    num_labels=5,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 10M MLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-10M-MLM\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-10M-MLM\",\n",
    "    num_labels=5,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 10M MTR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-10M-MTR\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-10M-MTR\",\n",
    "    num_labels=5,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 77M MTR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-77M-MTR\",\n",
    "    num_labels=5,\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5M MTR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## Model 5M MTR Model\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-5M-MTR\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-5M-MTR\",\n",
    "    num_labels=5,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_flav)\n",
    "val_dataset = Dataset.from_pandas(val_flav)\n",
    "test_dataset = Dataset.from_pandas(test_flav)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10517/10517 [00:03<00:00, 3401.16 examples/s]\n",
      "Map: 100%|██████████| 2254/2254 [00:00<00:00, 3880.74 examples/s]\n",
      "Map: 100%|██████████| 2254/2254 [00:00<00:00, 3882.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "\n",
    "    return tokenizer(examples[\"Canonicalized SMILES\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoded_labels = label_encoder.fit_transform(train_dataset['Canonicalized Taste'])\n",
    "\n",
    "train_dataset = train_dataset.add_column('label', encoded_labels)\n",
    "\n",
    "encoded_labels = label_encoder.fit_transform(val_dataset['Canonicalized Taste'])\n",
    "\n",
    "val_dataset = val_dataset.add_column('label', encoded_labels)\n",
    "\n",
    "encoded_labels = label_encoder.fit_transform(test_dataset['Canonicalized Taste'])\n",
    "\n",
    "test_dataset = test_dataset.add_column('label', encoded_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\"Canonicalized SMILES\", \"Standardized SMILES\", \n",
    "                     \"Canonicalized Taste\", \"Original Labels\", \"Source\", \"is_multiclass\"]\n",
    "\n",
    "# Remove columns from all datasets\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "val_dataset = val_dataset.remove_columns(columns_to_remove)\n",
    "test_dataset = test_dataset.remove_columns(columns_to_remove) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    probabilities= softmax(logits, axis=1)\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "        \n",
    "    return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities,multi_class=\"ovr\"),  # AUC-ROC requires probabilities\n",
    "            \"Precision\": precision_score(labels, predictions,average=\"macro\"),\n",
    "            \"Recall\": recall_score(labels, predictions,average=\"macro\"),\n",
    "            \"F1-score\": f1_score(labels, predictions,average=\"macro\")\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",  # Sequence classification task\n",
    "    r=8,  # Rank of LoRA matrices\n",
    "    lora_alpha=16,  # Scaling factor double of rank( from the rule of thumb)\n",
    "    target_modules='all-linear',\n",
    "    lora_dropout=0.1,  \n",
    "    #init_lora_weights=\"gaussian\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Custom Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#focal loss computation\n",
    "\n",
    "def focal_loss_multiclass(inputs, targets, alpha=1, gamma=2):\n",
    "    log_prob = F.log_softmax(inputs, dim=-1)\n",
    "    prob = torch.exp(log_prob)  # Convert log probabilities back to normal probabilities\n",
    "\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[-1])\n",
    "    pt = torch.sum(prob * targets_one_hot, dim=-1)  # Get probability of the true class\n",
    "\n",
    "    focal_loss = -alpha * (1 - pt) ** gamma * torch.sum(log_prob * targets_one_hot, dim=-1)\n",
    "    \n",
    "    return focal_loss.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = focal_loss_multiclass(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_flavor_chemberta_100m_mtr_lora\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"steps\",\n",
    "    logging_dir=\"./logs_flavor_chemberta\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_total_limit=10,\n",
    "    remove_unused_columns=False,\n",
    "    metric_for_best_model='eval_mcc_metric',\n",
    "    greater_is_better=True,  \n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "         \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning using WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load():\n",
    "    train_flav=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/fart_train.csv')\n",
    "    val_flav=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/fart_val.csv')\n",
    "    train_flav.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "    val_flav.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "\n",
    "    return train_flav, val_flav  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data_process):\n",
    "\n",
    "    dataset = Dataset.from_pandas(data_process)\n",
    "       \n",
    "\n",
    "    return dataset\n",
    "\n",
    "def tokenize_function(examples,tokenizer):\n",
    "\n",
    "    return tokenizer(examples[\"Canonicalized SMILES\"], padding=\"max_length\", truncation=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(dataset):\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    encoded_labels = label_encoder.fit_transform(dataset['Canonicalized Taste'])\n",
    "\n",
    "    dataset = dataset.add_column('label', encoded_labels)\n",
    "    \n",
    "    columns_to_remove = [\"Canonicalized SMILES\", \"Standardized SMILES\", \n",
    "                     \"Canonicalized Taste\", \"Original Labels\", \"Source\", \"is_multiclass\"]\n",
    "\n",
    "\n",
    "    dataset = dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def lora_config(r, lora_alpha, dropout):\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",  # Sequence classification task\n",
    "        r=r,  # Rank of LoRA matrices\n",
    "        lora_alpha=lora_alpha,  # Scaling factor double of rank( from the rule of thumb)\n",
    "        target_modules='all-linear',\n",
    "        lora_dropout=dropout  # Dropout rate\n",
    "        #init_lora_weights=\"gaussian\"\n",
    "    )\n",
    "\n",
    "    return lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def focal_loss_multiclass(inputs, targets, alpha=1, gamma=2):\n",
    "    log_prob = F.log_softmax(inputs, dim=-1)\n",
    "    prob = torch.exp(log_prob)  # Convert log probabilities back to normal probabilities\n",
    "\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[-1])\n",
    "    pt = torch.sum(prob * targets_one_hot, dim=-1)  # Get probability of the true class\n",
    "\n",
    "    focal_loss = -alpha * (1 - pt) ** gamma * torch.sum(log_prob * targets_one_hot, dim=-1)\n",
    "    \n",
    "    return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = focal_loss_multiclass(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100M MTR Model hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_training():\n",
    "\n",
    "    run = wandb.init(project=\"flavor analysis chemberta Hyperparameter Tuning\")\n",
    "    config = run.config\n",
    "\n",
    "\n",
    "    save_dir = f\"./models_Mlm_10_Chemberta/{wandb.run.id}\"  # Unique directory for each run\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "   \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-10M-MLM\")\n",
    "\n",
    "\n",
    "    train_data, val_data=data_load()\n",
    "    training_data=data_prep(train_data)\n",
    "    validation_data=data_prep(val_data)    \n",
    "    training_data=training_data.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "    validation_data=validation_data.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "\n",
    "\n",
    "    training_data=label_encoding(training_data)\n",
    "    validation_data=label_encoding(validation_data)\n",
    "\n",
    "\n",
    "    # Load the model with a classification head\n",
    "\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-10M-MLM\",                                 # Define any other model here\n",
    "    num_labels=5,\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    peft_config = lora_config(config.r, config.lora_alpha, config.dropout)\n",
    "    lora_model = get_peft_model(model, peft_config)\n",
    "    lora_model.print_trainable_parameters()\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=save_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=config.lr,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"steps\",\n",
    "    logging_dir=\"./logs_flavor_chem_wandb\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_mcc_metric\",\n",
    "    greater_is_better=True,\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "\n",
    "        logits, labels = eval_pred\n",
    "\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        probabilities= softmax(logits, axis=1)\n",
    "        mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "            \n",
    "        return {\n",
    "                \"eval_mcc_metric\": mcc,\n",
    "                \"Accuracy\": metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "                \"AUC-ROC\": roc_auc_score(labels, probabilities,multi_class=\"ovr\"),  # AUC-ROC requires probabilities\n",
    "                \"Precision\": precision_score(labels, predictions,average=\"macro\"),\n",
    "                \"Recall\": recall_score(labels, predictions,average=\"macro\"),\n",
    "                \"F1-score\": f1_score(labels, predictions,average=\"macro\")\n",
    "            }\n",
    "\n",
    "\n",
    "    trainer_flavor = CustomTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_data,\n",
    "    eval_dataset= validation_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "\n",
    "    trainer_flavor.train()\n",
    "    trainer_flavor.save_model(save_dir)\n",
    "        \n",
    "    print(f\"Model saved to {save_dir}\")\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To check best params for more number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_flavor.train()\n",
    "trainer_flavor.save_model('./manual_config_chemberta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: p5bx31ga\n",
      "Sweep URL: https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oggmqthu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0017052657096512777\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mharodharsha21\u001b[0m (\u001b[33mharodharsha21-iit-ropar\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'flavor analysis chemberta Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250325_111916-oggmqthu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/oggmqthu' target=\"_blank\">vibrant-sweep-1</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/oggmqthu' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/oggmqthu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10517/10517 [00:02<00:00, 3773.50 examples/s]\n",
      "Map: 100%|██████████| 2254/2254 [00:00<00:00, 3939.05 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,128,773 || all params: 5,706,234 || trainable%: 37.3061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2954178/3798197364.py:78: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_flavor = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1645' max='1645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1645/1645 04:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.372100</td>\n",
       "      <td>0.223117</td>\n",
       "      <td>0.689075</td>\n",
       "      <td>0.842059</td>\n",
       "      <td>0.926388</td>\n",
       "      <td>0.631765</td>\n",
       "      <td>0.583832</td>\n",
       "      <td>0.594180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.213882</td>\n",
       "      <td>0.710414</td>\n",
       "      <td>0.848713</td>\n",
       "      <td>0.935786</td>\n",
       "      <td>0.617623</td>\n",
       "      <td>0.610957</td>\n",
       "      <td>0.607410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.302000</td>\n",
       "      <td>0.200733</td>\n",
       "      <td>0.735789</td>\n",
       "      <td>0.856256</td>\n",
       "      <td>0.945150</td>\n",
       "      <td>0.689243</td>\n",
       "      <td>0.776865</td>\n",
       "      <td>0.723069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.266200</td>\n",
       "      <td>0.191977</td>\n",
       "      <td>0.732917</td>\n",
       "      <td>0.858474</td>\n",
       "      <td>0.957703</td>\n",
       "      <td>0.625368</td>\n",
       "      <td>0.642933</td>\n",
       "      <td>0.633809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.190028</td>\n",
       "      <td>0.735321</td>\n",
       "      <td>0.858917</td>\n",
       "      <td>0.959356</td>\n",
       "      <td>0.829013</td>\n",
       "      <td>0.776665</td>\n",
       "      <td>0.795119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.231300</td>\n",
       "      <td>0.178072</td>\n",
       "      <td>0.751707</td>\n",
       "      <td>0.865129</td>\n",
       "      <td>0.959807</td>\n",
       "      <td>0.660544</td>\n",
       "      <td>0.788225</td>\n",
       "      <td>0.681024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>0.150595</td>\n",
       "      <td>0.764945</td>\n",
       "      <td>0.880213</td>\n",
       "      <td>0.971509</td>\n",
       "      <td>0.799686</td>\n",
       "      <td>0.776359</td>\n",
       "      <td>0.785588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>0.158628</td>\n",
       "      <td>0.752306</td>\n",
       "      <td>0.870453</td>\n",
       "      <td>0.952124</td>\n",
       "      <td>0.654271</td>\n",
       "      <td>0.647802</td>\n",
       "      <td>0.646536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.229400</td>\n",
       "      <td>0.145187</td>\n",
       "      <td>0.771493</td>\n",
       "      <td>0.881100</td>\n",
       "      <td>0.970972</td>\n",
       "      <td>0.789703</td>\n",
       "      <td>0.793413</td>\n",
       "      <td>0.791301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.210200</td>\n",
       "      <td>0.142044</td>\n",
       "      <td>0.755250</td>\n",
       "      <td>0.871340</td>\n",
       "      <td>0.972498</td>\n",
       "      <td>0.844589</td>\n",
       "      <td>0.714996</td>\n",
       "      <td>0.745710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.202700</td>\n",
       "      <td>0.141721</td>\n",
       "      <td>0.779556</td>\n",
       "      <td>0.881544</td>\n",
       "      <td>0.974914</td>\n",
       "      <td>0.805196</td>\n",
       "      <td>0.872847</td>\n",
       "      <td>0.833502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.188900</td>\n",
       "      <td>0.136521</td>\n",
       "      <td>0.770312</td>\n",
       "      <td>0.879326</td>\n",
       "      <td>0.974539</td>\n",
       "      <td>0.701238</td>\n",
       "      <td>0.790913</td>\n",
       "      <td>0.722811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>0.130897</td>\n",
       "      <td>0.774291</td>\n",
       "      <td>0.876664</td>\n",
       "      <td>0.976570</td>\n",
       "      <td>0.726272</td>\n",
       "      <td>0.868730</td>\n",
       "      <td>0.764257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>0.132783</td>\n",
       "      <td>0.784692</td>\n",
       "      <td>0.883319</td>\n",
       "      <td>0.974658</td>\n",
       "      <td>0.812137</td>\n",
       "      <td>0.878405</td>\n",
       "      <td>0.838414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>0.130704</td>\n",
       "      <td>0.782133</td>\n",
       "      <td>0.884650</td>\n",
       "      <td>0.976079</td>\n",
       "      <td>0.778853</td>\n",
       "      <td>0.872078</td>\n",
       "      <td>0.815170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.155000</td>\n",
       "      <td>0.123803</td>\n",
       "      <td>0.786924</td>\n",
       "      <td>0.888199</td>\n",
       "      <td>0.977112</td>\n",
       "      <td>0.800542</td>\n",
       "      <td>0.805494</td>\n",
       "      <td>0.802493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_MTR100_Chemberta/oggmqthu\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▂▄▅▆▆▇▅▇▇██████</td></tr><tr><td>eval/Accuracy</td><td>▁▂▃▃▄▄▇▅▇▅▇▇▆▇▇█</td></tr><tr><td>eval/F1-score</td><td>▁▁▅▂▇▃▆▃▇▅█▅▆█▇▇</td></tr><tr><td>eval/Precision</td><td>▁▁▃▁█▂▇▂▆█▇▄▄▇▆▇</td></tr><tr><td>eval/Recall</td><td>▁▂▆▂▆▆▆▃▆▄█▆███▆</td></tr><tr><td>eval/loss</td><td>█▇▆▆▆▅▃▃▃▂▂▂▁▂▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▃▄▄▄▅▆▆▇▆▇▇▇███</td></tr><tr><td>eval/runtime</td><td>▂▁▁▁▇▆█▄▂▄▄▂▁▂▂▁</td></tr><tr><td>eval/samples_per_second</td><td>▇███▂▃▁▅▇▅▅▇█▇▇█</td></tr><tr><td>eval/steps_per_second</td><td>▇███▂▃▁▅▇▅▅▇█▇▇█</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▅▇█▆▄▃▂▅▁▄▃▆▆▂▄▃</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▆▆▅▄▃▃▃▃▃▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.97711</td></tr><tr><td>eval/Accuracy</td><td>0.8882</td></tr><tr><td>eval/F1-score</td><td>0.80249</td></tr><tr><td>eval/Precision</td><td>0.80054</td></tr><tr><td>eval/Recall</td><td>0.80549</td></tr><tr><td>eval/loss</td><td>0.1238</td></tr><tr><td>eval/mcc_metric</td><td>0.78692</td></tr><tr><td>eval/runtime</td><td>4.0481</td></tr><tr><td>eval/samples_per_second</td><td>556.81</td></tr><tr><td>eval/steps_per_second</td><td>17.539</td></tr><tr><td>total_flos</td><td>852563953612800.0</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>1645</td></tr><tr><td>train/grad_norm</td><td>0.90175</td></tr><tr><td>train/learning_rate</td><td>5e-05</td></tr><tr><td>train/loss</td><td>0.155</td></tr><tr><td>train_loss</td><td>0.22824</td></tr><tr><td>train_runtime</td><td>286.2847</td></tr><tr><td>train_samples_per_second</td><td>183.681</td></tr><tr><td>train_steps_per_second</td><td>5.746</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vibrant-sweep-1</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/oggmqthu' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/oggmqthu</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250325_111916-oggmqthu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: imo6nm9x with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001090370280478251\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'flavor analysis chemberta Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250325_112421-imo6nm9x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/imo6nm9x' target=\"_blank\">drawn-sweep-2</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/imo6nm9x' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/imo6nm9x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10517/10517 [00:02<00:00, 3761.15 examples/s]\n",
      "Map: 100%|██████████| 2254/2254 [00:00<00:00, 3907.92 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,139,269 || all params: 4,642,682 || trainable%: 24.5390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2954178/3798197364.py:78: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_flavor = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1645' max='1645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1645/1645 04:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.363000</td>\n",
       "      <td>0.203302</td>\n",
       "      <td>0.698741</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.951003</td>\n",
       "      <td>0.625263</td>\n",
       "      <td>0.593799</td>\n",
       "      <td>0.607826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.276100</td>\n",
       "      <td>0.182639</td>\n",
       "      <td>0.713612</td>\n",
       "      <td>0.853150</td>\n",
       "      <td>0.961684</td>\n",
       "      <td>0.822182</td>\n",
       "      <td>0.812947</td>\n",
       "      <td>0.813223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.275700</td>\n",
       "      <td>0.178204</td>\n",
       "      <td>0.746159</td>\n",
       "      <td>0.862467</td>\n",
       "      <td>0.961817</td>\n",
       "      <td>0.823076</td>\n",
       "      <td>0.722510</td>\n",
       "      <td>0.737629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.235500</td>\n",
       "      <td>0.169735</td>\n",
       "      <td>0.715226</td>\n",
       "      <td>0.854925</td>\n",
       "      <td>0.965896</td>\n",
       "      <td>0.837466</td>\n",
       "      <td>0.748617</td>\n",
       "      <td>0.785853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.165078</td>\n",
       "      <td>0.744515</td>\n",
       "      <td>0.863798</td>\n",
       "      <td>0.963276</td>\n",
       "      <td>0.712580</td>\n",
       "      <td>0.783838</td>\n",
       "      <td>0.740727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.218200</td>\n",
       "      <td>0.153336</td>\n",
       "      <td>0.762573</td>\n",
       "      <td>0.876220</td>\n",
       "      <td>0.972265</td>\n",
       "      <td>0.733287</td>\n",
       "      <td>0.845288</td>\n",
       "      <td>0.755418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.192800</td>\n",
       "      <td>0.135687</td>\n",
       "      <td>0.760331</td>\n",
       "      <td>0.878438</td>\n",
       "      <td>0.976473</td>\n",
       "      <td>0.864175</td>\n",
       "      <td>0.770740</td>\n",
       "      <td>0.808796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.202200</td>\n",
       "      <td>0.138744</td>\n",
       "      <td>0.761127</td>\n",
       "      <td>0.873114</td>\n",
       "      <td>0.975677</td>\n",
       "      <td>0.645151</td>\n",
       "      <td>0.661335</td>\n",
       "      <td>0.651822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.208300</td>\n",
       "      <td>0.134805</td>\n",
       "      <td>0.775197</td>\n",
       "      <td>0.885093</td>\n",
       "      <td>0.973723</td>\n",
       "      <td>0.816869</td>\n",
       "      <td>0.848704</td>\n",
       "      <td>0.827289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.125999</td>\n",
       "      <td>0.775966</td>\n",
       "      <td>0.885093</td>\n",
       "      <td>0.976379</td>\n",
       "      <td>0.869190</td>\n",
       "      <td>0.720487</td>\n",
       "      <td>0.760723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.181300</td>\n",
       "      <td>0.126931</td>\n",
       "      <td>0.770076</td>\n",
       "      <td>0.876220</td>\n",
       "      <td>0.976304</td>\n",
       "      <td>0.782752</td>\n",
       "      <td>0.804218</td>\n",
       "      <td>0.791468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.179000</td>\n",
       "      <td>0.124127</td>\n",
       "      <td>0.783030</td>\n",
       "      <td>0.885980</td>\n",
       "      <td>0.977216</td>\n",
       "      <td>0.791408</td>\n",
       "      <td>0.802772</td>\n",
       "      <td>0.796907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.166900</td>\n",
       "      <td>0.118253</td>\n",
       "      <td>0.782783</td>\n",
       "      <td>0.882431</td>\n",
       "      <td>0.979657</td>\n",
       "      <td>0.788146</td>\n",
       "      <td>0.811962</td>\n",
       "      <td>0.798182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.164200</td>\n",
       "      <td>0.119020</td>\n",
       "      <td>0.787686</td>\n",
       "      <td>0.886424</td>\n",
       "      <td>0.978012</td>\n",
       "      <td>0.861936</td>\n",
       "      <td>0.815957</td>\n",
       "      <td>0.831484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>0.119716</td>\n",
       "      <td>0.789039</td>\n",
       "      <td>0.890417</td>\n",
       "      <td>0.978664</td>\n",
       "      <td>0.800160</td>\n",
       "      <td>0.803430</td>\n",
       "      <td>0.801267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>0.114601</td>\n",
       "      <td>0.790924</td>\n",
       "      <td>0.890861</td>\n",
       "      <td>0.979204</td>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.807096</td>\n",
       "      <td>0.831087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_MTR100_Chemberta/imo6nm9x\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▄▄▅▄▆▇▇▇▇▇▇████</td></tr><tr><td>eval/Accuracy</td><td>▁▂▃▂▄▆▆▅▇▇▆▇▇▇██</td></tr><tr><td>eval/F1-score</td><td>▁▇▅▇▅▆▇▂█▆▇▇▇█▇█</td></tr><tr><td>eval/Precision</td><td>▁▇▇▇▄▄█▂▆█▆▆▆█▆█</td></tr><tr><td>eval/Recall</td><td>▁▇▅▅▆█▆▃█▄▇▇▇▇▇▇</td></tr><tr><td>eval/loss</td><td>█▆▆▅▅▄▃▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▂▅▂▄▆▆▆▇▇▆▇▇███</td></tr><tr><td>eval/runtime</td><td>▁▃▆▃▃█▂▁▃▃█▃▄▁▁▂</td></tr><tr><td>eval/samples_per_second</td><td>█▆▃▆▆▁▇█▆▆▁▆▅██▇</td></tr><tr><td>eval/steps_per_second</td><td>█▆▃▆▆▁▇█▆▆▁▆▅██▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▃▂▂▂▄▁▁█▃█▂▆▄▁▃▃</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▅▅▄▄▃▃▃▃▂▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.9792</td></tr><tr><td>eval/Accuracy</td><td>0.89086</td></tr><tr><td>eval/F1-score</td><td>0.83109</td></tr><tr><td>eval/Precision</td><td>0.86863</td></tr><tr><td>eval/Recall</td><td>0.8071</td></tr><tr><td>eval/loss</td><td>0.1146</td></tr><tr><td>eval/mcc_metric</td><td>0.79092</td></tr><tr><td>eval/runtime</td><td>3.9074</td></tr><tr><td>eval/samples_per_second</td><td>576.858</td></tr><tr><td>eval/steps_per_second</td><td>18.171</td></tr><tr><td>total_flos</td><td>680756572354560.0</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>1645</td></tr><tr><td>train/grad_norm</td><td>1.07871</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>0.1433</td></tr><tr><td>train_loss</td><td>0.20915</td></tr><tr><td>train_runtime</td><td>270.6745</td></tr><tr><td>train_samples_per_second</td><td>194.274</td></tr><tr><td>train_steps_per_second</td><td>6.077</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drawn-sweep-2</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/imo6nm9x' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/imo6nm9x</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250325_112421-imo6nm9x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: glzr8jim with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0011492784634238222\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'flavor analysis chemberta Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250325_112906-glzr8jim</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/glzr8jim' target=\"_blank\">tough-sweep-3</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/glzr8jim' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/glzr8jim</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10517/10517 [00:02<00:00, 3804.38 examples/s]\n",
      "Map: 100%|██████████| 2254/2254 [00:00<00:00, 3925.03 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,128,773 || all params: 5,706,234 || trainable%: 37.3061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2954178/3798197364.py:78: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_flavor = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1645' max='1645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1645/1645 04:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.363300</td>\n",
       "      <td>0.221187</td>\n",
       "      <td>0.670252</td>\n",
       "      <td>0.832742</td>\n",
       "      <td>0.936991</td>\n",
       "      <td>0.599444</td>\n",
       "      <td>0.572938</td>\n",
       "      <td>0.582786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.189608</td>\n",
       "      <td>0.735002</td>\n",
       "      <td>0.857587</td>\n",
       "      <td>0.958517</td>\n",
       "      <td>0.813728</td>\n",
       "      <td>0.776404</td>\n",
       "      <td>0.786809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.278800</td>\n",
       "      <td>0.180736</td>\n",
       "      <td>0.753124</td>\n",
       "      <td>0.868678</td>\n",
       "      <td>0.961806</td>\n",
       "      <td>0.739694</td>\n",
       "      <td>0.787603</td>\n",
       "      <td>0.760614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.237700</td>\n",
       "      <td>0.197088</td>\n",
       "      <td>0.624040</td>\n",
       "      <td>0.817657</td>\n",
       "      <td>0.963485</td>\n",
       "      <td>0.838941</td>\n",
       "      <td>0.660370</td>\n",
       "      <td>0.726857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.244800</td>\n",
       "      <td>0.175170</td>\n",
       "      <td>0.739170</td>\n",
       "      <td>0.859805</td>\n",
       "      <td>0.961712</td>\n",
       "      <td>0.823054</td>\n",
       "      <td>0.781396</td>\n",
       "      <td>0.793482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>0.154075</td>\n",
       "      <td>0.767593</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.971343</td>\n",
       "      <td>0.721025</td>\n",
       "      <td>0.851831</td>\n",
       "      <td>0.742313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.200300</td>\n",
       "      <td>0.129148</td>\n",
       "      <td>0.764519</td>\n",
       "      <td>0.879769</td>\n",
       "      <td>0.975460</td>\n",
       "      <td>0.858464</td>\n",
       "      <td>0.839514</td>\n",
       "      <td>0.847164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.204200</td>\n",
       "      <td>0.134454</td>\n",
       "      <td>0.766344</td>\n",
       "      <td>0.876664</td>\n",
       "      <td>0.974284</td>\n",
       "      <td>0.723579</td>\n",
       "      <td>0.728323</td>\n",
       "      <td>0.723969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>0.135654</td>\n",
       "      <td>0.781343</td>\n",
       "      <td>0.887755</td>\n",
       "      <td>0.972494</td>\n",
       "      <td>0.770035</td>\n",
       "      <td>0.789416</td>\n",
       "      <td>0.775852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.196400</td>\n",
       "      <td>0.123058</td>\n",
       "      <td>0.770132</td>\n",
       "      <td>0.881988</td>\n",
       "      <td>0.976953</td>\n",
       "      <td>0.865389</td>\n",
       "      <td>0.784615</td>\n",
       "      <td>0.817121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.179500</td>\n",
       "      <td>0.127148</td>\n",
       "      <td>0.784936</td>\n",
       "      <td>0.884206</td>\n",
       "      <td>0.976059</td>\n",
       "      <td>0.794319</td>\n",
       "      <td>0.813651</td>\n",
       "      <td>0.801947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>0.123385</td>\n",
       "      <td>0.791687</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.977882</td>\n",
       "      <td>0.819293</td>\n",
       "      <td>0.870088</td>\n",
       "      <td>0.840941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.170700</td>\n",
       "      <td>0.120071</td>\n",
       "      <td>0.785735</td>\n",
       "      <td>0.883319</td>\n",
       "      <td>0.979842</td>\n",
       "      <td>0.745049</td>\n",
       "      <td>0.879995</td>\n",
       "      <td>0.786404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.164100</td>\n",
       "      <td>0.117571</td>\n",
       "      <td>0.792556</td>\n",
       "      <td>0.889086</td>\n",
       "      <td>0.979243</td>\n",
       "      <td>0.816126</td>\n",
       "      <td>0.881840</td>\n",
       "      <td>0.844336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.119243</td>\n",
       "      <td>0.785220</td>\n",
       "      <td>0.888199</td>\n",
       "      <td>0.978306</td>\n",
       "      <td>0.766749</td>\n",
       "      <td>0.867299</td>\n",
       "      <td>0.800233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.144200</td>\n",
       "      <td>0.114124</td>\n",
       "      <td>0.785427</td>\n",
       "      <td>0.888642</td>\n",
       "      <td>0.978914</td>\n",
       "      <td>0.820542</td>\n",
       "      <td>0.865312</td>\n",
       "      <td>0.838733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_MTR100_Chemberta/glzr8jim\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▅▅▅▇▇▇▇█▇█████</td></tr><tr><td>eval/Accuracy</td><td>▂▅▆▁▅▇▇▇█▇▇█▇███</td></tr><tr><td>eval/F1-score</td><td>▁▆▆▅▇▅█▅▆▇▇█▆█▇█</td></tr><tr><td>eval/Precision</td><td>▁▇▅▇▇▄█▄▅█▆▇▅▇▅▇</td></tr><tr><td>eval/Recall</td><td>▁▆▆▃▆▇▇▅▆▆▆█████</td></tr><tr><td>eval/loss</td><td>█▆▅▆▅▄▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▃▆▆▁▆▇▇▇█▇██████</td></tr><tr><td>eval/runtime</td><td>▅▂▁▅▆▃█▂▂▂▄▂▂▂▃▇</td></tr><tr><td>eval/samples_per_second</td><td>▄▇█▄▃▆▁▇▇▇▅▇▇▇▆▂</td></tr><tr><td>eval/steps_per_second</td><td>▄▇█▄▃▆▁▇▇▇▅▇▇▇▆▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▃▃▄▅▄▁▁▆▂█▃▅▄▁▄▅</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▅▅▄▄▃▃▃▃▃▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.97891</td></tr><tr><td>eval/Accuracy</td><td>0.88864</td></tr><tr><td>eval/F1-score</td><td>0.83873</td></tr><tr><td>eval/Precision</td><td>0.82054</td></tr><tr><td>eval/Recall</td><td>0.86531</td></tr><tr><td>eval/loss</td><td>0.11412</td></tr><tr><td>eval/mcc_metric</td><td>0.78543</td></tr><tr><td>eval/runtime</td><td>4.1871</td></tr><tr><td>eval/samples_per_second</td><td>538.325</td></tr><tr><td>eval/steps_per_second</td><td>16.957</td></tr><tr><td>total_flos</td><td>852563953612800.0</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>1645</td></tr><tr><td>train/grad_norm</td><td>1.0321</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>0.1442</td></tr><tr><td>train_loss</td><td>0.21174</td></tr><tr><td>train_runtime</td><td>285.8135</td></tr><tr><td>train_samples_per_second</td><td>183.984</td></tr><tr><td>train_steps_per_second</td><td>5.756</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tough-sweep-3</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/glzr8jim' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/glzr8jim</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250325_112906-glzr8jim/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y6ioij00 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0009721589973334608\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'flavor analysis chemberta Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250325_113407-y6ioij00</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/y6ioij00' target=\"_blank\">amber-sweep-4</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/y6ioij00' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/y6ioij00</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10517/10517 [00:02<00:00, 3794.35 examples/s]\n",
      "Map: 100%|██████████| 2254/2254 [00:00<00:00, 3853.10 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,128,773 || all params: 5,706,234 || trainable%: 37.3061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2954178/3798197364.py:78: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_flavor = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1645' max='1645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1645/1645 04:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.363600</td>\n",
       "      <td>0.209390</td>\n",
       "      <td>0.704460</td>\n",
       "      <td>0.846051</td>\n",
       "      <td>0.951406</td>\n",
       "      <td>0.621534</td>\n",
       "      <td>0.611995</td>\n",
       "      <td>0.609518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.276000</td>\n",
       "      <td>0.173565</td>\n",
       "      <td>0.720749</td>\n",
       "      <td>0.858030</td>\n",
       "      <td>0.963050</td>\n",
       "      <td>0.832882</td>\n",
       "      <td>0.811748</td>\n",
       "      <td>0.818070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.273700</td>\n",
       "      <td>0.183659</td>\n",
       "      <td>0.737483</td>\n",
       "      <td>0.855812</td>\n",
       "      <td>0.958301</td>\n",
       "      <td>0.813254</td>\n",
       "      <td>0.789230</td>\n",
       "      <td>0.792354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.228700</td>\n",
       "      <td>0.199301</td>\n",
       "      <td>0.673690</td>\n",
       "      <td>0.837622</td>\n",
       "      <td>0.945686</td>\n",
       "      <td>0.842906</td>\n",
       "      <td>0.707133</td>\n",
       "      <td>0.760877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.237500</td>\n",
       "      <td>0.169542</td>\n",
       "      <td>0.744847</td>\n",
       "      <td>0.862023</td>\n",
       "      <td>0.956564</td>\n",
       "      <td>0.727155</td>\n",
       "      <td>0.784071</td>\n",
       "      <td>0.751440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.157318</td>\n",
       "      <td>0.760887</td>\n",
       "      <td>0.875333</td>\n",
       "      <td>0.968162</td>\n",
       "      <td>0.736452</td>\n",
       "      <td>0.844286</td>\n",
       "      <td>0.755085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.195600</td>\n",
       "      <td>0.138943</td>\n",
       "      <td>0.760961</td>\n",
       "      <td>0.878882</td>\n",
       "      <td>0.973667</td>\n",
       "      <td>0.816203</td>\n",
       "      <td>0.835803</td>\n",
       "      <td>0.820232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.197800</td>\n",
       "      <td>0.139936</td>\n",
       "      <td>0.768099</td>\n",
       "      <td>0.877107</td>\n",
       "      <td>0.972278</td>\n",
       "      <td>0.853698</td>\n",
       "      <td>0.797060</td>\n",
       "      <td>0.816335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.203200</td>\n",
       "      <td>0.140686</td>\n",
       "      <td>0.761975</td>\n",
       "      <td>0.878882</td>\n",
       "      <td>0.970545</td>\n",
       "      <td>0.813230</td>\n",
       "      <td>0.838388</td>\n",
       "      <td>0.818749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.189800</td>\n",
       "      <td>0.128867</td>\n",
       "      <td>0.776309</td>\n",
       "      <td>0.885980</td>\n",
       "      <td>0.974617</td>\n",
       "      <td>0.871250</td>\n",
       "      <td>0.782432</td>\n",
       "      <td>0.818917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.177900</td>\n",
       "      <td>0.132896</td>\n",
       "      <td>0.771825</td>\n",
       "      <td>0.877995</td>\n",
       "      <td>0.972517</td>\n",
       "      <td>0.851078</td>\n",
       "      <td>0.870093</td>\n",
       "      <td>0.859152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.178600</td>\n",
       "      <td>0.129926</td>\n",
       "      <td>0.789543</td>\n",
       "      <td>0.890417</td>\n",
       "      <td>0.975101</td>\n",
       "      <td>0.821412</td>\n",
       "      <td>0.871896</td>\n",
       "      <td>0.842685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.165200</td>\n",
       "      <td>0.122613</td>\n",
       "      <td>0.775876</td>\n",
       "      <td>0.879326</td>\n",
       "      <td>0.976819</td>\n",
       "      <td>0.776654</td>\n",
       "      <td>0.868788</td>\n",
       "      <td>0.809146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.161700</td>\n",
       "      <td>0.121245</td>\n",
       "      <td>0.790477</td>\n",
       "      <td>0.888199</td>\n",
       "      <td>0.977211</td>\n",
       "      <td>0.866970</td>\n",
       "      <td>0.881986</td>\n",
       "      <td>0.873541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>0.122691</td>\n",
       "      <td>0.794605</td>\n",
       "      <td>0.893079</td>\n",
       "      <td>0.976950</td>\n",
       "      <td>0.820745</td>\n",
       "      <td>0.873754</td>\n",
       "      <td>0.843489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.119024</td>\n",
       "      <td>0.794261</td>\n",
       "      <td>0.893079</td>\n",
       "      <td>0.977243</td>\n",
       "      <td>0.824084</td>\n",
       "      <td>0.873141</td>\n",
       "      <td>0.844782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_MTR100_Chemberta/y6ioij00\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▂▅▄▁▃▆▇▇▇▇▇█████</td></tr><tr><td>eval/Accuracy</td><td>▂▄▃▁▄▆▆▆▆▇▆█▆▇██</td></tr><tr><td>eval/F1-score</td><td>▁▇▆▅▅▅▇▆▇▇█▇▆█▇▇</td></tr><tr><td>eval/Precision</td><td>▁▇▆▇▄▄▆█▆█▇▇▅█▇▇</td></tr><tr><td>eval/Recall</td><td>▁▆▆▃▅▇▇▆▇▅██████</td></tr><tr><td>eval/loss</td><td>█▅▆▇▅▄▃▃▃▂▂▂▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▃▄▅▁▅▆▆▆▆▇▇█▇███</td></tr><tr><td>eval/runtime</td><td>▁▂▁▁▁▃▃▂▁█▁▃▂▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>█▇███▆▆▇█▁█▆▇▇██</td></tr><tr><td>eval/steps_per_second</td><td>█▇███▆▆▇█▁█▆▇▇██</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▃▃▃▃▅▂▁▅▃█▆▅▅▁▄▃</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▅▅▄▄▃▃▃▃▂▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.97724</td></tr><tr><td>eval/Accuracy</td><td>0.89308</td></tr><tr><td>eval/F1-score</td><td>0.84478</td></tr><tr><td>eval/Precision</td><td>0.82408</td></tr><tr><td>eval/Recall</td><td>0.87314</td></tr><tr><td>eval/loss</td><td>0.11902</td></tr><tr><td>eval/mcc_metric</td><td>0.79426</td></tr><tr><td>eval/runtime</td><td>4.0543</td></tr><tr><td>eval/samples_per_second</td><td>555.955</td></tr><tr><td>eval/steps_per_second</td><td>17.512</td></tr><tr><td>total_flos</td><td>852563953612800.0</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>1645</td></tr><tr><td>train/grad_norm</td><td>0.76774</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>0.146</td></tr><tr><td>train_loss</td><td>0.20785</td></tr><tr><td>train_runtime</td><td>293.1309</td></tr><tr><td>train_samples_per_second</td><td>179.391</td></tr><tr><td>train_steps_per_second</td><td>5.612</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-sweep-4</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/y6ioij00' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/y6ioij00</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250325_113407-y6ioij00/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: to3uvb1h with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001092980471805136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'flavor analysis chemberta Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250325_113922-to3uvb1h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/to3uvb1h' target=\"_blank\">fancy-sweep-5</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/p5bx31ga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/to3uvb1h' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/to3uvb1h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10517/10517 [00:02<00:00, 3650.60 examples/s]\n",
      "Map: 100%|██████████| 2254/2254 [00:00<00:00, 3855.54 examples/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,128,773 || all params: 5,706,234 || trainable%: 37.3061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2954178/3798197364.py:78: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_flavor = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1645' max='1645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1645/1645 04:46, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.361200</td>\n",
       "      <td>0.218163</td>\n",
       "      <td>0.687245</td>\n",
       "      <td>0.839397</td>\n",
       "      <td>0.942113</td>\n",
       "      <td>0.613976</td>\n",
       "      <td>0.595243</td>\n",
       "      <td>0.601145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.277700</td>\n",
       "      <td>0.183562</td>\n",
       "      <td>0.745702</td>\n",
       "      <td>0.866460</td>\n",
       "      <td>0.959946</td>\n",
       "      <td>0.830733</td>\n",
       "      <td>0.774714</td>\n",
       "      <td>0.793639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.177634</td>\n",
       "      <td>0.746863</td>\n",
       "      <td>0.864241</td>\n",
       "      <td>0.961250</td>\n",
       "      <td>0.826519</td>\n",
       "      <td>0.719926</td>\n",
       "      <td>0.738828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.239000</td>\n",
       "      <td>0.196164</td>\n",
       "      <td>0.670627</td>\n",
       "      <td>0.834073</td>\n",
       "      <td>0.958913</td>\n",
       "      <td>0.768313</td>\n",
       "      <td>0.715214</td>\n",
       "      <td>0.735569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.239200</td>\n",
       "      <td>0.176210</td>\n",
       "      <td>0.747892</td>\n",
       "      <td>0.865572</td>\n",
       "      <td>0.956969</td>\n",
       "      <td>0.733378</td>\n",
       "      <td>0.783156</td>\n",
       "      <td>0.754735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>0.158084</td>\n",
       "      <td>0.759298</td>\n",
       "      <td>0.874445</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.744523</td>\n",
       "      <td>0.839834</td>\n",
       "      <td>0.760698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.135716</td>\n",
       "      <td>0.771171</td>\n",
       "      <td>0.882431</td>\n",
       "      <td>0.975543</td>\n",
       "      <td>0.863401</td>\n",
       "      <td>0.783400</td>\n",
       "      <td>0.815475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.199000</td>\n",
       "      <td>0.139380</td>\n",
       "      <td>0.764518</td>\n",
       "      <td>0.874889</td>\n",
       "      <td>0.974871</td>\n",
       "      <td>0.850055</td>\n",
       "      <td>0.728172</td>\n",
       "      <td>0.753449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.205800</td>\n",
       "      <td>0.132802</td>\n",
       "      <td>0.775513</td>\n",
       "      <td>0.884206</td>\n",
       "      <td>0.974390</td>\n",
       "      <td>0.782033</td>\n",
       "      <td>0.856066</td>\n",
       "      <td>0.807673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.193200</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.771549</td>\n",
       "      <td>0.883319</td>\n",
       "      <td>0.976518</td>\n",
       "      <td>0.870169</td>\n",
       "      <td>0.782056</td>\n",
       "      <td>0.817798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.182600</td>\n",
       "      <td>0.130557</td>\n",
       "      <td>0.776848</td>\n",
       "      <td>0.880213</td>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.808008</td>\n",
       "      <td>0.874205</td>\n",
       "      <td>0.835124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.176900</td>\n",
       "      <td>0.127298</td>\n",
       "      <td>0.777628</td>\n",
       "      <td>0.884650</td>\n",
       "      <td>0.975033</td>\n",
       "      <td>0.798258</td>\n",
       "      <td>0.793174</td>\n",
       "      <td>0.794922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.166600</td>\n",
       "      <td>0.119431</td>\n",
       "      <td>0.784806</td>\n",
       "      <td>0.883319</td>\n",
       "      <td>0.977675</td>\n",
       "      <td>0.781533</td>\n",
       "      <td>0.880888</td>\n",
       "      <td>0.818535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.162700</td>\n",
       "      <td>0.120988</td>\n",
       "      <td>0.783749</td>\n",
       "      <td>0.884206</td>\n",
       "      <td>0.977207</td>\n",
       "      <td>0.794465</td>\n",
       "      <td>0.811713</td>\n",
       "      <td>0.801931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.147700</td>\n",
       "      <td>0.123050</td>\n",
       "      <td>0.789866</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.977229</td>\n",
       "      <td>0.793258</td>\n",
       "      <td>0.868948</td>\n",
       "      <td>0.820605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>0.118609</td>\n",
       "      <td>0.795441</td>\n",
       "      <td>0.893523</td>\n",
       "      <td>0.977679</td>\n",
       "      <td>0.795269</td>\n",
       "      <td>0.873475</td>\n",
       "      <td>0.823797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_MTR100_Chemberta/to3uvb1h\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▅▄▄▆█▇▇█▇▇████</td></tr><tr><td>eval/Accuracy</td><td>▂▅▅▁▅▆▇▆▇▇▆▇▇▇██</td></tr><tr><td>eval/F1-score</td><td>▁▇▅▅▆▆▇▆▇▇█▇█▇██</td></tr><tr><td>eval/Precision</td><td>▁▇▇▅▄▅█▇▆█▆▆▆▆▆▆</td></tr><tr><td>eval/Recall</td><td>▁▅▄▄▆▇▆▄▇▆█▆█▆██</td></tr><tr><td>eval/loss</td><td>█▆▅▆▅▄▂▂▂▁▂▂▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▂▅▅▁▅▆▇▆▇▇▇▇▇▇██</td></tr><tr><td>eval/runtime</td><td>▂▃▂█▁▂▁▂▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▇▆▇▁█▇█▇▇▇▇█████</td></tr><tr><td>eval/steps_per_second</td><td>▇▆▇▁█▇█▇▇▇▇█████</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▄▄▄▅▇▅▁▇▄█▄▇▆▃▄▅</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▅▅▄▄▃▃▃▃▃▂▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.97768</td></tr><tr><td>eval/Accuracy</td><td>0.89352</td></tr><tr><td>eval/F1-score</td><td>0.8238</td></tr><tr><td>eval/Precision</td><td>0.79527</td></tr><tr><td>eval/Recall</td><td>0.87347</td></tr><tr><td>eval/loss</td><td>0.11861</td></tr><tr><td>eval/mcc_metric</td><td>0.79544</td></tr><tr><td>eval/runtime</td><td>4.0552</td></tr><tr><td>eval/samples_per_second</td><td>555.832</td></tr><tr><td>eval/steps_per_second</td><td>17.508</td></tr><tr><td>total_flos</td><td>852563953612800.0</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>1645</td></tr><tr><td>train/grad_norm</td><td>0.90675</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>0.1448</td></tr><tr><td>train_loss</td><td>0.20985</td></tr><tr><td>train_runtime</td><td>287.0036</td></tr><tr><td>train_samples_per_second</td><td>183.221</td></tr><tr><td>train_steps_per_second</td><td>5.732</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fancy-sweep-5</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/to3uvb1h' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/to3uvb1h</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250325_113922-to3uvb1h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_runtime': 295.493498149, '_step': 32, '_timestamp': 1742899458.7640474, '_wandb': {'runtime': 295}, 'eval/AUC-ROC': 0.9776794999278964, 'eval/Accuracy': 0.8935226264418811, 'eval/F1-score': 0.8237965481576575, 'eval/Precision': 0.7952690077541764, 'eval/Recall': 0.8734746496338153, 'eval/loss': 0.11860866844654085, 'eval/mcc_metric': 0.7954413436068798, 'eval/runtime': 4.0552, 'eval/samples_per_second': 555.832, 'eval/steps_per_second': 17.508, 'total_flos': 852563953612800, 'train/epoch': 5, 'train/global_step': 1645, 'train/grad_norm': 0.9067480564117432, 'train/learning_rate': 2.9899161842693692e-05, 'train/loss': 0.1448, 'train_loss': 0.20984870858467825, 'train_runtime': 287.0036, 'train_samples_per_second': 183.221, 'train_steps_per_second': 5.732}\n",
      "Best hyperparameters: {'r': 128, 'lr': 0.0011492784634238222, 'bf16': False, 'fp16': False, 'fsdp': [], 'seed': 42, 'tf32': None, 'debug': [], 'optim': 'adamw_torch', 'top_k': 50, 'top_p': 1, 'is_gpu': True, 'prefix': None, 'do_eval': True, 'dropout': 0.1, 'no_cuda': False, 'tp_size': 0, 'use_cpu': False, 'do_train': False, 'id2label': {'0': 'LABEL_0', '1': 'LABEL_1', '2': 'LABEL_2', '3': 'LABEL_3', '4': 'LABEL_4'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4}, 'norm_std': [2.9210526350021033, 1.5294133532822063, 2.9209947673330334, 0.21956154740898992, 0.22097666681598951, 160.4856642380458, 151.38170855657367, 160.3304390667665, 60.484857692625106, 0.181038611279414, 0, 0, 0, 0, 0.24851193112366385, 0.317494124851492, 0.37175815103599535, 0.6098706561111424, 539.8195290502504, 8.140940922894863, 6.600767667198695, 6.700942921964325, 5.536318526756788, 4.020569431789569, 4.316039675035455, 3.229701298304296, 4.058753110098356, 2.399274478688092, 4.590084765547685, 1.8657465201411236, 8.197075845395899, 1.3989800795766576, 8.727770321711972, 4.719034225006412, 3.6844834579923407, 66.65125255607474, 11.022808176926915, 9.88512023443511, 5.895101555004671, 6.0315631910071374, 4.465786134186721, 8.73293454096314, 7.292192943139112, 5.798809757257198, 5.458840154330179, 5.34562222799046, 28.624753237838465, 22.7685485030176, 13.735506972569182, 12.75558914023291, 12.647297666063738, 16.73803715869515, 1.3236865505015507, 8.012917117258175, 6.328266302270954, 30.80439768300023, 14.510669158473307, 33.76748799216324, 0, 8.851153866015428, 8.222102882220607, 7.329351085680612, 4.87773057457412, 10.796349487508555, 24.55359833254403, 10.33295824604808, 8.986884190324291, 26.77991276665104, 29.521288543995215, 4.077418430037268, 11.23487898363004, 0, 50.277243284807206, 19.12173183245714, 9.819697177666312, 1.4201437981599128, 12.511435257208836, 14.212538029397628, 16.973978925056553, 19.21649041911615, 15.092240504961104, 19.889237093009676, 25.80872442073538, 9.254317550453823, 19.013243564373347, 3.6841568734614953, 17.690679185577395, 10.27595457263499, 3.3283202642652645, 2.8773795244438474, 9.228734822190496, 5.106296483962912, 4.008127533955226, 2.3345092198667503, 0.23958883840178577, 11.48532061063049, 2.0042680181777808, 3.411142707197923, 0.7103265443180337, 0.8009597262862117, 1.0630493791282618, 1.249503799091361, 0.8592211073826755, 1.4909738617970665, 2.8049912821495706, 1.5692082041123123, 3.718886071238216, 4.918753910447648, 0.6213838320183964, 0.6971589290933399, 0.9385507839118636, 1.7370945619837506, 2.7759468746763334, 43.91556441471313, 0.2929625321198007, 0.6742399816263887, 0.6447563579731193, 0.26136083143708466, 0.1703202147866646, 1.3696411924562566, 0.3394696140137124, 0.26977939457438505, 0.3350074869447194, 0.3408584597974497, 1.2690580420372088, 1.2684116362885036, 0.1297126917051003, 0.06304965563156611, 0.17914965229828922, 1.485673805113914, 1.1656052934139842, 0.5018632205797633, 0.15576643470973517, 0.2883562378800223, 0.3774901929558512, 0.3394696140137124, 0.07983606764988928, 0.1030741645577756, 0.11692041889415362, 1.0010868912132271, 0.7705779932112281, 1.157481598590082, 0.13507534533122212, 0.8359812306885952, 0.7600865243553028, 0.04757124327808961, 0.07183232513905516, 0.03513570421263404, 1.239225396368063, 0.015097985029438592, 1.3364349277900949, 0.013378265133341392, 0.032663541616103894, 0.060970137226002974, 0.44400840883756576, 1.159532265122051, 0.198246590935912, 0.1491817288215558, 1.28126795861232, 0.143114919141507, 0.11579880303510388, 0.25012811724209466, 0.1830406121462275, 0.03504726333553974, 0.015295758691880374, 0.3034514997274073, 0.2749689545601939, 0.04859983910409953, 0.09878498419533764, 0.5707110234042025, 0.17028898672063034, 0.24456026600763192, 0.21322057789532145, 0.1917343827305721, 0.13591391704896466, 0.03519702423260403, 0.1108018278371122, 0.0680510883818226, 0.5264724473438641, 0.2602735481879015, 0.25847912916802446, 0.10886360159063148, 0.1002693464072736, 0.35113436163289397, 0.2260341350934195, 0.16874580630684471, 0, 0.4146998571400424, 0.5347143492505464, 0.3137422508894841, 0.27962501103110715, 0.1547563582555832, 0.08130444916739461, 0.08949068223889126, 0.225304925348536, 0.014421012861987593, 0.2736413019822887, 2.253629375384596, 0.22817317920167496], 'run_name': './models_MTR100_Chemberta/glzr8jim', 'use_ipex': False, 'adafactor': False, 'data_seed': None, 'deepspeed': None, 'do_sample': False, 'hub_token': '<HUB_TOKEN>', 'log_level': 'passive', 'max_steps': -1, 'norm_mean': [11.199569164274653, -0.9728601944583676, 11.199595401578872, 0.1914454376660732, 0.608589373135307, 365.064017672, 342.24912812000014, 364.6033136038417, 134.06547, 0.004249, 0, 0, 0, 0, 1.1861084842221647, 1.890967178564785, 2.519587985439997, 2.0112818114267816, 795.5621221754437, 18.14439203724506, 14.536240385432391, 15.215140271072489, 12.068994414289726, 8.453657900068215, 9.114162139055054, 6.434168605708085, 7.215103879809845, 4.436200487997215, 5.109730699855831, 3.055231525907226, 3.6252747118486264, -2.202564923376624, 18.195385007867852, 7.970699358994477, 4.5379164631837545, 150.95250337667272, 13.184208966483704, 8.814008658052902, 3.81918390789873, 3.4969386790830774, 2.9222201316693712, 2.644444123964607, 6.408740449956927, 4.95314480536345, 2.6263770771853108, 2.4113616526384853, 26.24052195128434, 37.102909834641714, 19.89943953042712, 16.353848799228413, 15.638332143998122, 21.706094849865753, 0.28727529762970366, 8.054432014422119, 3.2648099385428853, 32.629006626588726, 16.26551059790217, 47.70605007162041, 0, 5.325837027308287, 9.698460925314944, 5.573601891254677, 2.581492771453006, 7.312496194388467, 33.07539073817076, 10.718462271839512, 6.99277406210818, 31.684923475431933, 36.92162447084414, 1.2074202610211655, 5.110701506051421, 0, 71.04050338999998, 9.57750975344203, 10.066085526965992, 0.07691213090851719, 13.38923196114951, 16.862422387837878, 21.382953923695233, 15.651918121909311, 14.440634953378058, 19.13130604146014, 22.114944705243296, 8.183429061888226, 13.699768012021506, 2.1212691930096144, 17.474216494453906, 7.846769617492272, 2.6683841482907034, 0.11868201225906092, 9.064881467380092, 2.659801877718109, 4.055917032498944, 0.259848432909807, 0.413963629624058, 25.186704, 1.79722, 5.353545, 0.272499, 0.562898, 0.835397, 1.236854, 0.729917, 1.966771, 4.216321, 1.414081, 6.486208, 5.688314, 0.205632, 0.409204, 0.614836, 2.802168, 2.7549044689500004, 97.31541557350002, 0.069051, 0.151924, 0.130758, 0.06279, 0.027038, 0.999062, 0.096951, 0.042862, 0.096089, 0.100163, 1.033857, 1.034286, 0.016206, 0.00357, 0.016776, 1.488795, 0.915699, 0.232236, 0.012241, 0.074885, 0.131561, 0.096951, 0.004026, 0.009835, 0.011646, 0.250196, 0.131237, 0.768633, 0.015927, 0.539599, 0.451885, 0.001726, 0.003335, 0.001218, 1.236474, 0.000226, 0.555529, 0.000149, 0.001046, 0.002578, 0.126995, 0.732216, 0.037978, 0.019179, 0.720141, 0.018951, 0.013025, 0.059523, 0.027553, 0.000831, 0.0002, 0.073914, 0.061694, 0.002249, 0.007716, 0.236426, 0.0287, 0.05231, 0.041425, 0.033421, 0.017275, 0.001082, 0.011915, 0.004249, 0.196769, 0.039316, 0.038686, 0.00409, 0.003615, 0.116124, 0.051192, 0.025177, 0, 0.161908, 0.315775, 0.087229, 0.079586, 0.023227, 0.005966, 0.007901, 0.050376, 0.000186, 0.065723, 0.380193, 0.051566], 'num_beams': 1, 'optimizer': ['adamw'], 'ray_scope': 'last', 'report_to': ['wandb'], 'typical_p': 1, 'use_cache': True, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'do_predict': False, 'eval_delay': 0, 'eval_steps': 100, 'hidden_act': 'gelu', 'is_decoder': False, 'local_rank': 0, 'lora_alpha': 128, 'max_length': 20, 'min_length': 0, 'model_type': 'roberta', 'optim_args': None, 'output_dir': './models_MTR100_Chemberta/glzr8jim', 'past_index': -1, 'save_steps': 500, 'vocab_size': 600, 'ddp_backend': None, 'ddp_timeout': 1800, 'fsdp_config': {'xla': False, 'xla_fsdp_v2': False, 'min_num_params': 0, 'xla_fsdp_grad_ckpt': False}, 'hidden_size': 384, 'label_names': None, 'logging_dir': './logs_flavor_chem_wandb', 'peft_config': {'default': {'r': 128, 'bias': 'none', 'revision': None, 'use_dora': False, 'lora_bias': False, 'peft_type': 'LORA', 'task_type': 'SEQ_CLS', 'eva_config': None, 'lora_alpha': 128, 'use_rslora': False, 'auto_mapping': None, 'lora_dropout': 0.1, 'megatron_core': 'megatron.core', 'fan_in_fan_out': False, 'inference_mode': False, 'layers_pattern': None, 'runtime_config': {'ephemeral_gpu_offload': False}, 'target_modules': ['out_proj', 'query', 'value', 'key', 'dense'], 'exclude_modules': None, 'megatron_config': None, 'modules_to_save': ['classifier', 'score'], 'init_lora_weights': True, 'layer_replication': None, 'layers_to_transform': None, 'base_model_name_or_path': 'DeepChem/ChemBERTa-77M-MTR'}}, 'push_to_hub': False, 'return_dict': True, 'temperature': 1, 'torch_dtype': 'float32', 'torchdynamo': None, 'torchscript': False, 'adam_epsilon': 1e-08, 'bos_token_id': 0, 'disable_tqdm': False, 'eos_token_id': 2, 'fp16_backend': 'auto', 'hub_model_id': None, 'hub_strategy': 'every_save', 'pad_token_id': 1, 'problem_type': None, 'sep_token_id': None, 'use_bfloat16': False, 'warmup_ratio': 0, 'warmup_steps': 0, 'weight_decay': 0.01, '_name_or_path': 'DeepChem/ChemBERTa-77M-MTR', 'architectures': ['RobertaForRegression'], 'bad_words_ids': None, 'eval_on_start': False, 'eval_strategy': 'steps', 'jit_mode_eval': False, 'learning_rate': 0.0011492784634238222, 'logging_steps': 100, 'max_grad_norm': 1, 'mp_parameters': '', 'output_scores': False, 'save_strategy': 'steps', 'split_batches': None, 'torch_compile': False, 'tpu_num_cores': None, 'bf16_full_eval': False, 'early_stopping': False, 'fp16_full_eval': False, 'fp16_opt_level': 'O1', 'layer_norm_eps': 1e-12, 'length_penalty': 1, 'tf_legacy_loss': False, 'use_mps_device': False, 'finetuning_task': None, 'group_by_length': False, 'hub_always_push': False, 'num_beam_groups': 1, 'save_only_model': False, 'suppress_tokens': None, 'tokenizer_class': None, 'type_vocab_size': 1, 'dispatch_batches': None, 'full_determinism': False, 'hub_private_repo': None, 'ignore_data_skip': False, 'log_on_each_node': True, 'logging_strategy': 'steps', 'num_train_epochs': 5, 'save_safetensors': True, 'save_total_limit': None, 'use_liger_kernel': False, 'ddp_bucket_cap_mb': None, 'diversity_penalty': 0, 'greater_is_better': True, 'initializer_range': 0.02, 'intermediate_size': 464, 'log_level_replica': 'warning', 'lr_scheduler_type': 'linear', 'num_hidden_layers': 3, 'output_attentions': False, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'save_on_each_node': False, 'tpu_metrics_debug': False, 'accelerator_config': {'even_batches': True, 'non_blocking': False, 'split_batches': False, 'dispatch_batches': None, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None}, 'batch_eval_metrics': False, 'classifier_dropout': None, 'is_encoder_decoder': False, 'length_column_name': 'length', 'logging_first_step': False, 'repetition_penalty': 1, 'torch_compile_mode': None, 'add_cross_attention': False, 'evaluation_strategy': 'steps', 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'fsdp_min_num_params': 0, 'hidden_dropout_prob': 0.144, 'include_for_metrics': [], 'neftune_noise_alpha': None, 'num_attention_heads': 12, 'skip_memory_metrics': True, 'tie_encoder_decoder': False, 'tie_word_embeddings': True, 'auto_find_batch_size': False, 'dataloader_drop_last': False, 'model/num_parameters': 5706234, 'no_repeat_ngram_size': 0, 'num_return_sequences': 1, 'optim_target_modules': None, 'output_hidden_states': False, 'overwrite_output_dir': False, 'prediction_loss_only': False, 'push_to_hub_model_id': None, 'task_specific_params': None, 'transformers_version': '4.50.0', 'begin_suppress_tokens': None, 'dataloader_pin_memory': True, 'ddp_broadcast_buffers': None, 'metric_for_best_model': 'eval_mcc_metric', 'remove_invalid_values': False, 'remove_unused_columns': False, 'torch_compile_backend': None, 'dataloader_num_workers': 0, 'decoder_start_token_id': None, 'eval_do_concat_batches': True, 'eval_use_gather_object': False, 'gradient_checkpointing': False, 'half_precision_backend': 'auto', 'label_smoothing_factor': 0, 'load_best_model_at_end': True, 'logging_nan_inf_filter': True, 'resume_from_checkpoint': None, 'chunk_size_feed_forward': 0, 'eval_accumulation_steps': None, 'max_position_embeddings': 515, 'per_gpu_eval_batch_size': None, 'position_embedding_type': 'absolute', 'return_dict_in_generate': False, 'torch_empty_cache_steps': None, 'per_gpu_train_batch_size': None, 'push_to_hub_organization': None, 'include_tokens_per_second': False, 'dataloader_prefetch_factor': None, 'ddp_find_unused_parameters': None, 'include_inputs_for_metrics': False, 'per_device_eval_batch_size': 32, 'use_legacy_prediction_loop': False, 'cross_attention_hidden_size': None, 'gradient_accumulation_steps': 1, 'per_device_train_batch_size': 32, '_attn_implementation_autoset': True, 'attention_probs_dropout_prob': 0.109, 'encoder_no_repeat_ngram_size': 0, 'average_tokens_across_devices': False, 'dataloader_persistent_workers': False, 'gradient_checkpointing_kwargs': None, 'include_num_input_tokens_seen': False, 'exponential_decay_length_penalty': None, 'fsdp_transformer_layer_cls_to_wrap': None, 'restore_callback_states_from_checkpoint': False}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    sweep_config = {\n",
    "    \"name\": \"Flavor Hyperparameter Tuning\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"goal\": \"maximize\", \n",
    "        \"name\": \"eval/mcc_metric\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 1e-5,  \n",
    "            \"max\": 2e-3\n",
    "        },\n",
    "        \"r\": {\n",
    "            \"values\": [4,8,16,32,64, 128]\n",
    "        },\n",
    "        \"lora_alpha\": {\n",
    "            \"values\": [4,8,16,32,64,128]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"values\": [0.0,0.1,0.2]\n",
    "        },\n",
    "        \n",
    "        \"optimizer\": {\n",
    "            \"value\": [\"adamw\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"huggingface\")\n",
    "    wandb.agent(sweep_id, function=run_training, count=5)\n",
    "\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(f\"huggingface/{sweep_id}\")\n",
    "    print(sweep.runs[0].summary_metrics)\n",
    "\n",
    "    runs_with_rmse = [run for run in sweep.runs if 'eval/mcc_metric' in run.summary_metrics]\n",
    "    if runs_with_rmse:\n",
    "        # Sort by rmse in descending order (maximize)\n",
    "        best_run = sorted(runs_with_rmse, key=lambda run: run.summary_metrics['eval/mcc_metric'])[0]\n",
    "    else:\n",
    "        raise ValueError(\"No runs found with 'eval/mcc_metric' metric.\")\n",
    "\n",
    "    best_hyperparameters = best_run.config\n",
    "    print(f\"Best hyperparameters: {best_hyperparameters}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Ensure deterministic behavior in PyTorch computations\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-10M-MTR\",\n",
    "    num_labels=5,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True,\n",
    "    \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-10M-MTR\",trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel  \n",
    "\n",
    "adapter_model = PeftModel.from_pretrained(base_model, \"/home/raghvendra2/Molformer_Finetuning/results_flavor_chemberta_wandb_reepoch_10mtr/checkpoint-1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute metrics\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    probabilities= softmax(logits, axis=1)\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "        \n",
    "    return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities,multi_class=\"ovr\"),  # AUC-ROC requires probabilities\n",
    "            \"Precision\": precision_score(labels, predictions,average=\"macro\"),\n",
    "            \"Recall\": recall_score(labels, predictions,average=\"macro\"),\n",
    "            \"F1-score\": f1_score(labels, predictions,average=\"macro\")\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_results_flavor\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    report_to=\"none\",  # Disable logging to W&B for test\n",
    "    \n",
    "\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3133714/1987706221.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer= CustomTrainer(\n",
    "    model=adapter_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for all 77 MTR Moels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Missing 'adapter_config.json' in ./models_MTR100_Chemberta/ujye8pur\n",
      "⚠️ Missing 'adapter_config.json' in ./models_MTR100_Chemberta/5k0m1ybg\n",
      "⚠️ Missing 'adapter_config.json' in ./models_MTR100_Chemberta/vx2zkfhq\n",
      "⚠️ Missing 'adapter_config.json' in ./models_MTR100_Chemberta/tl30zdlb\n",
      "⚠️ Missing 'adapter_config.json' in ./models_MTR100_Chemberta/t6motadp\n",
      "Valid Model Checkpoints: ['./models_MTR100_Chemberta/glzr8jim', './models_MTR100_Chemberta/y6ioij00', './models_MTR100_Chemberta/imo6nm9x', './models_MTR100_Chemberta/to3uvb1h', './models_MTR100_Chemberta/oggmqthu']\n",
      "Evaluating model: ./models_MTR100_Chemberta/glzr8jim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2954178/2028187298.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2954178/2028187298.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR100_Chemberta/glzr8jim: {'eval_mcc_metric': 0.7857732041076384, 'eval_loss': 0.13933634757995605, 'eval_model_preparation_time': 0.0047, 'eval_Accuracy': 0.8864241348713399, 'eval_AUC-ROC': 0.9560709660479298, 'eval_Precision': 0.80451841570831, 'eval_Recall': 0.7921601853640271, 'eval_F1-score': 0.7971128505954014, 'eval_runtime': 5.277, 'eval_samples_per_second': 427.137, 'eval_steps_per_second': 13.455}\n",
      "Evaluating model: ./models_MTR100_Chemberta/y6ioij00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2954178/2028187298.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR100_Chemberta/y6ioij00: {'eval_mcc_metric': 0.7777237717003995, 'eval_loss': 0.13908489048480988, 'eval_model_preparation_time': 0.0045, 'eval_Accuracy': 0.8824312333629104, 'eval_AUC-ROC': 0.9680355972198766, 'eval_Precision': 0.8615301215900842, 'eval_Recall': 0.755471099116446, 'eval_F1-score': 0.7915827963618229, 'eval_runtime': 4.6413, 'eval_samples_per_second': 485.64, 'eval_steps_per_second': 15.297}\n",
      "Evaluating model: ./models_MTR100_Chemberta/imo6nm9x\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2954178/2028187298.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR100_Chemberta/imo6nm9x: {'eval_mcc_metric': 0.7818169758422309, 'eval_loss': 0.1437530368566513, 'eval_model_preparation_time': 0.0037, 'eval_Accuracy': 0.8850931677018633, 'eval_AUC-ROC': 0.9721605705286832, 'eval_Precision': 0.8250201990356656, 'eval_Recall': 0.7875592578624443, 'eval_F1-score': 0.8045247463831717, 'eval_runtime': 3.9065, 'eval_samples_per_second': 576.986, 'eval_steps_per_second': 18.175}\n",
      "Evaluating model: ./models_MTR100_Chemberta/to3uvb1h\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2954178/2028187298.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR100_Chemberta/to3uvb1h: {'eval_mcc_metric': 0.777782885687969, 'eval_loss': 0.1434350460767746, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.883318544809228, 'eval_AUC-ROC': 0.9714154098726826, 'eval_Precision': 0.8247383778991484, 'eval_Recall': 0.7833793596443682, 'eval_F1-score': 0.8022087198646425, 'eval_runtime': 4.0312, 'eval_samples_per_second': 559.135, 'eval_steps_per_second': 17.613}\n",
      "Evaluating model: ./models_MTR100_Chemberta/oggmqthu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR100_Chemberta/oggmqthu: {'eval_mcc_metric': 0.7667172633940723, 'eval_loss': 0.14829595386981964, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8757763975155279, 'eval_AUC-ROC': 0.9531141891298681, 'eval_Precision': 0.7546311912638144, 'eval_Recall': 0.746212206833805, 'eval_F1-score': 0.7496316603246458, 'eval_runtime': 4.0222, 'eval_samples_per_second': 560.384, 'eval_steps_per_second': 17.652}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "models_dir = \"./models_MTR100_Chemberta\"\n",
    "\n",
    "for ckpt in os.listdir(models_dir):\n",
    "    model_path = os.path.join(models_dir, ckpt)\n",
    "    \n",
    "    if os.path.isdir(model_path):\n",
    "        adapter_config_path = os.path.join(model_path, \"adapter_config.json\")\n",
    "        \n",
    "        if not os.path.exists(adapter_config_path):\n",
    "            print(f\"⚠️ Missing 'adapter_config.json' in {model_path}\")\n",
    "\n",
    "# Evaluate each saved model\n",
    "\n",
    "valid_checkpoints = [\n",
    "    os.path.join(models_dir, ckpt)\n",
    "    for ckpt in os.listdir(models_dir)\n",
    "    if os.path.isdir(os.path.join(models_dir, ckpt)) and \n",
    "       os.path.exists(os.path.join(models_dir, ckpt, \"adapter_config.json\"))\n",
    "]\n",
    "\n",
    "print(\"Valid Model Checkpoints:\", valid_checkpoints)\n",
    "\n",
    "for model_path in valid_checkpoints:\n",
    "    print(f\"Evaluating model: {model_path}\")\n",
    "\n",
    "    adapter_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    adapter_model.eval()\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=adapter_model,\n",
    "        args=training_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    test_results_clin = trainer.evaluate()\n",
    "    print(f\"Test Results for {model_path}: {test_results_clin}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for check points of glzr8jim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel  \n",
    "\n",
    "adapter_model = PeftModel.from_pretrained(base_model, \"/home/raghvendra2/Molformer_Finetuning/models_MTR100_Chemberta/to3uvb1h/checkpoint-1645\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for checkpoint 1645: {'eval_mcc_metric': 0.7857732041076384, 'eval_loss': 0.13933634757995605, 'eval_Accuracy': 0.8864241348713399, 'eval_AUC-ROC': 0.9560709660479298, 'eval_Precision': 0.80451841570831, 'eval_Recall': 0.7921601853640271, 'eval_F1-score': 0.7971128505954014, 'eval_runtime': 3.3347, 'eval_samples_per_second': 675.93, 'eval_steps_per_second': 5.398}\n"
     ]
    }
   ],
   "source": [
    "test_results_flavor = trainer.evaluate()\n",
    "\n",
    "print(\"Test Results for checkpoint 1645:\", test_results_flavor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for checkpoint 1500: {'eval_mcc_metric': 0.7893589818962506, 'eval_loss': 0.14277541637420654, 'eval_Accuracy': 0.888642413487134, 'eval_AUC-ROC': 0.970534896759912, 'eval_Precision': 0.8309968335234739, 'eval_Recall': 0.7920133444927788, 'eval_F1-score': 0.8095269496517847, 'eval_runtime': 7.7548, 'eval_samples_per_second': 290.659, 'eval_steps_per_second': 2.321}\n"
     ]
    }
   ],
   "source": [
    "test_results_flavor = trainer.evaluate()\n",
    "\n",
    "print(\"Test Results for checkpoint 1645:\", test_results_flavor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result for imo6nm9x checkpoint 1645  (Best Model until now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for checkpoint 1000: {'eval_mcc_metric': 0.7882497847517572, 'eval_loss': 0.14135079085826874, 'eval_Accuracy': 0.8873114463176575, 'eval_AUC-ROC': 0.9720325211171271, 'eval_Precision': 0.8327510094500526, 'eval_Recall': 0.8274682544764185, 'eval_F1-score': 0.8297152462405265, 'eval_runtime': 3.0135, 'eval_samples_per_second': 747.963, 'eval_steps_per_second': 5.973}\n"
     ]
    }
   ],
   "source": [
    "test_results_flavor = trainer.evaluate()\n",
    "\n",
    "print(\"Test Results for checkpoint 1645:\", test_results_flavor) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best model is : checkpoint 1500 of result flavor chemberta wandb for the original flavor paper model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 MTR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Model Checkpoints: ['./models_MTR10_Chemberta/g4esefrp', './models_MTR10_Chemberta/ejr2572h', './models_MTR10_Chemberta/ljfmymgw', './models_MTR10_Chemberta/6wz8dk9m', './models_MTR10_Chemberta/ng77zdds', './models_MTR10_Chemberta/01aq0qc9', './models_MTR10_Chemberta/qfe8vcqo', './models_MTR10_Chemberta/bc9aajo1', './models_MTR10_Chemberta/bxb0kwqm', './models_MTR10_Chemberta/wmaugze4', './models_MTR10_Chemberta/sdj2wujw', './models_MTR10_Chemberta/l3pooke3', './models_MTR10_Chemberta/6fbtk138', './models_MTR10_Chemberta/7zpn8eng', './models_MTR10_Chemberta/g9h0hhdh', './models_MTR10_Chemberta/9m90icx1', './models_MTR10_Chemberta/fh69w10f', './models_MTR10_Chemberta/izd262gb', './models_MTR10_Chemberta/1y764hkg', './models_MTR10_Chemberta/ot8tkhls']\n",
      "Evaluating model: ./models_MTR10_Chemberta/g4esefrp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/g4esefrp: {'eval_mcc_metric': -0.051316471359806895, 'eval_loss': 1.0326316356658936, 'eval_model_preparation_time': 0.0035, 'eval_Accuracy': 0.14640638864241348, 'eval_AUC-ROC': 0.43906379255989547, 'eval_Precision': 0.2489830873085715, 'eval_Recall': 0.18508591537359448, 'eval_F1-score': 0.14766558492259813, 'eval_runtime': 6.5404, 'eval_samples_per_second': 344.629, 'eval_steps_per_second': 10.856}\n",
      "Evaluating model: ./models_MTR10_Chemberta/ejr2572h\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/ejr2572h: {'eval_mcc_metric': 0.7866560449462396, 'eval_loss': 0.1447119116783142, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.885980479148181, 'eval_AUC-ROC': 0.9715066356563075, 'eval_Precision': 0.8646903595235462, 'eval_Recall': 0.6974415806217465, 'eval_F1-score': 0.719808474928872, 'eval_runtime': 6.3306, 'eval_samples_per_second': 356.051, 'eval_steps_per_second': 11.215}\n",
      "Evaluating model: ./models_MTR10_Chemberta/ljfmymgw\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/ljfmymgw: {'eval_mcc_metric': 0.7821532659249115, 'eval_loss': 0.14776849746704102, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8837622005323869, 'eval_AUC-ROC': 0.9723125903238156, 'eval_Precision': 0.8613768663665144, 'eval_Recall': 0.6965203566199836, 'eval_F1-score': 0.7186124420025131, 'eval_runtime': 6.3015, 'eval_samples_per_second': 357.695, 'eval_steps_per_second': 11.267}\n",
      "Evaluating model: ./models_MTR10_Chemberta/6wz8dk9m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/6wz8dk9m: {'eval_mcc_metric': 0.7668119811955401, 'eval_loss': 0.14871440827846527, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8775510204081632, 'eval_AUC-ROC': 0.9690304146799606, 'eval_Precision': 0.8111675250675251, 'eval_Recall': 0.7393671017209662, 'eval_F1-score': 0.767372027794409, 'eval_runtime': 6.3406, 'eval_samples_per_second': 355.486, 'eval_steps_per_second': 11.198}\n",
      "Evaluating model: ./models_MTR10_Chemberta/ng77zdds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/ng77zdds: {'eval_mcc_metric': 0.7903027900598758, 'eval_loss': 0.13793590664863586, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.888642413487134, 'eval_AUC-ROC': 0.9712670821306159, 'eval_Precision': 0.8297460295827033, 'eval_Recall': 0.7985800549192325, 'eval_F1-score': 0.8120801935825286, 'eval_runtime': 6.3815, 'eval_samples_per_second': 353.209, 'eval_steps_per_second': 11.126}\n",
      "Evaluating model: ./models_MTR10_Chemberta/01aq0qc9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/01aq0qc9: {'eval_mcc_metric': 0.7430506848294517, 'eval_loss': 0.18837043642997742, 'eval_model_preparation_time': 0.0045, 'eval_Accuracy': 0.8637976929902396, 'eval_AUC-ROC': 0.9323429148394224, 'eval_Precision': 0.8413073954652214, 'eval_Recall': 0.6646134687107592, 'eval_F1-score': 0.6913864900401119, 'eval_runtime': 5.5536, 'eval_samples_per_second': 405.864, 'eval_steps_per_second': 12.785}\n",
      "Evaluating model: ./models_MTR10_Chemberta/qfe8vcqo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/qfe8vcqo: {'eval_mcc_metric': 0.7958600550347535, 'eval_loss': 0.13877424597740173, 'eval_model_preparation_time': 0.0037, 'eval_Accuracy': 0.8917480035492458, 'eval_AUC-ROC': 0.9663726912480035, 'eval_Precision': 0.880921824044, 'eval_Recall': 0.7662753528081125, 'eval_F1-score': 0.805181953288814, 'eval_runtime': 6.2391, 'eval_samples_per_second': 361.272, 'eval_steps_per_second': 11.38}\n",
      "Evaluating model: ./models_MTR10_Chemberta/bc9aajo1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/bc9aajo1: {'eval_mcc_metric': 0.7643431820635597, 'eval_loss': 0.1676427721977234, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8735581188997338, 'eval_AUC-ROC': 0.9562373760530886, 'eval_Precision': 0.7645788136181467, 'eval_Recall': 0.7538178204002817, 'eval_F1-score': 0.7581014272158283, 'eval_runtime': 5.7196, 'eval_samples_per_second': 394.085, 'eval_steps_per_second': 12.413}\n",
      "Evaluating model: ./models_MTR10_Chemberta/bxb0kwqm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/bxb0kwqm: {'eval_mcc_metric': 0.7578172672777702, 'eval_loss': 0.15704374015331268, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8717834960070985, 'eval_AUC-ROC': 0.9631741922590947, 'eval_Precision': 0.8466492193563179, 'eval_Recall': 0.6729556781098237, 'eval_F1-score': 0.6985194228220318, 'eval_runtime': 5.8746, 'eval_samples_per_second': 383.686, 'eval_steps_per_second': 12.086}\n",
      "Evaluating model: ./models_MTR10_Chemberta/wmaugze4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/wmaugze4: {'eval_mcc_metric': 0.7458759952897328, 'eval_loss': 0.17986901104450226, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8646850044365573, 'eval_AUC-ROC': 0.9327447034435042, 'eval_Precision': 0.7320474956194788, 'eval_Recall': 0.6674241202703084, 'eval_F1-score': 0.6829243355826524, 'eval_runtime': 5.5514, 'eval_samples_per_second': 406.026, 'eval_steps_per_second': 12.79}\n",
      "Evaluating model: ./models_MTR10_Chemberta/sdj2wujw\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/sdj2wujw: {'eval_mcc_metric': 0.7333632887355505, 'eval_loss': 0.17621594667434692, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.860248447204969, 'eval_AUC-ROC': 0.9496709618840541, 'eval_Precision': 0.8420489107313628, 'eval_Recall': 0.6503143267384901, 'eval_F1-score': 0.6825494840074434, 'eval_runtime': 5.8442, 'eval_samples_per_second': 385.683, 'eval_steps_per_second': 12.149}\n",
      "Evaluating model: ./models_MTR10_Chemberta/l3pooke3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/l3pooke3: {'eval_mcc_metric': 0.7930913320253843, 'eval_loss': 0.14359517395496368, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8895297249334516, 'eval_AUC-ROC': 0.9621190217440319, 'eval_Precision': 0.7700547891183185, 'eval_Recall': 0.7665916641746098, 'eval_F1-score': 0.7678136810786037, 'eval_runtime': 6.4431, 'eval_samples_per_second': 349.829, 'eval_steps_per_second': 11.019}\n",
      "Evaluating model: ./models_MTR10_Chemberta/6fbtk138\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/6fbtk138: {'eval_mcc_metric': 0.7933993539706464, 'eval_loss': 0.15008734166622162, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8873114463176575, 'eval_AUC-ROC': 0.9704246430750263, 'eval_Precision': 0.7670510669552366, 'eval_Recall': 0.7352891009623562, 'eval_F1-score': 0.7427875600405793, 'eval_runtime': 5.8871, 'eval_samples_per_second': 382.87, 'eval_steps_per_second': 12.06}\n",
      "Evaluating model: ./models_MTR10_Chemberta/7zpn8eng\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/7zpn8eng: {'eval_mcc_metric': 0.7651667377248028, 'eval_loss': 0.14991040527820587, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8757763975155279, 'eval_AUC-ROC': 0.970537196967868, 'eval_Precision': 0.7902004049412119, 'eval_Recall': 0.7104300055721067, 'eval_F1-score': 0.7363837623750085, 'eval_runtime': 6.1911, 'eval_samples_per_second': 364.071, 'eval_steps_per_second': 11.468}\n",
      "Evaluating model: ./models_MTR10_Chemberta/g9h0hhdh\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/g9h0hhdh: {'eval_mcc_metric': 0.7802386002887226, 'eval_loss': 0.1445203423500061, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.883318544809228, 'eval_AUC-ROC': 0.9715442514202284, 'eval_Precision': 0.8220906747114665, 'eval_Recall': 0.7903911904074594, 'eval_F1-score': 0.8042745847485124, 'eval_runtime': 5.9114, 'eval_samples_per_second': 381.3, 'eval_steps_per_second': 12.011}\n",
      "Evaluating model: ./models_MTR10_Chemberta/9m90icx1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/9m90icx1: {'eval_mcc_metric': 0.7514094282605679, 'eval_loss': 0.16853775084018707, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.867790594498669, 'eval_AUC-ROC': 0.9560897739864972, 'eval_Precision': 0.6891623854302724, 'eval_Recall': 0.6721829327142185, 'eval_F1-score': 0.6789185236137615, 'eval_runtime': 5.869, 'eval_samples_per_second': 384.05, 'eval_steps_per_second': 12.097}\n",
      "Evaluating model: ./models_MTR10_Chemberta/fh69w10f\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/fh69w10f: {'eval_mcc_metric': 0.7889399220829554, 'eval_loss': 0.143296480178833, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8877551020408163, 'eval_AUC-ROC': 0.9722684012847921, 'eval_Precision': 0.8653844054536565, 'eval_Recall': 0.7304610308885935, 'eval_F1-score': 0.7636675609156894, 'eval_runtime': 5.9807, 'eval_samples_per_second': 376.882, 'eval_steps_per_second': 11.872}\n",
      "Evaluating model: ./models_MTR10_Chemberta/izd262gb\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/izd262gb: {'eval_mcc_metric': 0.7535055121586963, 'eval_loss': 0.178395614027977, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8682342502218279, 'eval_AUC-ROC': 0.9349904085573113, 'eval_Precision': 0.7087291941627231, 'eval_Recall': 0.675850986501599, 'eval_F1-score': 0.6858847985640688, 'eval_runtime': 5.7972, 'eval_samples_per_second': 388.808, 'eval_steps_per_second': 12.247}\n",
      "Evaluating model: ./models_MTR10_Chemberta/1y764hkg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_3543620/504190986.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/1y764hkg: {'eval_mcc_metric': 0.7830645275961695, 'eval_loss': 0.1425733119249344, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8846495119787046, 'eval_AUC-ROC': 0.9724818401104148, 'eval_Precision': 0.8660406281416468, 'eval_Recall': 0.7554246405058954, 'eval_F1-score': 0.7920559684082156, 'eval_runtime': 6.1651, 'eval_samples_per_second': 365.604, 'eval_steps_per_second': 11.516}\n",
      "Evaluating model: ./models_MTR10_Chemberta/ot8tkhls\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR10_Chemberta/ot8tkhls: {'eval_mcc_metric': 0.7672753355369059, 'eval_loss': 0.16462251543998718, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8762200532386868, 'eval_AUC-ROC': 0.9429835568930061, 'eval_Precision': 0.8636765181482524, 'eval_Recall': 0.6763263006647735, 'eval_F1-score': 0.7053200238365681, 'eval_runtime': 6.2716, 'eval_samples_per_second': 359.399, 'eval_steps_per_second': 11.321}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from peft import PeftModel  \n",
    "\n",
    "models_dir = \"./models_MTR10_Chemberta\"\n",
    "\n",
    "for ckpt in os.listdir(models_dir):\n",
    "    model_path = os.path.join(models_dir, ckpt)\n",
    "    \n",
    "    if os.path.isdir(model_path):\n",
    "        adapter_config_path = os.path.join(model_path, \"adapter_config.json\")\n",
    "        \n",
    "        if not os.path.exists(adapter_config_path):\n",
    "            print(f\"⚠️ Missing 'adapter_config.json' in {model_path}\")\n",
    "\n",
    "# Evaluate each saved model\n",
    "\n",
    "valid_checkpoints = [\n",
    "    os.path.join(models_dir, ckpt)\n",
    "    for ckpt in os.listdir(models_dir)\n",
    "    if os.path.isdir(os.path.join(models_dir, ckpt)) and \n",
    "       os.path.exists(os.path.join(models_dir, ckpt, \"adapter_config.json\"))\n",
    "]\n",
    "\n",
    "print(\"Valid Model Checkpoints:\", valid_checkpoints)\n",
    "\n",
    "for model_path in valid_checkpoints:\n",
    "    print(f\"Evaluating model: {model_path}\")\n",
    "\n",
    "    adapter_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    adapter_model.eval()\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=adapter_model,\n",
    "        args=training_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    test_results_clin = trainer.evaluate()\n",
    "    print(f\"Test Results for {model_path}: {test_results_clin}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints of 10MTR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel  \n",
    "\n",
    "adapter_model = PeftModel.from_pretrained(base_model, \"/home/raghvendra2/Molformer_Finetuning/models_MTR10_Chemberta/qfe8vcqo/checkpoint-1500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3543620/3341849556.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "        model=adapter_model,\n",
    "        args=training_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model of MTR 10M model ng77zdds 1645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for checkpoint 1645 of ng77zdds: {'eval_mcc_metric': 0.8020882141160608, 'eval_loss': 0.1354757696390152, 'eval_model_preparation_time': 0.0055, 'eval_Accuracy': 0.8948535936113576, 'eval_AUC-ROC': 0.9712263554530509, 'eval_Precision': 0.8163329237327627, 'eval_Recall': 0.8065458720498409, 'eval_F1-score': 0.8099227080402003, 'eval_runtime': 6.2435, 'eval_samples_per_second': 361.016, 'eval_steps_per_second': 11.372}\n"
     ]
    }
   ],
   "source": [
    "test_results_flavor = trainer.evaluate()\n",
    "\n",
    "print(\"Test Results for checkpoint 1645 of ng77zdds:\", test_results_flavor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for checkpoint 1500 of ng77zdds: {'eval_mcc_metric': 0.7903027900598758, 'eval_loss': 0.13793590664863586, 'eval_model_preparation_time': 0.0043, 'eval_Accuracy': 0.888642413487134, 'eval_AUC-ROC': 0.9712670821306159, 'eval_Precision': 0.8297460295827033, 'eval_Recall': 0.7985800549192325, 'eval_F1-score': 0.8120801935825286, 'eval_runtime': 6.3741, 'eval_samples_per_second': 353.618, 'eval_steps_per_second': 11.139}\n"
     ]
    }
   ],
   "source": [
    "test_results_flavor = trainer.evaluate()\n",
    "\n",
    "print(\"Test Results for checkpoint 1500 of ng77zdds:\", test_results_flavor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/71 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for checkpoint 1500 of qfe8vcqo: {'eval_mcc_metric': 0.7895870461452488, 'eval_loss': 0.1416712999343872, 'eval_model_preparation_time': 0.0042, 'eval_Accuracy': 0.888642413487134, 'eval_AUC-ROC': 0.9652783429504371, 'eval_Precision': 0.8719392292967356, 'eval_Recall': 0.7636337052785079, 'eval_F1-score': 0.7999856428943348, 'eval_runtime': 6.3872, 'eval_samples_per_second': 352.894, 'eval_steps_per_second': 11.116}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_results_flavor = trainer.evaluate()\n",
    "\n",
    "print(\"Test Results for checkpoint 1500 of qfe8vcqo:\", test_results_flavor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 MLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_990012/897935396.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Model Checkpoints: ['./models_Mlm_10_Chemberta/fv1ilzi8', './models_Mlm_10_Chemberta/4fb63oti', './models_Mlm_10_Chemberta/gfgudjue', './models_Mlm_10_Chemberta/e374cax6', './models_Mlm_10_Chemberta/8jzpzyyg', './models_Mlm_10_Chemberta/ao6l1ar5', './models_Mlm_10_Chemberta/29mn1i36', './models_Mlm_10_Chemberta/v1e93rjp', './models_Mlm_10_Chemberta/4lskz3k4', './models_Mlm_10_Chemberta/6ocfxa7u']\n",
      "Evaluating model: ./models_Mlm_10_Chemberta/fv1ilzi8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/71 00:00 < 00:03, 18.50 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_990012/897935396.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_10_Chemberta/fv1ilzi8: {'eval_mcc_metric': 0.0704170504547604, 'eval_loss': 1.0308160781860352, 'eval_model_preparation_time': 0.0033, 'eval_Accuracy': 0.2067435669920142, 'eval_AUC-ROC': 0.5201085713307327, 'eval_Precision': 0.2973564404255674, 'eval_Recall': 0.22460865555170856, 'eval_F1-score': 0.15843567515925416, 'eval_runtime': 3.6606, 'eval_samples_per_second': 615.744, 'eval_steps_per_second': 19.396}\n",
      "Evaluating model: ./models_Mlm_10_Chemberta/4fb63oti\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_990012/897935396.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_10_Chemberta/4fb63oti: {'eval_mcc_metric': 0.740909053754227, 'eval_loss': 0.18313008546829224, 'eval_model_preparation_time': 0.0037, 'eval_Accuracy': 0.8615794143744454, 'eval_AUC-ROC': 0.9283378355112839, 'eval_Precision': 0.8270258145704948, 'eval_Recall': 0.6637824095302897, 'eval_F1-score': 0.685709578115499, 'eval_runtime': 3.5679, 'eval_samples_per_second': 631.747, 'eval_steps_per_second': 19.9}\n",
      "Evaluating model: ./models_Mlm_10_Chemberta/gfgudjue\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_990012/897935396.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_10_Chemberta/gfgudjue: {'eval_mcc_metric': 0.754609633972367, 'eval_loss': 0.16939516365528107, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8717834960070985, 'eval_AUC-ROC': 0.9374650956515712, 'eval_Precision': 0.8552058451573432, 'eval_Recall': 0.661818253483307, 'eval_F1-score': 0.6959998780471583, 'eval_runtime': 3.6029, 'eval_samples_per_second': 625.612, 'eval_steps_per_second': 19.707}\n",
      "Evaluating model: ./models_Mlm_10_Chemberta/e374cax6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_990012/897935396.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_10_Chemberta/e374cax6: {'eval_mcc_metric': 0.7593566052351548, 'eval_loss': 0.17612887918949127, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8735581188997338, 'eval_AUC-ROC': 0.9329317680677949, 'eval_Precision': 0.8597654326088628, 'eval_Recall': 0.6692281948095298, 'eval_F1-score': 0.7013560816174796, 'eval_runtime': 3.8342, 'eval_samples_per_second': 587.872, 'eval_steps_per_second': 18.518}\n",
      "Evaluating model: ./models_Mlm_10_Chemberta/8jzpzyyg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_990012/897935396.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_10_Chemberta/8jzpzyyg: {'eval_mcc_metric': 0.7606406163740841, 'eval_loss': 0.16965076327323914, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8744454303460515, 'eval_AUC-ROC': 0.9446570255887197, 'eval_Precision': 0.8584451053943859, 'eval_Recall': 0.6708489711002195, 'eval_F1-score': 0.7026687834980267, 'eval_runtime': 3.6722, 'eval_samples_per_second': 613.805, 'eval_steps_per_second': 19.335}\n",
      "Evaluating model: ./models_Mlm_10_Chemberta/ao6l1ar5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_990012/897935396.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_10_Chemberta/ao6l1ar5: {'eval_mcc_metric': 0.742340558564735, 'eval_loss': 0.18377947807312012, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8624667258207631, 'eval_AUC-ROC': 0.9242068808410323, 'eval_Precision': 0.8273666539646651, 'eval_Recall': 0.6641823066854988, 'eval_F1-score': 0.6860684003517648, 'eval_runtime': 3.6803, 'eval_samples_per_second': 612.451, 'eval_steps_per_second': 19.292}\n",
      "Evaluating model: ./models_Mlm_10_Chemberta/29mn1i36\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_990012/897935396.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_10_Chemberta/29mn1i36: {'eval_mcc_metric': 0.7648311331409482, 'eval_loss': 0.1637691706418991, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8771073646850044, 'eval_AUC-ROC': 0.9352994369006513, 'eval_Precision': 0.8635057662593392, 'eval_Recall': 0.6714753965682861, 'eval_F1-score': 0.7058746215029943, 'eval_runtime': 3.6354, 'eval_samples_per_second': 620.01, 'eval_steps_per_second': 19.53}\n",
      "Evaluating model: ./models_Mlm_10_Chemberta/v1e93rjp\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_990012/897935396.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_10_Chemberta/v1e93rjp: {'eval_mcc_metric': 0.7307316602577645, 'eval_loss': 0.19169306755065918, 'eval_model_preparation_time': 0.0037, 'eval_Accuracy': 0.8589174800354925, 'eval_AUC-ROC': 0.9121644495936069, 'eval_Precision': 0.8309833144548868, 'eval_Recall': 0.6498413016992705, 'eval_F1-score': 0.6802427550062317, 'eval_runtime': 3.5312, 'eval_samples_per_second': 638.311, 'eval_steps_per_second': 20.107}\n",
      "Evaluating model: ./models_Mlm_10_Chemberta/4lskz3k4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_990012/897935396.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_10_Chemberta/4lskz3k4: {'eval_mcc_metric': 0.7349980888326919, 'eval_loss': 0.19541019201278687, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8593611357586513, 'eval_AUC-ROC': 0.9138323763599596, 'eval_Precision': 0.7295496767165879, 'eval_Recall': 0.6574871971495032, 'eval_F1-score': 0.6758098650437983, 'eval_runtime': 3.6219, 'eval_samples_per_second': 622.322, 'eval_steps_per_second': 19.603}\n",
      "Evaluating model: ./models_Mlm_10_Chemberta/6ocfxa7u\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_10_Chemberta/6ocfxa7u: {'eval_mcc_metric': 0.7414589304407849, 'eval_loss': 0.1874830275774002, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8633540372670807, 'eval_AUC-ROC': 0.9239020585135893, 'eval_Precision': 0.833281359845302, 'eval_Recall': 0.661965819696989, 'eval_F1-score': 0.6879138746837524, 'eval_runtime': 3.8368, 'eval_samples_per_second': 587.463, 'eval_steps_per_second': 18.505}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "models_dir = \"./models_Mlm_10_Chemberta\"\n",
    "\n",
    "for ckpt in os.listdir(models_dir):\n",
    "    model_path = os.path.join(models_dir, ckpt)\n",
    "    \n",
    "    if os.path.isdir(model_path):\n",
    "        adapter_config_path = os.path.join(model_path, \"adapter_config.json\")\n",
    "        \n",
    "        if not os.path.exists(adapter_config_path):\n",
    "            print(f\"⚠️ Missing 'adapter_config.json' in {model_path}\")\n",
    "\n",
    "# Evaluate each saved model\n",
    "\n",
    "valid_checkpoints = [\n",
    "    os.path.join(models_dir, ckpt)\n",
    "    for ckpt in os.listdir(models_dir)\n",
    "    if os.path.isdir(os.path.join(models_dir, ckpt)) and \n",
    "       os.path.exists(os.path.join(models_dir, ckpt, \"adapter_config.json\"))\n",
    "]\n",
    "\n",
    "print(\"Valid Model Checkpoints:\", valid_checkpoints)\n",
    "\n",
    "for model_path in valid_checkpoints:\n",
    "    print(f\"Evaluating model: {model_path}\")\n",
    "\n",
    "    adapter_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    adapter_model.eval()\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=adapter_model,\n",
    "        args=training_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    test_results_clin = trainer.evaluate()\n",
    "    print(f\"Test Results for {model_path}: {test_results_clin}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for all 5M MTR models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-5M-MTR\",\n",
    "    num_labels=5,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True,\n",
    "    \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-5M-MTR\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Missing 'adapter_config.json' in ./models_Mtr_5_Chemberta/84cefzee\n",
      "Valid Model Checkpoints: ['./models_Mtr_5_Chemberta/eeuaszyg', './models_Mtr_5_Chemberta/wvxxdsok', './models_Mtr_5_Chemberta/hfjmj8af', './models_Mtr_5_Chemberta/i5zckto9', './models_Mtr_5_Chemberta/24p9zgdn', './models_Mtr_5_Chemberta/io1c4ug5', './models_Mtr_5_Chemberta/nq42s50m', './models_Mtr_5_Chemberta/lfvffslg', './models_Mtr_5_Chemberta/4p2kl0le', './models_Mtr_5_Chemberta/ret7ydyc', './models_Mtr_5_Chemberta/v773pzco', './models_Mtr_5_Chemberta/qlr8qiyn', './models_Mtr_5_Chemberta/7lx28l2q', './models_Mtr_5_Chemberta/ndgq1b2l', './models_Mtr_5_Chemberta/0n7q4g0x', './models_Mtr_5_Chemberta/s53s98co', './models_Mtr_5_Chemberta/r7mxurpz', './models_Mtr_5_Chemberta/7fpw74gq', './models_Mtr_5_Chemberta/1uc5cs8k', './models_Mtr_5_Chemberta/qdtwt30e', './models_Mtr_5_Chemberta/sj3yydqn', './models_Mtr_5_Chemberta/cgefgnd3', './models_Mtr_5_Chemberta/5x17bw96', './models_Mtr_5_Chemberta/j92alz30']\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/eeuaszyg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/71 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/eeuaszyg: {'eval_mcc_metric': 0.0651055566688356, 'eval_loss': 1.035671353340149, 'eval_model_preparation_time': 0.0034, 'eval_Accuracy': 0.2684117125110914, 'eval_AUC-ROC': 0.6048832895929168, 'eval_Precision': 0.26998547654108057, 'eval_Recall': 0.2531751674551164, 'eval_F1-score': 0.1977021888255427, 'eval_runtime': 3.5781, 'eval_samples_per_second': 629.947, 'eval_steps_per_second': 19.843}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/wvxxdsok\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/wvxxdsok: {'eval_mcc_metric': 0.787005962950063, 'eval_loss': 0.1475902944803238, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8873114463176575, 'eval_AUC-ROC': 0.9503441275025594, 'eval_Precision': 0.7425513484941237, 'eval_Recall': 0.7574074614311995, 'eval_F1-score': 0.7476545706894304, 'eval_runtime': 3.6807, 'eval_samples_per_second': 612.378, 'eval_steps_per_second': 19.29}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/hfjmj8af\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/hfjmj8af: {'eval_mcc_metric': 0.7530232352002739, 'eval_loss': 0.16767391562461853, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8686779059449867, 'eval_AUC-ROC': 0.9438039631651737, 'eval_Precision': 0.7729346410476363, 'eval_Recall': 0.7038889021063186, 'eval_F1-score': 0.7271120810644203, 'eval_runtime': 3.5678, 'eval_samples_per_second': 631.769, 'eval_steps_per_second': 19.9}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/i5zckto9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/i5zckto9: {'eval_mcc_metric': 0.7390312690688539, 'eval_loss': 0.17914626002311707, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8620230700976043, 'eval_AUC-ROC': 0.947685470067124, 'eval_Precision': 0.6998397083794436, 'eval_Recall': 0.6616486525427223, 'eval_F1-score': 0.67502951197281, 'eval_runtime': 3.8152, 'eval_samples_per_second': 590.8, 'eval_steps_per_second': 18.61}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/24p9zgdn\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/24p9zgdn: {'eval_mcc_metric': 0.7519646779400456, 'eval_loss': 0.17050717771053314, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8682342502218279, 'eval_AUC-ROC': 0.9496293885338144, 'eval_Precision': 0.7855679533902455, 'eval_Recall': 0.7347662492359218, 'eval_F1-score': 0.7551395927857127, 'eval_runtime': 3.556, 'eval_samples_per_second': 633.854, 'eval_steps_per_second': 19.966}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/io1c4ug5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/io1c4ug5: {'eval_mcc_metric': 0.7599998573532042, 'eval_loss': 0.162087544798851, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8740017746228926, 'eval_AUC-ROC': 0.9627824893460163, 'eval_Precision': 0.7246371076223455, 'eval_Recall': 0.6698514003493337, 'eval_F1-score': 0.6888297999620836, 'eval_runtime': 3.5605, 'eval_samples_per_second': 633.056, 'eval_steps_per_second': 19.941}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/nq42s50m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/nq42s50m: {'eval_mcc_metric': 0.7565117878623016, 'eval_loss': 0.1742604374885559, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8713398402839396, 'eval_AUC-ROC': 0.9453843378202462, 'eval_Precision': 0.6938010237202412, 'eval_Recall': 0.6710703721285134, 'eval_F1-score': 0.6806088063790013, 'eval_runtime': 3.6609, 'eval_samples_per_second': 615.703, 'eval_steps_per_second': 19.394}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/lfvffslg\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/lfvffslg: {'eval_mcc_metric': 0.7486256687843692, 'eval_loss': 0.17598804831504822, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8673469387755102, 'eval_AUC-ROC': 0.94431638374553, 'eval_Precision': 0.6899161468025053, 'eval_Recall': 0.6655308585249371, 'eval_F1-score': 0.6757995478934248, 'eval_runtime': 3.66, 'eval_samples_per_second': 615.841, 'eval_steps_per_second': 19.399}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/4p2kl0le\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/4p2kl0le: {'eval_mcc_metric': 0.7922832370919513, 'eval_loss': 0.13667497038841248, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8895297249334516, 'eval_AUC-ROC': 0.958335401296123, 'eval_Precision': 0.767476398219038, 'eval_Recall': 0.7331228786790375, 'eval_F1-score': 0.7469094345185043, 'eval_runtime': 3.6615, 'eval_samples_per_second': 615.592, 'eval_steps_per_second': 19.391}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/ret7ydyc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/ret7ydyc: {'eval_mcc_metric': 0.7494264204699775, 'eval_loss': 0.1744409203529358, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8673469387755102, 'eval_AUC-ROC': 0.9435746087941403, 'eval_Precision': 0.6908493771629605, 'eval_Recall': 0.6685243613426962, 'eval_F1-score': 0.6776288225484517, 'eval_runtime': 3.8181, 'eval_samples_per_second': 590.34, 'eval_steps_per_second': 18.595}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/v773pzco\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/v773pzco: {'eval_mcc_metric': 0.779754385910448, 'eval_loss': 0.1472068578004837, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8819875776397516, 'eval_AUC-ROC': 0.9511220264515533, 'eval_Precision': 0.7979816011855128, 'eval_Recall': 0.8239869972179441, 'eval_F1-score': 0.8099201205367065, 'eval_runtime': 3.5807, 'eval_samples_per_second': 629.481, 'eval_steps_per_second': 19.828}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/qlr8qiyn\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/qlr8qiyn: {'eval_mcc_metric': 0.7400455526713832, 'eval_loss': 0.16639114916324615, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8655723158828749, 'eval_AUC-ROC': 0.9611121483370189, 'eval_Precision': 0.8545922819696455, 'eval_Recall': 0.6497295959522315, 'eval_F1-score': 0.6898779184753373, 'eval_runtime': 3.5626, 'eval_samples_per_second': 632.679, 'eval_steps_per_second': 19.929}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/7lx28l2q\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/7lx28l2q: {'eval_mcc_metric': 0.7566391514968037, 'eval_loss': 0.15640893578529358, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8726708074534162, 'eval_AUC-ROC': 0.9525040553150884, 'eval_Precision': 0.8597779745883493, 'eval_Recall': 0.6990214916973706, 'eval_F1-score': 0.7433055470674511, 'eval_runtime': 3.6618, 'eval_samples_per_second': 615.54, 'eval_steps_per_second': 19.389}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/ndgq1b2l\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/ndgq1b2l: {'eval_mcc_metric': 0.7632103045045602, 'eval_loss': 0.15403041243553162, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8757763975155279, 'eval_AUC-ROC': 0.9535849490089972, 'eval_Precision': 0.8579272743363477, 'eval_Recall': 0.7040038735293056, 'eval_F1-score': 0.7462833925145368, 'eval_runtime': 3.5805, 'eval_samples_per_second': 629.519, 'eval_steps_per_second': 19.83}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/0n7q4g0x\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/0n7q4g0x: {'eval_mcc_metric': 0.7659941672257725, 'eval_loss': 0.16212618350982666, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8753327417923691, 'eval_AUC-ROC': 0.9407773362619973, 'eval_Precision': 0.7587025191827304, 'eval_Recall': 0.6803051938961573, 'eval_F1-score': 0.7004362841605393, 'eval_runtime': 3.568, 'eval_samples_per_second': 631.728, 'eval_steps_per_second': 19.899}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/s53s98co\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/s53s98co: {'eval_mcc_metric': 0.775463883472275, 'eval_loss': 0.14482459425926208, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8819875776397516, 'eval_AUC-ROC': 0.9687997751692006, 'eval_Precision': 0.7975165993506426, 'eval_Recall': 0.7138352326264439, 'eval_F1-score': 0.7438332882084613, 'eval_runtime': 3.6569, 'eval_samples_per_second': 616.373, 'eval_steps_per_second': 19.415}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/r7mxurpz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/r7mxurpz: {'eval_mcc_metric': 0.7705100325411581, 'eval_loss': 0.1532725840806961, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.878438331854481, 'eval_AUC-ROC': 0.9666861261421982, 'eval_Precision': 0.7843936777647682, 'eval_Recall': 0.7152037431982745, 'eval_F1-score': 0.7384913439108391, 'eval_runtime': 3.5737, 'eval_samples_per_second': 630.719, 'eval_steps_per_second': 19.867}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/7fpw74gq\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/7fpw74gq: {'eval_mcc_metric': 0.7406668010187301, 'eval_loss': 0.16286855936050415, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8642413487133984, 'eval_AUC-ROC': 0.9531267111494071, 'eval_Precision': 0.7478656197722325, 'eval_Recall': 0.6559093159663213, 'eval_F1-score': 0.682981811266985, 'eval_runtime': 3.545, 'eval_samples_per_second': 635.817, 'eval_steps_per_second': 20.028}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/1uc5cs8k\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/1uc5cs8k: {'eval_mcc_metric': 0.7558654399577743, 'eval_loss': 0.16676174104213715, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8708961845607809, 'eval_AUC-ROC': 0.9494609925414753, 'eval_Precision': 0.7128965792860746, 'eval_Recall': 0.6683716678444505, 'eval_F1-score': 0.6832693558897273, 'eval_runtime': 3.8096, 'eval_samples_per_second': 591.661, 'eval_steps_per_second': 18.637}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/qdtwt30e\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/qdtwt30e: {'eval_mcc_metric': 0.7458383301400656, 'eval_loss': 0.16902202367782593, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8646850044365573, 'eval_AUC-ROC': 0.9434091870347826, 'eval_Precision': 0.7325238477555442, 'eval_Recall': 0.6664578143894739, 'eval_F1-score': 0.6826702686592011, 'eval_runtime': 3.5763, 'eval_samples_per_second': 630.26, 'eval_steps_per_second': 19.853}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/sj3yydqn\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/sj3yydqn: {'eval_mcc_metric': 0.7884615118588195, 'eval_loss': 0.13780535757541656, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8877551020408163, 'eval_AUC-ROC': 0.9696235837571147, 'eval_Precision': 0.7673452556415395, 'eval_Recall': 0.7286581383632396, 'eval_F1-score': 0.7435661153633066, 'eval_runtime': 3.6604, 'eval_samples_per_second': 615.779, 'eval_steps_per_second': 19.397}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/cgefgnd3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/cgefgnd3: {'eval_mcc_metric': 0.7722590521446594, 'eval_loss': 0.14912612736225128, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8802129547471162, 'eval_AUC-ROC': 0.9589468942440134, 'eval_Precision': 0.76052944188729, 'eval_Recall': 0.7119211833907053, 'eval_F1-score': 0.7318655197216989, 'eval_runtime': 3.658, 'eval_samples_per_second': 616.179, 'eval_steps_per_second': 19.409}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/5x17bw96\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1992331/994277692.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/5x17bw96: {'eval_mcc_metric': 0.732940156486814, 'eval_loss': 0.1951296329498291, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8589174800354925, 'eval_AUC-ROC': 0.9232400794964539, 'eval_Precision': 0.6351294160479245, 'eval_Recall': 0.6204615992745854, 'eval_F1-score': 0.6239080423384326, 'eval_runtime': 3.5885, 'eval_samples_per_second': 628.116, 'eval_steps_per_second': 19.785}\n",
      "Evaluating model: ./models_Mtr_5_Chemberta/j92alz30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [71/71 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mtr_5_Chemberta/j92alz30: {'eval_mcc_metric': 0.5544837921085852, 'eval_loss': 0.31705886125564575, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.782608695652174, 'eval_AUC-ROC': 0.8121271977935832, 'eval_Precision': 0.586801825020637, 'eval_Recall': 0.46280637664456503, 'eval_F1-score': 0.49256044259750764, 'eval_runtime': 3.5959, 'eval_samples_per_second': 626.832, 'eval_steps_per_second': 19.745}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel  \n",
    "import os\n",
    "\n",
    "models_dir = \"./models_Mtr_5_Chemberta\"\n",
    "\n",
    "for ckpt in os.listdir(models_dir):\n",
    "    model_path = os.path.join(models_dir, ckpt)\n",
    "    \n",
    "    if os.path.isdir(model_path):\n",
    "        adapter_config_path = os.path.join(model_path, \"adapter_config.json\")\n",
    "        \n",
    "        if not os.path.exists(adapter_config_path):\n",
    "            print(f\"⚠️ Missing 'adapter_config.json' in {model_path}\")\n",
    "\n",
    "# Evaluate each saved model\n",
    "\n",
    "valid_checkpoints = [\n",
    "    os.path.join(models_dir, ckpt)\n",
    "    for ckpt in os.listdir(models_dir)\n",
    "    if os.path.isdir(os.path.join(models_dir, ckpt)) and \n",
    "       os.path.exists(os.path.join(models_dir, ckpt, \"adapter_config.json\"))\n",
    "]\n",
    "\n",
    "print(\"Valid Model Checkpoints:\", valid_checkpoints)\n",
    "\n",
    "for model_path in valid_checkpoints:\n",
    "    print(f\"Evaluating model: {model_path}\")\n",
    "\n",
    "    adapter_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    adapter_model.eval()\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=adapter_model,\n",
    "        args=training_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    test_results_clin = trainer.evaluate()\n",
    "    print(f\"Test Results for {model_path}: {test_results_clin}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for checkpoints of 5M MTR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel  \n",
    "\n",
    "adapter_model = PeftModel.from_pretrained(base_model, \"/home/raghvendra2/Molformer_Finetuning/models_Mtr_5_Chemberta/ndgq1b2l/checkpoint-1645\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1992331/3341849556.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "        model=adapter_model,\n",
    "        args=training_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4/71 00:00 < 00:04, 14.12 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for checkpoint 1645 of sj3yydqn: {'eval_mcc_metric': 0.7810350014174989, 'eval_loss': 0.1457151621580124, 'eval_model_preparation_time': 0.0043, 'eval_Accuracy': 0.8846495119787046, 'eval_AUC-ROC': 0.9527411756627775, 'eval_Precision': 0.7988920909771744, 'eval_Recall': 0.7210630577180025, 'eval_F1-score': 0.7486025273539199, 'eval_runtime': 4.7489, 'eval_samples_per_second': 474.638, 'eval_steps_per_second': 14.951}\n"
     ]
    }
   ],
   "source": [
    "test_results_flavor = trainer.evaluate()\n",
    "\n",
    "print(\"Test Results for checkpoint 1645 of sj3yydqn:\", test_results_flavor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10MLM Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-77M-MTR\",\n",
    "    num_labels=5,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True,\n",
    "    \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Valid nested checkpoints found: ['./models_MTR100_Chemberta/ujye8pur/checkpoint-500', './models_MTR100_Chemberta/glzr8jim/checkpoint-1500', './models_MTR100_Chemberta/glzr8jim/checkpoint-1000', './models_MTR100_Chemberta/glzr8jim/checkpoint-500', './models_MTR100_Chemberta/glzr8jim/checkpoint-1645', './models_MTR100_Chemberta/y6ioij00/checkpoint-1500', './models_MTR100_Chemberta/y6ioij00/checkpoint-1000', './models_MTR100_Chemberta/y6ioij00/checkpoint-500', './models_MTR100_Chemberta/y6ioij00/checkpoint-1645', './models_MTR100_Chemberta/5k0m1ybg/checkpoint-1500', './models_MTR100_Chemberta/5k0m1ybg/checkpoint-1000', './models_MTR100_Chemberta/5k0m1ybg/checkpoint-500', './models_MTR100_Chemberta/5k0m1ybg/checkpoint-1645', './models_MTR100_Chemberta/imo6nm9x/checkpoint-1500', './models_MTR100_Chemberta/imo6nm9x/checkpoint-1000', './models_MTR100_Chemberta/imo6nm9x/checkpoint-500', './models_MTR100_Chemberta/imo6nm9x/checkpoint-1645', './models_MTR100_Chemberta/to3uvb1h/checkpoint-1500', './models_MTR100_Chemberta/to3uvb1h/checkpoint-1000', './models_MTR100_Chemberta/to3uvb1h/checkpoint-500', './models_MTR100_Chemberta/to3uvb1h/checkpoint-1645', './models_MTR100_Chemberta/oggmqthu/checkpoint-1500', './models_MTR100_Chemberta/oggmqthu/checkpoint-1000', './models_MTR100_Chemberta/oggmqthu/checkpoint-500', './models_MTR100_Chemberta/oggmqthu/checkpoint-1645', './models_MTR100_Chemberta/vx2zkfhq/checkpoint-500', './models_MTR100_Chemberta/tl30zdlb/checkpoint-1000', './models_MTR100_Chemberta/tl30zdlb/checkpoint-500', './models_MTR100_Chemberta/t6motadp/checkpoint-1500', './models_MTR100_Chemberta/t6motadp/checkpoint-1000', './models_MTR100_Chemberta/t6motadp/checkpoint-500', './models_MTR100_Chemberta/t6motadp/checkpoint-1645']\n",
      "\n",
      "🔍 Evaluating model: ujye8pur/checkpoint-500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for ujye8pur/checkpoint-500: {'eval_mcc_metric': 0.19421155282376865, 'eval_loss': 0.9777860045433044, 'eval_Accuracy': 0.48535936113575867, 'eval_AUC-ROC': 0.6205852690220697, 'eval_Precision': 0.2693402384519366, 'eval_Recall': 0.32627241350001607, 'eval_F1-score': 0.236805625235501, 'eval_runtime': 2.9214, 'eval_samples_per_second': 771.552, 'eval_steps_per_second': 6.161}\n",
      "\n",
      "🔍 Evaluating model: glzr8jim/checkpoint-1500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for glzr8jim/checkpoint-1500: {'eval_mcc_metric': 0.7834343375170396, 'eval_loss': 0.13969959318637848, 'eval_Accuracy': 0.8855368234250222, 'eval_AUC-ROC': 0.9549969661367305, 'eval_Precision': 0.7988102389221967, 'eval_Recall': 0.7907065709696487, 'eval_F1-score': 0.7944742733644798, 'eval_runtime': 3.3104, 'eval_samples_per_second': 680.883, 'eval_steps_per_second': 5.437}\n",
      "\n",
      "🔍 Evaluating model: glzr8jim/checkpoint-1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for glzr8jim/checkpoint-1000: {'eval_mcc_metric': 0.7561777901437662, 'eval_loss': 0.157387375831604, 'eval_Accuracy': 0.8731144631765749, 'eval_AUC-ROC': 0.9607157091940854, 'eval_Precision': 0.7586056774144084, 'eval_Recall': 0.6631932680711806, 'eval_F1-score': 0.6919552713312738, 'eval_runtime': 2.9928, 'eval_samples_per_second': 753.14, 'eval_steps_per_second': 6.014}\n",
      "\n",
      "🔍 Evaluating model: glzr8jim/checkpoint-500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for glzr8jim/checkpoint-500: {'eval_mcc_metric': 0.7502513988290391, 'eval_loss': 0.18352656066417694, 'eval_Accuracy': 0.865128660159716, 'eval_AUC-ROC': 0.9459783526571478, 'eval_Precision': 0.732178217647446, 'eval_Recall': 0.6760370566249214, 'eval_F1-score': 0.6856137635277199, 'eval_runtime': 2.9859, 'eval_samples_per_second': 754.879, 'eval_steps_per_second': 6.028}\n",
      "\n",
      "🔍 Evaluating model: glzr8jim/checkpoint-1645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for glzr8jim/checkpoint-1645: {'eval_mcc_metric': 0.7857732041076384, 'eval_loss': 0.13933634757995605, 'eval_Accuracy': 0.8864241348713399, 'eval_AUC-ROC': 0.9560709660479298, 'eval_Precision': 0.80451841570831, 'eval_Recall': 0.7921601853640271, 'eval_F1-score': 0.7971128505954014, 'eval_runtime': 3.3059, 'eval_samples_per_second': 681.806, 'eval_steps_per_second': 5.445}\n",
      "\n",
      "🔍 Evaluating model: y6ioij00/checkpoint-1500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for y6ioij00/checkpoint-1500: {'eval_mcc_metric': 0.7777237717003995, 'eval_loss': 0.13908487558364868, 'eval_Accuracy': 0.8824312333629104, 'eval_AUC-ROC': 0.9680355972198766, 'eval_Precision': 0.8615301215900842, 'eval_Recall': 0.755471099116446, 'eval_F1-score': 0.7915827963618229, 'eval_runtime': 2.995, 'eval_samples_per_second': 752.576, 'eval_steps_per_second': 6.01}\n",
      "\n",
      "🔍 Evaluating model: y6ioij00/checkpoint-1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for y6ioij00/checkpoint-1000: {'eval_mcc_metric': 0.7412615894514512, 'eval_loss': 0.15623323619365692, 'eval_Accuracy': 0.8664596273291926, 'eval_AUC-ROC': 0.9657289955449591, 'eval_Precision': 0.8556133179258886, 'eval_Recall': 0.6498670110123272, 'eval_F1-score': 0.6913256232325058, 'eval_runtime': 2.9885, 'eval_samples_per_second': 754.219, 'eval_steps_per_second': 6.023}\n",
      "\n",
      "🔍 Evaluating model: y6ioij00/checkpoint-500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for y6ioij00/checkpoint-500: {'eval_mcc_metric': 0.7432949433893207, 'eval_loss': 0.17865656316280365, 'eval_Accuracy': 0.8615794143744454, 'eval_AUC-ROC': 0.9521815768854778, 'eval_Precision': 0.6944408816073097, 'eval_Recall': 0.6691210670043418, 'eval_F1-score': 0.6751547117438224, 'eval_runtime': 2.9818, 'eval_samples_per_second': 755.93, 'eval_steps_per_second': 6.037}\n",
      "\n",
      "🔍 Evaluating model: y6ioij00/checkpoint-1645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for y6ioij00/checkpoint-1645: {'eval_mcc_metric': 0.7799976714498148, 'eval_loss': 0.1390303522348404, 'eval_Accuracy': 0.883318544809228, 'eval_AUC-ROC': 0.9668998755927947, 'eval_Precision': 0.8650642614069216, 'eval_Recall': 0.7556980372899021, 'eval_F1-score': 0.7927498574132986, 'eval_runtime': 2.9755, 'eval_samples_per_second': 757.529, 'eval_steps_per_second': 6.049}\n",
      "\n",
      "🔍 Evaluating model: 5k0m1ybg/checkpoint-1500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for 5k0m1ybg/checkpoint-1500: {'eval_mcc_metric': 0.7861823500814078, 'eval_loss': 0.1503613293170929, 'eval_Accuracy': 0.8868677905944987, 'eval_AUC-ROC': 0.9555982712852101, 'eval_Precision': 0.7837498821907072, 'eval_Recall': 0.7571685483823567, 'eval_F1-score': 0.7692619624826265, 'eval_runtime': 3.2681, 'eval_samples_per_second': 689.702, 'eval_steps_per_second': 5.508}\n",
      "\n",
      "🔍 Evaluating model: 5k0m1ybg/checkpoint-1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for 5k0m1ybg/checkpoint-1000: {'eval_mcc_metric': 0.7528682456747839, 'eval_loss': 0.1641027331352234, 'eval_Accuracy': 0.8717834960070985, 'eval_AUC-ROC': 0.9484502428965049, 'eval_Precision': 0.8560778135534199, 'eval_Recall': 0.6621800926560468, 'eval_F1-score': 0.6980971135064987, 'eval_runtime': 2.982, 'eval_samples_per_second': 755.863, 'eval_steps_per_second': 6.036}\n",
      "\n",
      "🔍 Evaluating model: 5k0m1ybg/checkpoint-500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for 5k0m1ybg/checkpoint-500: {'eval_mcc_metric': 0.7536075650181511, 'eval_loss': 0.18033510446548462, 'eval_Accuracy': 0.867790594498669, 'eval_AUC-ROC': 0.9435619422200482, 'eval_Precision': 0.749063591964226, 'eval_Recall': 0.6726338501362976, 'eval_F1-score': 0.6904755347758996, 'eval_runtime': 2.9422, 'eval_samples_per_second': 766.1, 'eval_steps_per_second': 6.118}\n",
      "\n",
      "🔍 Evaluating model: 5k0m1ybg/checkpoint-1645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for 5k0m1ybg/checkpoint-1645: {'eval_mcc_metric': 0.7947615088853518, 'eval_loss': 0.14564938843250275, 'eval_Accuracy': 0.8913043478260869, 'eval_AUC-ROC': 0.9556414960002446, 'eval_Precision': 0.7784190145841964, 'eval_Recall': 0.763899668305314, 'eval_F1-score': 0.7692187918551457, 'eval_runtime': 2.951, 'eval_samples_per_second': 763.808, 'eval_steps_per_second': 6.1}\n",
      "\n",
      "🔍 Evaluating model: imo6nm9x/checkpoint-1500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for imo6nm9x/checkpoint-1500: {'eval_mcc_metric': 0.7818169758422309, 'eval_loss': 0.1437530368566513, 'eval_Accuracy': 0.8850931677018633, 'eval_AUC-ROC': 0.9721605705286832, 'eval_Precision': 0.8250201990356656, 'eval_Recall': 0.7875592578624443, 'eval_F1-score': 0.8045247463831717, 'eval_runtime': 3.2846, 'eval_samples_per_second': 686.23, 'eval_steps_per_second': 5.48}\n",
      "\n",
      "🔍 Evaluating model: imo6nm9x/checkpoint-1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for imo6nm9x/checkpoint-1000: {'eval_mcc_metric': 0.7505830110360489, 'eval_loss': 0.16083496809005737, 'eval_Accuracy': 0.870452528837622, 'eval_AUC-ROC': 0.9649679384016391, 'eval_Precision': 0.8573939402955351, 'eval_Recall': 0.6589279419307597, 'eval_F1-score': 0.6970335804335622, 'eval_runtime': 2.9609, 'eval_samples_per_second': 761.255, 'eval_steps_per_second': 6.079}\n",
      "\n",
      "🔍 Evaluating model: imo6nm9x/checkpoint-500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for imo6nm9x/checkpoint-500: {'eval_mcc_metric': 0.7526104369480295, 'eval_loss': 0.1792353391647339, 'eval_Accuracy': 0.867790594498669, 'eval_AUC-ROC': 0.9531487492655296, 'eval_Precision': 0.6839922258902702, 'eval_Recall': 0.6768026837451615, 'eval_F1-score': 0.6792147873649024, 'eval_runtime': 3.2876, 'eval_samples_per_second': 685.599, 'eval_steps_per_second': 5.475}\n",
      "\n",
      "🔍 Evaluating model: imo6nm9x/checkpoint-1645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for imo6nm9x/checkpoint-1645: {'eval_mcc_metric': 0.7882497847517572, 'eval_loss': 0.14135079085826874, 'eval_Accuracy': 0.8873114463176575, 'eval_AUC-ROC': 0.9720325211171271, 'eval_Precision': 0.8327510094500526, 'eval_Recall': 0.8274682544764185, 'eval_F1-score': 0.8297152462405265, 'eval_runtime': 2.9562, 'eval_samples_per_second': 762.455, 'eval_steps_per_second': 6.089}\n",
      "\n",
      "🔍 Evaluating model: to3uvb1h/checkpoint-1500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for to3uvb1h/checkpoint-1500: {'eval_mcc_metric': 0.777782885687969, 'eval_loss': 0.1434350311756134, 'eval_Accuracy': 0.883318544809228, 'eval_AUC-ROC': 0.9714154098726826, 'eval_Precision': 0.8247383778991484, 'eval_Recall': 0.7833793596443682, 'eval_F1-score': 0.8022087198646425, 'eval_runtime': 3.3207, 'eval_samples_per_second': 678.777, 'eval_steps_per_second': 5.421}\n",
      "\n",
      "🔍 Evaluating model: to3uvb1h/checkpoint-1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for to3uvb1h/checkpoint-1000: {'eval_mcc_metric': 0.7526312450898183, 'eval_loss': 0.15572887659072876, 'eval_Accuracy': 0.8717834960070985, 'eval_AUC-ROC': 0.9603426928577126, 'eval_Precision': 0.7568937122921888, 'eval_Recall': 0.6577760954882745, 'eval_F1-score': 0.6888927540741175, 'eval_runtime': 2.9924, 'eval_samples_per_second': 753.238, 'eval_steps_per_second': 6.015}\n",
      "\n",
      "🔍 Evaluating model: to3uvb1h/checkpoint-500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for to3uvb1h/checkpoint-500: {'eval_mcc_metric': 0.7433824366848646, 'eval_loss': 0.18622320890426636, 'eval_Accuracy': 0.8624667258207631, 'eval_AUC-ROC': 0.9491061139912048, 'eval_Precision': 0.6958470209767611, 'eval_Recall': 0.6681240824198336, 'eval_F1-score': 0.6759382185055587, 'eval_runtime': 2.9908, 'eval_samples_per_second': 753.656, 'eval_steps_per_second': 6.019}\n",
      "\n",
      "🔍 Evaluating model: to3uvb1h/checkpoint-1645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for to3uvb1h/checkpoint-1645: {'eval_mcc_metric': 0.7893589818962506, 'eval_loss': 0.14277541637420654, 'eval_Accuracy': 0.888642413487134, 'eval_AUC-ROC': 0.970534896759912, 'eval_Precision': 0.8309968335234739, 'eval_Recall': 0.7920133444927788, 'eval_F1-score': 0.8095269496517847, 'eval_runtime': 3.3192, 'eval_samples_per_second': 679.087, 'eval_steps_per_second': 5.423}\n",
      "\n",
      "🔍 Evaluating model: oggmqthu/checkpoint-1500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for oggmqthu/checkpoint-1500: {'eval_mcc_metric': 0.7646476032969329, 'eval_loss': 0.15173858404159546, 'eval_Accuracy': 0.8748890860692103, 'eval_AUC-ROC': 0.9539073098699481, 'eval_Precision': 0.7699535609880498, 'eval_Recall': 0.7440526568986802, 'eval_F1-score': 0.7558138359370141, 'eval_runtime': 2.9863, 'eval_samples_per_second': 754.787, 'eval_steps_per_second': 6.028}\n",
      "\n",
      "🔍 Evaluating model: oggmqthu/checkpoint-1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for oggmqthu/checkpoint-1000: {'eval_mcc_metric': 0.7385925481100566, 'eval_loss': 0.17853213846683502, 'eval_Accuracy': 0.8620230700976043, 'eval_AUC-ROC': 0.952035520418581, 'eval_Precision': 0.738291124479222, 'eval_Recall': 0.6535935188237787, 'eval_F1-score': 0.677905482451869, 'eval_runtime': 2.984, 'eval_samples_per_second': 755.37, 'eval_steps_per_second': 6.032}\n",
      "\n",
      "🔍 Evaluating model: oggmqthu/checkpoint-500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for oggmqthu/checkpoint-500: {'eval_mcc_metric': 0.7426713014278017, 'eval_loss': 0.20437933504581451, 'eval_Accuracy': 0.8615794143744454, 'eval_AUC-ROC': 0.944137863291773, 'eval_Precision': 0.7301470060169304, 'eval_Recall': 0.669373528503963, 'eval_F1-score': 0.6826231496184025, 'eval_runtime': 3.3044, 'eval_samples_per_second': 682.111, 'eval_steps_per_second': 5.447}\n",
      "\n",
      "🔍 Evaluating model: oggmqthu/checkpoint-1645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for oggmqthu/checkpoint-1645: {'eval_mcc_metric': 0.7667172633940723, 'eval_loss': 0.14829595386981964, 'eval_Accuracy': 0.8757763975155279, 'eval_AUC-ROC': 0.9531141891298681, 'eval_Precision': 0.7546311912638144, 'eval_Recall': 0.746212206833805, 'eval_F1-score': 0.7496316603246458, 'eval_runtime': 2.9929, 'eval_samples_per_second': 753.106, 'eval_steps_per_second': 6.014}\n",
      "\n",
      "🔍 Evaluating model: vx2zkfhq/checkpoint-500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for vx2zkfhq/checkpoint-500: {'eval_mcc_metric': 0.7460191561959748, 'eval_loss': 0.18280988931655884, 'eval_Accuracy': 0.8646850044365573, 'eval_AUC-ROC': 0.9526404638451791, 'eval_Precision': 0.6994605300786482, 'eval_Recall': 0.666295517711146, 'eval_F1-score': 0.6761989540170544, 'eval_runtime': 2.9386, 'eval_samples_per_second': 767.039, 'eval_steps_per_second': 6.125}\n",
      "\n",
      "🔍 Evaluating model: tl30zdlb/checkpoint-1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for tl30zdlb/checkpoint-1000: {'eval_mcc_metric': 0.7353863792389076, 'eval_loss': 0.15578249096870422, 'eval_Accuracy': 0.8637976929902396, 'eval_AUC-ROC': 0.9687349144078918, 'eval_Precision': 0.8553351091006247, 'eval_Recall': 0.6467957836508158, 'eval_F1-score': 0.6888694618672135, 'eval_runtime': 2.9402, 'eval_samples_per_second': 766.625, 'eval_steps_per_second': 6.122}\n",
      "\n",
      "🔍 Evaluating model: tl30zdlb/checkpoint-500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for tl30zdlb/checkpoint-500: {'eval_mcc_metric': 0.7628025408739891, 'eval_loss': 0.17303045094013214, 'eval_Accuracy': 0.8708961845607809, 'eval_AUC-ROC': 0.9574226925519712, 'eval_Precision': 0.6897947167167306, 'eval_Recall': 0.6845226184087301, 'eval_F1-score': 0.6832200096471022, 'eval_runtime': 2.9382, 'eval_samples_per_second': 767.123, 'eval_steps_per_second': 6.126}\n",
      "\n",
      "🔍 Evaluating model: t6motadp/checkpoint-1500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for t6motadp/checkpoint-1500: {'eval_mcc_metric': 0.776442822237734, 'eval_loss': 0.14893807470798492, 'eval_Accuracy': 0.8819875776397516, 'eval_AUC-ROC': 0.9697350251900344, 'eval_Precision': 0.7593816467773177, 'eval_Recall': 0.6838207946505889, 'eval_F1-score': 0.7042818114602781, 'eval_runtime': 2.9415, 'eval_samples_per_second': 766.275, 'eval_steps_per_second': 6.119}\n",
      "\n",
      "🔍 Evaluating model: t6motadp/checkpoint-1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for t6motadp/checkpoint-1000: {'eval_mcc_metric': 0.7434281295722228, 'eval_loss': 0.16649644076824188, 'eval_Accuracy': 0.8669032830523514, 'eval_AUC-ROC': 0.9327607881911548, 'eval_Precision': 0.7533437807611343, 'eval_Recall': 0.6527979285484278, 'eval_F1-score': 0.6835916064247971, 'eval_runtime': 2.9347, 'eval_samples_per_second': 768.064, 'eval_steps_per_second': 6.134}\n",
      "\n",
      "🔍 Evaluating model: t6motadp/checkpoint-500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for t6motadp/checkpoint-500: {'eval_mcc_metric': 0.7212500930895972, 'eval_loss': 0.22483740746974945, 'eval_Accuracy': 0.8527062999112689, 'eval_AUC-ROC': 0.9137968485924197, 'eval_Precision': 0.6934293181375896, 'eval_Recall': 0.6475057130933363, 'eval_F1-score': 0.6638213090494984, 'eval_runtime': 2.9338, 'eval_samples_per_second': 768.275, 'eval_steps_per_second': 6.135}\n",
      "\n",
      "🔍 Evaluating model: t6motadp/checkpoint-1645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for t6motadp/checkpoint-1645: {'eval_mcc_metric': 0.7816993453854211, 'eval_loss': 0.14186696708202362, 'eval_Accuracy': 0.8846495119787046, 'eval_AUC-ROC': 0.9697139680969613, 'eval_Precision': 0.7635851304227466, 'eval_Recall': 0.686487387957943, 'eval_F1-score': 0.7073148113378649, 'eval_runtime': 2.9284, 'eval_samples_per_second': 769.709, 'eval_steps_per_second': 6.147}\n",
      "Valid Model Checkpoints: ['./models_MTR100_Chemberta/glzr8jim', './models_MTR100_Chemberta/y6ioij00', './models_MTR100_Chemberta/imo6nm9x', './models_MTR100_Chemberta/to3uvb1h', './models_MTR100_Chemberta/oggmqthu']\n",
      "Evaluating model: ./models_MTR100_Chemberta/glzr8jim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR100_Chemberta/glzr8jim: {'eval_mcc_metric': 0.7857732041076384, 'eval_loss': 0.13933634757995605, 'eval_Accuracy': 0.8864241348713399, 'eval_AUC-ROC': 0.9560709660479298, 'eval_Precision': 0.80451841570831, 'eval_Recall': 0.7921601853640271, 'eval_F1-score': 0.7971128505954014, 'eval_runtime': 2.9807, 'eval_samples_per_second': 756.188, 'eval_steps_per_second': 6.039}\n",
      "Evaluating model: ./models_MTR100_Chemberta/y6ioij00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR100_Chemberta/y6ioij00: {'eval_mcc_metric': 0.7777237717003995, 'eval_loss': 0.13908487558364868, 'eval_Accuracy': 0.8824312333629104, 'eval_AUC-ROC': 0.9680355972198766, 'eval_Precision': 0.8615301215900842, 'eval_Recall': 0.755471099116446, 'eval_F1-score': 0.7915827963618229, 'eval_runtime': 3.3141, 'eval_samples_per_second': 680.133, 'eval_steps_per_second': 5.431}\n",
      "Evaluating model: ./models_MTR100_Chemberta/imo6nm9x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR100_Chemberta/imo6nm9x: {'eval_mcc_metric': 0.7818169758422309, 'eval_loss': 0.1437530368566513, 'eval_Accuracy': 0.8850931677018633, 'eval_AUC-ROC': 0.9721605705286832, 'eval_Precision': 0.8250201990356656, 'eval_Recall': 0.7875592578624443, 'eval_F1-score': 0.8045247463831717, 'eval_runtime': 2.9361, 'eval_samples_per_second': 767.688, 'eval_steps_per_second': 6.131}\n",
      "Evaluating model: ./models_MTR100_Chemberta/to3uvb1h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR100_Chemberta/to3uvb1h: {'eval_mcc_metric': 0.777782885687969, 'eval_loss': 0.1434350311756134, 'eval_Accuracy': 0.883318544809228, 'eval_AUC-ROC': 0.9714154098726826, 'eval_Precision': 0.8247383778991484, 'eval_Recall': 0.7833793596443682, 'eval_F1-score': 0.8022087198646425, 'eval_runtime': 2.9953, 'eval_samples_per_second': 752.507, 'eval_steps_per_second': 6.009}\n",
      "Evaluating model: ./models_MTR100_Chemberta/oggmqthu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3224754749.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_MTR100_Chemberta/oggmqthu: {'eval_mcc_metric': 0.7667172633940723, 'eval_loss': 0.14829595386981964, 'eval_Accuracy': 0.8757763975155279, 'eval_AUC-ROC': 0.9531141891298681, 'eval_Precision': 0.7546311912638144, 'eval_Recall': 0.746212206833805, 'eval_F1-score': 0.7496316603246458, 'eval_runtime': 3.3239, 'eval_samples_per_second': 678.117, 'eval_steps_per_second': 5.415}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "for ckpt in os.listdir(models_dir):import os\n",
    "from peft import PeftModel\n",
    "\n",
    "models_dir = \"./models_MTR100_Chemberta\"\n",
    "\n",
    "def find_all_checkpoints(base_dir):\n",
    "    all_checkpoints = []\n",
    "    for folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            # Now look inside this folder for checkpoint-* subdirs\n",
    "            for subfolder in os.listdir(folder_path):\n",
    "                subfolder_path = os.path.join(folder_path, subfolder)\n",
    "                if os.path.isdir(subfolder_path) and subfolder.startswith(\"checkpoint-\"):\n",
    "                    # Check for adapter_config.json\n",
    "                    if os.path.exists(os.path.join(subfolder_path, \"adapter_config.json\")):\n",
    "                        all_checkpoints.append(subfolder_path)\n",
    "    return all_checkpoints\n",
    "\n",
    "valid_checkpoints = find_all_checkpoints(models_dir)\n",
    "print(\"🧠 Valid nested checkpoints found:\", valid_checkpoints)\n",
    "\n",
    "for checkpoint_path in valid_checkpoints:\n",
    "    checkpoint_name = os.path.basename(checkpoint_path)\n",
    "    parent_folder = os.path.basename(os.path.dirname(checkpoint_path))\n",
    "\n",
    "    print(f\"\\n🔍 Evaluating model: {parent_folder}/{checkpoint_name}\")\n",
    "\n",
    "    adapter_model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    adapter_model.eval()\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=adapter_model,\n",
    "        args=training_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    test_results_clin = trainer.evaluate()\n",
    "\n",
    "    print(f\"📌 Test Results for {parent_folder}/{checkpoint_name}: {test_results_clin}\")\n",
    "\n",
    "    model_path = os.path.join(models_dir, ckpt)\n",
    "    \n",
    "    if os.path.isdir(model_path):\n",
    "        adapter_config_path = os.path.join(model_path, \"adapter_config.json\")\n",
    "        \n",
    "        if not os.path.exists(adapter_config_path):\n",
    "            print(f\"⚠️ Missing 'adapter_config.json' in {model_path}\")\n",
    "\n",
    "# Evaluate each saved model\n",
    "\n",
    "valid_checkpoints = [\n",
    "    os.path.join(models_dir, ckpt)\n",
    "    for ckpt in os.listdir(models_dir)\n",
    "    if os.path.isdir(os.path.join(models_dir, ckpt)) and \n",
    "       os.path.exists(os.path.join(models_dir, ckpt, \"adapter_config.json\"))\n",
    "]\n",
    "\n",
    "print(\"Valid Model Checkpoints:\", valid_checkpoints)\n",
    "\n",
    "for model_path in valid_checkpoints:\n",
    "    print(f\"Evaluating model: {model_path}\")\n",
    "\n",
    "    adapter_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    adapter_model.eval()\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=adapter_model,\n",
    "        args=training_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    test_results_clin = trainer.evaluate()\n",
    "    print(f\"Test Results for {model_path}: {test_results_clin}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 77M MLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-77M-MLM\",\n",
    "    num_labels=5,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True,\n",
    "    \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_results_flavor\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    report_to=\"none\",  # Disable logging to W&B for test\n",
    "    \n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Valid nested checkpoints found: ['./models_Mlm_100_Chemberta/xutmnj12/checkpoint-1500', './models_Mlm_100_Chemberta/xutmnj12/checkpoint-1000', './models_Mlm_100_Chemberta/xutmnj12/checkpoint-500', './models_Mlm_100_Chemberta/xutmnj12/checkpoint-1645', './models_Mlm_100_Chemberta/sz247wpr/checkpoint-658', './models_Mlm_100_Chemberta/sz247wpr/checkpoint-500', './models_Mlm_100_Chemberta/ikd7q7fk/checkpoint-658', './models_Mlm_100_Chemberta/ikd7q7fk/checkpoint-500', './models_Mlm_100_Chemberta/kovlgv1s/checkpoint-1500', './models_Mlm_100_Chemberta/kovlgv1s/checkpoint-1000', './models_Mlm_100_Chemberta/kovlgv1s/checkpoint-500', './models_Mlm_100_Chemberta/kovlgv1s/checkpoint-1645', './models_Mlm_100_Chemberta/k0sfrn0p/checkpoint-1500', './models_Mlm_100_Chemberta/k0sfrn0p/checkpoint-1000', './models_Mlm_100_Chemberta/k0sfrn0p/checkpoint-500', './models_Mlm_100_Chemberta/k0sfrn0p/checkpoint-1645', './models_Mlm_100_Chemberta/0yvjid5t/checkpoint-658', './models_Mlm_100_Chemberta/0yvjid5t/checkpoint-500', './models_Mlm_100_Chemberta/d1wrlvdu/checkpoint-500', './models_Mlm_100_Chemberta/d6a28ks6/checkpoint-658', './models_Mlm_100_Chemberta/d6a28ks6/checkpoint-500', './models_Mlm_100_Chemberta/3fdjf7s7/checkpoint-658', './models_Mlm_100_Chemberta/3fdjf7s7/checkpoint-500', './models_Mlm_100_Chemberta/6a8xs6i0/checkpoint-500', './models_Mlm_100_Chemberta/a99qvoss/checkpoint-658', './models_Mlm_100_Chemberta/a99qvoss/checkpoint-500', './models_Mlm_100_Chemberta/q6ab3hnx/checkpoint-658', './models_Mlm_100_Chemberta/q6ab3hnx/checkpoint-500', './models_Mlm_100_Chemberta/jbxogsm2/checkpoint-1500', './models_Mlm_100_Chemberta/jbxogsm2/checkpoint-1000', './models_Mlm_100_Chemberta/jbxogsm2/checkpoint-500', './models_Mlm_100_Chemberta/gi5kecmi/checkpoint-658', './models_Mlm_100_Chemberta/gi5kecmi/checkpoint-500', './models_Mlm_100_Chemberta/u9d6q2lo/checkpoint-658', './models_Mlm_100_Chemberta/u9d6q2lo/checkpoint-500', './models_Mlm_100_Chemberta/wt5mgigm/checkpoint-1500', './models_Mlm_100_Chemberta/wt5mgigm/checkpoint-1000', './models_Mlm_100_Chemberta/wt5mgigm/checkpoint-500', './models_Mlm_100_Chemberta/wt5mgigm/checkpoint-1645', './models_Mlm_100_Chemberta/532smrg2/checkpoint-1000', './models_Mlm_100_Chemberta/532smrg2/checkpoint-500', './models_Mlm_100_Chemberta/gbyjoy6e/checkpoint-658', './models_Mlm_100_Chemberta/gbyjoy6e/checkpoint-500']\n",
      "\n",
      "🔍 Evaluating model: xutmnj12/checkpoint-1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for xutmnj12/checkpoint-1500: {'eval_mcc_metric': 0.7515927803916811, 'eval_loss': 0.19172461330890656, 'eval_Accuracy': 0.8700088731144632, 'eval_AUC-ROC': 0.9528106082436698, 'eval_Precision': 0.7540780668366062, 'eval_Recall': 0.726382307268234, 'eval_F1-score': 0.7377056675422857, 'eval_runtime': 3.1689, 'eval_samples_per_second': 711.282, 'eval_steps_per_second': 5.68}\n",
      "\n",
      "🔍 Evaluating model: xutmnj12/checkpoint-1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for xutmnj12/checkpoint-1000: {'eval_mcc_metric': 0.7381556177411126, 'eval_loss': 0.20143990218639374, 'eval_Accuracy': 0.8655723158828749, 'eval_AUC-ROC': 0.9462037248705334, 'eval_Precision': 0.8562507949004768, 'eval_Recall': 0.6418617688488828, 'eval_F1-score': 0.6859070010245756, 'eval_runtime': 2.9426, 'eval_samples_per_second': 765.981, 'eval_steps_per_second': 6.117}\n",
      "\n",
      "🔍 Evaluating model: xutmnj12/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for xutmnj12/checkpoint-500: {'eval_mcc_metric': 0.7338536537715541, 'eval_loss': 0.20480243861675262, 'eval_Accuracy': 0.8584738243123337, 'eval_AUC-ROC': 0.9371736676552583, 'eval_Precision': 0.7093752379429545, 'eval_Recall': 0.7840665209955991, 'eval_F1-score': 0.7235717842777554, 'eval_runtime': 3.1783, 'eval_samples_per_second': 709.176, 'eval_steps_per_second': 5.663}\n",
      "\n",
      "🔍 Evaluating model: xutmnj12/checkpoint-1645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for xutmnj12/checkpoint-1645: {'eval_mcc_metric': 0.7521464915364129, 'eval_loss': 0.19328723847866058, 'eval_Accuracy': 0.870452528837622, 'eval_AUC-ROC': 0.9531784124231235, 'eval_Precision': 0.7572555794128578, 'eval_Recall': 0.7250657272294483, 'eval_F1-score': 0.7380485089709764, 'eval_runtime': 3.1824, 'eval_samples_per_second': 708.267, 'eval_steps_per_second': 5.656}\n",
      "\n",
      "🔍 Evaluating model: sz247wpr/checkpoint-658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for sz247wpr/checkpoint-658: {'eval_mcc_metric': 0.7233006450863193, 'eval_loss': 0.19794334471225739, 'eval_Accuracy': 0.8553682342502218, 'eval_AUC-ROC': 0.9324020126587597, 'eval_Precision': 0.8399824452915114, 'eval_Recall': 0.6400624603647049, 'eval_F1-score': 0.6758858845455952, 'eval_runtime': 2.9396, 'eval_samples_per_second': 766.783, 'eval_steps_per_second': 6.123}\n",
      "\n",
      "🔍 Evaluating model: sz247wpr/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for sz247wpr/checkpoint-500: {'eval_mcc_metric': 0.72430838982655, 'eval_loss': 0.2104184478521347, 'eval_Accuracy': 0.854924578527063, 'eval_AUC-ROC': 0.910169070347604, 'eval_Precision': 0.8260430948474914, 'eval_Recall': 0.6436679844119826, 'eval_F1-score': 0.6732625055831842, 'eval_runtime': 3.1797, 'eval_samples_per_second': 708.88, 'eval_steps_per_second': 5.661}\n",
      "\n",
      "🔍 Evaluating model: ikd7q7fk/checkpoint-658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for ikd7q7fk/checkpoint-658: {'eval_mcc_metric': 0.7419793246099474, 'eval_loss': 0.19249001145362854, 'eval_Accuracy': 0.865128660159716, 'eval_AUC-ROC': 0.9296432847797422, 'eval_Precision': 0.852755204025884, 'eval_Recall': 0.6866640621349751, 'eval_F1-score': 0.7300430956971153, 'eval_runtime': 3.1562, 'eval_samples_per_second': 714.16, 'eval_steps_per_second': 5.703}\n",
      "\n",
      "🔍 Evaluating model: ikd7q7fk/checkpoint-500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for ikd7q7fk/checkpoint-500: {'eval_mcc_metric': 0.7409555692039872, 'eval_loss': 0.19144411385059357, 'eval_Accuracy': 0.8646850044365573, 'eval_AUC-ROC': 0.9266973256544127, 'eval_Precision': 0.8465550895920702, 'eval_Recall': 0.688897911223715, 'eval_F1-score': 0.7312272209981807, 'eval_runtime': 2.9172, 'eval_samples_per_second': 772.668, 'eval_steps_per_second': 6.17}\n",
      "\n",
      "🔍 Evaluating model: kovlgv1s/checkpoint-1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for kovlgv1s/checkpoint-1500: {'eval_mcc_metric': 0.7342137564076765, 'eval_loss': 0.19446489214897156, 'eval_Accuracy': 0.860248447204969, 'eval_AUC-ROC': 0.9519202263025145, 'eval_Precision': 0.6878685077459297, 'eval_Recall': 0.7149259559251673, 'eval_F1-score': 0.6903306505006462, 'eval_runtime': 3.2187, 'eval_samples_per_second': 700.279, 'eval_steps_per_second': 5.592}\n",
      "\n",
      "🔍 Evaluating model: kovlgv1s/checkpoint-1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for kovlgv1s/checkpoint-1000: {'eval_mcc_metric': 0.7084513048537422, 'eval_loss': 0.20086570084095, 'eval_Accuracy': 0.8513753327417923, 'eval_AUC-ROC': 0.9494769024472159, 'eval_Precision': 0.7947958503476438, 'eval_Recall': 0.6820920079787726, 'eval_F1-score': 0.7249915354431786, 'eval_runtime': 2.9872, 'eval_samples_per_second': 754.547, 'eval_steps_per_second': 6.026}\n",
      "\n",
      "🔍 Evaluating model: kovlgv1s/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for kovlgv1s/checkpoint-500: {'eval_mcc_metric': 0.7423314990336061, 'eval_loss': 0.19883593916893005, 'eval_Accuracy': 0.8633540372670807, 'eval_AUC-ROC': 0.9242379470524774, 'eval_Precision': 0.6957648110549656, 'eval_Recall': 0.6932927716767097, 'eval_F1-score': 0.6908755589345399, 'eval_runtime': 3.223, 'eval_samples_per_second': 699.357, 'eval_steps_per_second': 5.585}\n",
      "\n",
      "🔍 Evaluating model: kovlgv1s/checkpoint-1645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for kovlgv1s/checkpoint-1645: {'eval_mcc_metric': 0.7320377251817916, 'eval_loss': 0.1969209909439087, 'eval_Accuracy': 0.8598047914818101, 'eval_AUC-ROC': 0.9525929438784001, 'eval_Precision': 0.6922687690070904, 'eval_Recall': 0.7125012345914886, 'eval_F1-score': 0.6906020488842198, 'eval_runtime': 2.985, 'eval_samples_per_second': 755.103, 'eval_steps_per_second': 6.03}\n",
      "\n",
      "🔍 Evaluating model: k0sfrn0p/checkpoint-1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for k0sfrn0p/checkpoint-1500: {'eval_mcc_metric': 0.745377240742711, 'eval_loss': 0.18867076933383942, 'eval_Accuracy': 0.8673469387755102, 'eval_AUC-ROC': 0.9240643387138823, 'eval_Precision': 0.7169286661753869, 'eval_Recall': 0.6532359392276617, 'eval_F1-score': 0.6757555231363336, 'eval_runtime': 3.2263, 'eval_samples_per_second': 698.629, 'eval_steps_per_second': 5.579}\n",
      "\n",
      "🔍 Evaluating model: k0sfrn0p/checkpoint-1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for k0sfrn0p/checkpoint-1000: {'eval_mcc_metric': 0.6296497063594445, 'eval_loss': 0.22113291919231415, 'eval_Accuracy': 0.8149955634427685, 'eval_AUC-ROC': 0.874255501709448, 'eval_Precision': 0.6065668467631984, 'eval_Recall': 0.518895796382332, 'eval_F1-score': 0.540227755940425, 'eval_runtime': 2.9818, 'eval_samples_per_second': 755.921, 'eval_steps_per_second': 6.037}\n",
      "\n",
      "🔍 Evaluating model: k0sfrn0p/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for k0sfrn0p/checkpoint-500: {'eval_mcc_metric': 0.7065710674113834, 'eval_loss': 0.2449447214603424, 'eval_Accuracy': 0.8438331854480923, 'eval_AUC-ROC': 0.88074611426543, 'eval_Precision': 0.7243193756222379, 'eval_Recall': 0.6292032897665498, 'eval_F1-score': 0.6424112873123978, 'eval_runtime': 3.2235, 'eval_samples_per_second': 699.25, 'eval_steps_per_second': 5.584}\n",
      "\n",
      "🔍 Evaluating model: k0sfrn0p/checkpoint-1645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for k0sfrn0p/checkpoint-1645: {'eval_mcc_metric': 0.7538564530911973, 'eval_loss': 0.1873597800731659, 'eval_Accuracy': 0.8695652173913043, 'eval_AUC-ROC': 0.9243576636357957, 'eval_Precision': 0.8489141840021978, 'eval_Recall': 0.6637955022600265, 'eval_F1-score': 0.6929673332064148, 'eval_runtime': 2.9879, 'eval_samples_per_second': 754.382, 'eval_steps_per_second': 6.024}\n",
      "\n",
      "🔍 Evaluating model: 0yvjid5t/checkpoint-658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for 0yvjid5t/checkpoint-658: {'eval_mcc_metric': 0.7327720416734685, 'eval_loss': 0.20283129811286926, 'eval_Accuracy': 0.8606921029281278, 'eval_AUC-ROC': 0.9453031939096954, 'eval_Precision': 0.8439377639193936, 'eval_Recall': 0.6475590019460279, 'eval_F1-score': 0.6836970646724254, 'eval_runtime': 2.9227, 'eval_samples_per_second': 771.212, 'eval_steps_per_second': 6.159}\n",
      "\n",
      "🔍 Evaluating model: 0yvjid5t/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for 0yvjid5t/checkpoint-500: {'eval_mcc_metric': 0.7316784674933826, 'eval_loss': 0.20233118534088135, 'eval_Accuracy': 0.8589174800354925, 'eval_AUC-ROC': 0.9223233605218709, 'eval_Precision': 0.6996524483783776, 'eval_Recall': 0.6491223952561614, 'eval_F1-score': 0.6678992357486079, 'eval_runtime': 3.1526, 'eval_samples_per_second': 714.957, 'eval_steps_per_second': 5.71}\n",
      "\n",
      "🔍 Evaluating model: d1wrlvdu/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for d1wrlvdu/checkpoint-500: {'eval_mcc_metric': 0.694985160868272, 'eval_loss': 0.25309643149375916, 'eval_Accuracy': 0.8389529724933452, 'eval_AUC-ROC': 0.8535039425802043, 'eval_Precision': 0.6059986950156162, 'eval_Recall': 0.5882115468648133, 'eval_F1-score': 0.5930071107005246, 'eval_runtime': 2.9107, 'eval_samples_per_second': 774.379, 'eval_steps_per_second': 6.184}\n",
      "\n",
      "🔍 Evaluating model: d6a28ks6/checkpoint-658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for d6a28ks6/checkpoint-658: {'eval_mcc_metric': 0.7219500064808291, 'eval_loss': 0.20311683416366577, 'eval_Accuracy': 0.8558118899733806, 'eval_AUC-ROC': 0.9445070946033788, 'eval_Precision': 0.8383799440884262, 'eval_Recall': 0.638232313462664, 'eval_F1-score': 0.6765530437087939, 'eval_runtime': 2.9839, 'eval_samples_per_second': 755.397, 'eval_steps_per_second': 6.032}\n",
      "\n",
      "🔍 Evaluating model: d6a28ks6/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for d6a28ks6/checkpoint-500: {'eval_mcc_metric': 0.7142658195288292, 'eval_loss': 0.2040163278579712, 'eval_Accuracy': 0.8504880212954747, 'eval_AUC-ROC': 0.9447150623847612, 'eval_Precision': 0.6930504488037323, 'eval_Recall': 0.6671395409479339, 'eval_F1-score': 0.6786870690645163, 'eval_runtime': 2.9781, 'eval_samples_per_second': 756.853, 'eval_steps_per_second': 6.044}\n",
      "\n",
      "🔍 Evaluating model: 3fdjf7s7/checkpoint-658\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for 3fdjf7s7/checkpoint-658: {'eval_mcc_metric': 0.7165065043347795, 'eval_loss': 0.2090887576341629, 'eval_Accuracy': 0.8513753327417923, 'eval_AUC-ROC': 0.9391260199982971, 'eval_Precision': 0.7388555397131931, 'eval_Recall': 0.6339252090059346, 'eval_F1-score': 0.6635488189383238, 'eval_runtime': 3.1771, 'eval_samples_per_second': 709.442, 'eval_steps_per_second': 5.665}\n",
      "\n",
      "🔍 Evaluating model: 3fdjf7s7/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for 3fdjf7s7/checkpoint-500: {'eval_mcc_metric': 0.7278679315371784, 'eval_loss': 0.20962630212306976, 'eval_Accuracy': 0.8553682342502218, 'eval_AUC-ROC': 0.9297219471707138, 'eval_Precision': 0.6818073034918186, 'eval_Recall': 0.6461433161519332, 'eval_F1-score': 0.6592594070258537, 'eval_runtime': 2.9439, 'eval_samples_per_second': 765.64, 'eval_steps_per_second': 6.114}\n",
      "\n",
      "🔍 Evaluating model: 6a8xs6i0/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for 6a8xs6i0/checkpoint-500: {'eval_mcc_metric': 0.742203637990562, 'eval_loss': 0.226716086268425, 'eval_Accuracy': 0.8606921029281278, 'eval_AUC-ROC': 0.9024235768169208, 'eval_Precision': 0.6410628327425308, 'eval_Recall': 0.624204679684316, 'eval_F1-score': 0.6253995330432411, 'eval_runtime': 3.2279, 'eval_samples_per_second': 698.282, 'eval_steps_per_second': 5.576}\n",
      "\n",
      "🔍 Evaluating model: a99qvoss/checkpoint-658\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for a99qvoss/checkpoint-658: {'eval_mcc_metric': 0.733985401320564, 'eval_loss': 0.19938620924949646, 'eval_Accuracy': 0.8606921029281278, 'eval_AUC-ROC': 0.9334784404298844, 'eval_Precision': 0.8489034919817877, 'eval_Recall': 0.6464780594862665, 'eval_F1-score': 0.6796664281542979, 'eval_runtime': 3.1519, 'eval_samples_per_second': 715.135, 'eval_steps_per_second': 5.711}\n",
      "\n",
      "🔍 Evaluating model: a99qvoss/checkpoint-500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for a99qvoss/checkpoint-500: {'eval_mcc_metric': 0.7353105575740809, 'eval_loss': 0.19767917692661285, 'eval_Accuracy': 0.8615794143744454, 'eval_AUC-ROC': 0.9288132989607399, 'eval_Precision': 0.848757178461699, 'eval_Recall': 0.6479163943793445, 'eval_F1-score': 0.6816673651939139, 'eval_runtime': 2.9138, 'eval_samples_per_second': 773.558, 'eval_steps_per_second': 6.177}\n",
      "\n",
      "🔍 Evaluating model: q6ab3hnx/checkpoint-658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for q6ab3hnx/checkpoint-658: {'eval_mcc_metric': 0.7463880380697544, 'eval_loss': 0.1934467852115631, 'eval_Accuracy': 0.8673469387755102, 'eval_AUC-ROC': 0.9471037509253183, 'eval_Precision': 0.8515282295264454, 'eval_Recall': 0.6568764134754077, 'eval_F1-score': 0.6912447962506661, 'eval_runtime': 3.1781, 'eval_samples_per_second': 709.232, 'eval_steps_per_second': 5.664}\n",
      "\n",
      "🔍 Evaluating model: q6ab3hnx/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for q6ab3hnx/checkpoint-500: {'eval_mcc_metric': 0.7407640141242543, 'eval_loss': 0.19885306060314178, 'eval_Accuracy': 0.862910381543922, 'eval_AUC-ROC': 0.932307721697087, 'eval_Precision': 0.8362694945279083, 'eval_Recall': 0.6581162563794549, 'eval_F1-score': 0.6866106391759275, 'eval_runtime': 3.181, 'eval_samples_per_second': 708.593, 'eval_steps_per_second': 5.659}\n",
      "\n",
      "🔍 Evaluating model: jbxogsm2/checkpoint-1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for jbxogsm2/checkpoint-1500: {'eval_mcc_metric': 0.7366823570456721, 'eval_loss': 0.19648517668247223, 'eval_Accuracy': 0.8620230700976043, 'eval_AUC-ROC': 0.944459296784161, 'eval_Precision': 0.7088419652290514, 'eval_Recall': 0.683373528264271, 'eval_F1-score': 0.6940615523780366, 'eval_runtime': 2.926, 'eval_samples_per_second': 770.327, 'eval_steps_per_second': 6.152}\n",
      "\n",
      "🔍 Evaluating model: jbxogsm2/checkpoint-1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for jbxogsm2/checkpoint-1000: {'eval_mcc_metric': 0.7249778769328371, 'eval_loss': 0.20493312180042267, 'eval_Accuracy': 0.8589174800354925, 'eval_AUC-ROC': 0.9430401702420589, 'eval_Precision': 0.7862552697219171, 'eval_Recall': 0.6689705797043175, 'eval_F1-score': 0.7130367229243604, 'eval_runtime': 3.1598, 'eval_samples_per_second': 713.331, 'eval_steps_per_second': 5.697}\n",
      "\n",
      "🔍 Evaluating model: jbxogsm2/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for jbxogsm2/checkpoint-500: {'eval_mcc_metric': 0.7455775650378911, 'eval_loss': 0.20152515172958374, 'eval_Accuracy': 0.8646850044365573, 'eval_AUC-ROC': 0.9270770721930675, 'eval_Precision': 0.6816616896853674, 'eval_Recall': 0.694608822153824, 'eval_F1-score': 0.6822061180937639, 'eval_runtime': 2.9205, 'eval_samples_per_second': 771.79, 'eval_steps_per_second': 6.163}\n",
      "\n",
      "🔍 Evaluating model: gi5kecmi/checkpoint-658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for gi5kecmi/checkpoint-658: {'eval_mcc_metric': 0.7416033240064804, 'eval_loss': 0.18928112089633942, 'eval_Accuracy': 0.8646850044365573, 'eval_AUC-ROC': 0.9494673914241305, 'eval_Precision': 0.7175521682937753, 'eval_Recall': 0.6539430110345903, 'eval_F1-score': 0.6754301456803111, 'eval_runtime': 3.165, 'eval_samples_per_second': 712.16, 'eval_steps_per_second': 5.687}\n",
      "\n",
      "🔍 Evaluating model: gi5kecmi/checkpoint-500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for gi5kecmi/checkpoint-500: {'eval_mcc_metric': 0.739163857433399, 'eval_loss': 0.19696369767189026, 'eval_Accuracy': 0.8620230700976043, 'eval_AUC-ROC': 0.9272159007751398, 'eval_Precision': 0.6702897839037956, 'eval_Recall': 0.6566129634462633, 'eval_F1-score': 0.6619341230160769, 'eval_runtime': 2.915, 'eval_samples_per_second': 773.249, 'eval_steps_per_second': 6.175}\n",
      "\n",
      "🔍 Evaluating model: u9d6q2lo/checkpoint-658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for u9d6q2lo/checkpoint-658: {'eval_mcc_metric': 0.724672108248464, 'eval_loss': 0.19758668541908264, 'eval_Accuracy': 0.8562555456965395, 'eval_AUC-ROC': 0.9213624695017598, 'eval_Precision': 0.6312109435891815, 'eval_Recall': 0.605718593996085, 'eval_F1-score': 0.6136877227016605, 'eval_runtime': 3.22, 'eval_samples_per_second': 699.996, 'eval_steps_per_second': 5.59}\n",
      "\n",
      "🔍 Evaluating model: u9d6q2lo/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for u9d6q2lo/checkpoint-500: {'eval_mcc_metric': 0.7167517251598591, 'eval_loss': 0.2149065136909485, 'eval_Accuracy': 0.8504880212954747, 'eval_AUC-ROC': 0.9020879677136187, 'eval_Precision': 0.722033614061327, 'eval_Recall': 0.6381268330734974, 'eval_F1-score': 0.658734379036996, 'eval_runtime': 2.989, 'eval_samples_per_second': 754.105, 'eval_steps_per_second': 6.022}\n",
      "\n",
      "🔍 Evaluating model: wt5mgigm/checkpoint-1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for wt5mgigm/checkpoint-1500: {'eval_mcc_metric': 0.7556197394027053, 'eval_loss': 0.18587109446525574, 'eval_Accuracy': 0.8708961845607809, 'eval_AUC-ROC': 0.9508821803277767, 'eval_Precision': 0.724016556454012, 'eval_Recall': 0.6982829700966494, 'eval_F1-score': 0.7098701024520846, 'eval_runtime': 3.2207, 'eval_samples_per_second': 699.839, 'eval_steps_per_second': 5.589}\n",
      "\n",
      "🔍 Evaluating model: wt5mgigm/checkpoint-1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for wt5mgigm/checkpoint-1000: {'eval_mcc_metric': 0.7148359404765081, 'eval_loss': 0.20748455822467804, 'eval_Accuracy': 0.8482697426796806, 'eval_AUC-ROC': 0.927417529009593, 'eval_Precision': 0.6617804071296437, 'eval_Recall': 0.6541302066798323, 'eval_F1-score': 0.6575570881276073, 'eval_runtime': 2.9865, 'eval_samples_per_second': 754.731, 'eval_steps_per_second': 6.027}\n",
      "\n",
      "🔍 Evaluating model: wt5mgigm/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for wt5mgigm/checkpoint-500: {'eval_mcc_metric': 0.7081470860470594, 'eval_loss': 0.21620403230190277, 'eval_Accuracy': 0.8442768411712511, 'eval_AUC-ROC': 0.9315793239021748, 'eval_Precision': 0.8126900435741338, 'eval_Recall': 0.6440294612920517, 'eval_F1-score': 0.6673688901517647, 'eval_runtime': 2.9814, 'eval_samples_per_second': 756.024, 'eval_steps_per_second': 6.037}\n",
      "\n",
      "🔍 Evaluating model: wt5mgigm/checkpoint-1645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for wt5mgigm/checkpoint-1645: {'eval_mcc_metric': 0.7612697330008528, 'eval_loss': 0.1838589459657669, 'eval_Accuracy': 0.8717834960070985, 'eval_AUC-ROC': 0.9468502303345275, 'eval_Precision': 0.6952379489019463, 'eval_Recall': 0.7114263953722755, 'eval_F1-score': 0.7013062277732849, 'eval_runtime': 3.2209, 'eval_samples_per_second': 699.803, 'eval_steps_per_second': 5.588}\n",
      "\n",
      "🔍 Evaluating model: 532smrg2/checkpoint-1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for 532smrg2/checkpoint-1000: {'eval_mcc_metric': 0.736670685430862, 'eval_loss': 0.20298810303211212, 'eval_Accuracy': 0.862910381543922, 'eval_AUC-ROC': 0.9488226028013358, 'eval_Precision': 0.7658943134071283, 'eval_Recall': 0.7140026853910473, 'eval_F1-score': 0.7361749798036902, 'eval_runtime': 2.9784, 'eval_samples_per_second': 756.771, 'eval_steps_per_second': 6.043}\n",
      "\n",
      "🔍 Evaluating model: 532smrg2/checkpoint-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for 532smrg2/checkpoint-500: {'eval_mcc_metric': 0.734268880398551, 'eval_loss': 0.20883527398109436, 'eval_Accuracy': 0.8589174800354925, 'eval_AUC-ROC': 0.9440477414069232, 'eval_Precision': 0.7009575272273703, 'eval_Recall': 0.7516299954479507, 'eval_F1-score': 0.7085234353197551, 'eval_runtime': 2.9781, 'eval_samples_per_second': 756.847, 'eval_steps_per_second': 6.044}\n",
      "\n",
      "🔍 Evaluating model: gbyjoy6e/checkpoint-658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/179427769.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for gbyjoy6e/checkpoint-658: {'eval_mcc_metric': 0.7242799948767075, 'eval_loss': 0.2091398537158966, 'eval_Accuracy': 0.8553682342502218, 'eval_AUC-ROC': 0.9401006451281212, 'eval_Precision': 0.6406431158116671, 'eval_Recall': 0.6096868249240435, 'eval_F1-score': 0.6204827610222969, 'eval_runtime': 2.935, 'eval_samples_per_second': 767.966, 'eval_steps_per_second': 6.133}\n",
      "\n",
      "🔍 Evaluating model: gbyjoy6e/checkpoint-500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Test Results for gbyjoy6e/checkpoint-500: {'eval_mcc_metric': 0.7299963331063521, 'eval_loss': 0.2147645652294159, 'eval_Accuracy': 0.8553682342502218, 'eval_AUC-ROC': 0.8971433581892038, 'eval_Precision': 0.7331086934361826, 'eval_Recall': 0.6519809960114861, 'eval_F1-score': 0.6698237032430857, 'eval_runtime': 2.9354, 'eval_samples_per_second': 767.86, 'eval_steps_per_second': 6.132}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from peft import PeftModel\n",
    "\n",
    "models_dir = \"./models_BBBP_chemberta_focal_loss\"\n",
    "\n",
    "def find_all_checkpoints(base_dir):\n",
    "    all_checkpoints = []\n",
    "    for folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for subfolder in os.listdir(folder_path):\n",
    "                subfolder_path = os.path.join(folder_path, subfolder)\n",
    "                if os.path.isdir(subfolder_path) and subfolder.startswith(\"checkpoint-\"):\n",
    "                    if os.path.exists(os.path.join(subfolder_path, \"adapter_config.json\")):\n",
    "                        all_checkpoints.append(subfolder_path)\n",
    "    return all_checkpoints\n",
    "\n",
    "valid_checkpoints = find_all_checkpoints(models_dir)\n",
    "print(\"🧠 Valid nested checkpoints found:\", valid_checkpoints)\n",
    "\n",
    "for checkpoint_path in valid_checkpoints:\n",
    "    checkpoint_name = os.path.basename(checkpoint_path)\n",
    "    parent_folder = os.path.basename(os.path.dirname(checkpoint_path))\n",
    "\n",
    "    print(f\"\\n🔍 Evaluating model: {parent_folder}/{checkpoint_name}\")\n",
    "\n",
    "    adapter_model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    adapter_model.eval()\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=adapter_model,\n",
    "        args=training_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    test_results_clin = trainer.evaluate()\n",
    "\n",
    "    auc_score = test_results_clin.get(\"eval/AUC_ROC\", 0)\n",
    "    if auc_score > 0.95:\n",
    "        print(f\"✅ AUC_ROC > 0.95 for {parent_folder}/{checkpoint_name}\")\n",
    "        print(f\"📌 Test Results: {test_results_clin}\")\n",
    "    else:\n",
    "        print(f\"❌ Skipping {parent_folder}/{checkpoint_name} (AUC_ROC = {auc_score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Model Checkpoints: ['./models_Mlm_100_Chemberta/xutmnj12', './models_Mlm_100_Chemberta/sz247wpr', './models_Mlm_100_Chemberta/ikd7q7fk', './models_Mlm_100_Chemberta/kovlgv1s', './models_Mlm_100_Chemberta/k0sfrn0p', './models_Mlm_100_Chemberta/0yvjid5t', './models_Mlm_100_Chemberta/d1wrlvdu', './models_Mlm_100_Chemberta/d6a28ks6', './models_Mlm_100_Chemberta/3fdjf7s7', './models_Mlm_100_Chemberta/6a8xs6i0', './models_Mlm_100_Chemberta/a99qvoss', './models_Mlm_100_Chemberta/q6ab3hnx', './models_Mlm_100_Chemberta/jbxogsm2', './models_Mlm_100_Chemberta/gi5kecmi', './models_Mlm_100_Chemberta/u9d6q2lo', './models_Mlm_100_Chemberta/wt5mgigm', './models_Mlm_100_Chemberta/532smrg2', './models_Mlm_100_Chemberta/gbyjoy6e']\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/xutmnj12\n",
      "\n",
      "🔍 Evaluating model folder: xutmnj12\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/xutmnj12: {'eval_mcc_metric': 0.7521464915364129, 'eval_loss': 0.19328723847866058, 'eval_Accuracy': 0.870452528837622, 'eval_AUC-ROC': 0.9531784124231235, 'eval_Precision': 0.7572555794128578, 'eval_Recall': 0.7250657272294483, 'eval_F1-score': 0.7380485089709764, 'eval_runtime': 2.939, 'eval_samples_per_second': 766.931, 'eval_steps_per_second': 6.125}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/sz247wpr\n",
      "\n",
      "🔍 Evaluating model folder: sz247wpr\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/sz247wpr: {'eval_mcc_metric': 0.72430838982655, 'eval_loss': 0.2104184478521347, 'eval_Accuracy': 0.854924578527063, 'eval_AUC-ROC': 0.910169070347604, 'eval_Precision': 0.8260430948474914, 'eval_Recall': 0.6436679844119826, 'eval_F1-score': 0.6732625055831842, 'eval_runtime': 3.1773, 'eval_samples_per_second': 709.397, 'eval_steps_per_second': 5.665}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/ikd7q7fk\n",
      "\n",
      "🔍 Evaluating model folder: ikd7q7fk\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/ikd7q7fk: {'eval_mcc_metric': 0.7409555692039872, 'eval_loss': 0.19144411385059357, 'eval_Accuracy': 0.8646850044365573, 'eval_AUC-ROC': 0.9266973256544127, 'eval_Precision': 0.8465550895920702, 'eval_Recall': 0.688897911223715, 'eval_F1-score': 0.7312272209981807, 'eval_runtime': 2.919, 'eval_samples_per_second': 772.191, 'eval_steps_per_second': 6.167}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/kovlgv1s\n",
      "\n",
      "🔍 Evaluating model folder: kovlgv1s\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/kovlgv1s: {'eval_mcc_metric': 0.7320377251817916, 'eval_loss': 0.1969209909439087, 'eval_Accuracy': 0.8598047914818101, 'eval_AUC-ROC': 0.9525929438784001, 'eval_Precision': 0.6922687690070904, 'eval_Recall': 0.7125012345914886, 'eval_F1-score': 0.6906020488842198, 'eval_runtime': 2.9804, 'eval_samples_per_second': 756.274, 'eval_steps_per_second': 6.039}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/k0sfrn0p\n",
      "\n",
      "🔍 Evaluating model folder: k0sfrn0p\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/k0sfrn0p: {'eval_mcc_metric': 0.745377240742711, 'eval_loss': 0.18867076933383942, 'eval_Accuracy': 0.8673469387755102, 'eval_AUC-ROC': 0.9240643387138823, 'eval_Precision': 0.7169286661753869, 'eval_Recall': 0.6532359392276617, 'eval_F1-score': 0.6757555231363336, 'eval_runtime': 3.2183, 'eval_samples_per_second': 700.36, 'eval_steps_per_second': 5.593}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/0yvjid5t\n",
      "\n",
      "🔍 Evaluating model folder: 0yvjid5t\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/0yvjid5t: {'eval_mcc_metric': 0.7327720416734685, 'eval_loss': 0.20283129811286926, 'eval_Accuracy': 0.8606921029281278, 'eval_AUC-ROC': 0.9453031939096954, 'eval_Precision': 0.8439377639193936, 'eval_Recall': 0.6475590019460279, 'eval_F1-score': 0.6836970646724254, 'eval_runtime': 2.9167, 'eval_samples_per_second': 772.799, 'eval_steps_per_second': 6.171}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/d1wrlvdu\n",
      "\n",
      "🔍 Evaluating model folder: d1wrlvdu\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/d1wrlvdu: {'eval_mcc_metric': 0.6269658000980638, 'eval_loss': 0.24145101010799408, 'eval_Accuracy': 0.8096716947648624, 'eval_AUC-ROC': 0.8861460530494141, 'eval_Precision': 0.592236401678622, 'eval_Recall': 0.5398703314437439, 'eval_F1-score': 0.5469782734245088, 'eval_runtime': 2.9127, 'eval_samples_per_second': 773.866, 'eval_steps_per_second': 6.18}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/d6a28ks6\n",
      "\n",
      "🔍 Evaluating model folder: d6a28ks6\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/d6a28ks6: {'eval_mcc_metric': 0.7142658195288292, 'eval_loss': 0.2040163278579712, 'eval_Accuracy': 0.8504880212954747, 'eval_AUC-ROC': 0.9447150623847612, 'eval_Precision': 0.6930504488037323, 'eval_Recall': 0.6671395409479339, 'eval_F1-score': 0.6786870690645163, 'eval_runtime': 2.9806, 'eval_samples_per_second': 756.232, 'eval_steps_per_second': 6.039}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/3fdjf7s7\n",
      "\n",
      "🔍 Evaluating model folder: 3fdjf7s7\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/3fdjf7s7: {'eval_mcc_metric': 0.7165065043347795, 'eval_loss': 0.2090887576341629, 'eval_Accuracy': 0.8513753327417923, 'eval_AUC-ROC': 0.9391260199982971, 'eval_Precision': 0.7388555397131931, 'eval_Recall': 0.6339252090059346, 'eval_F1-score': 0.6635488189383238, 'eval_runtime': 2.9308, 'eval_samples_per_second': 769.085, 'eval_steps_per_second': 6.142}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/6a8xs6i0\n",
      "\n",
      "🔍 Evaluating model folder: 6a8xs6i0\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/6a8xs6i0: {'eval_mcc_metric': 0.7352735242634216, 'eval_loss': 0.20702017843723297, 'eval_Accuracy': 0.8598047914818101, 'eval_AUC-ROC': 0.907016671296347, 'eval_Precision': 0.8399836132368446, 'eval_Recall': 0.6510461399244443, 'eval_F1-score': 0.6785944801507757, 'eval_runtime': 3.2219, 'eval_samples_per_second': 699.588, 'eval_steps_per_second': 5.587}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/a99qvoss\n",
      "\n",
      "🔍 Evaluating model folder: a99qvoss\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/a99qvoss: {'eval_mcc_metric': 0.733985401320564, 'eval_loss': 0.19938620924949646, 'eval_Accuracy': 0.8606921029281278, 'eval_AUC-ROC': 0.9334784404298844, 'eval_Precision': 0.8489034919817877, 'eval_Recall': 0.6464780594862665, 'eval_F1-score': 0.6796664281542979, 'eval_runtime': 2.9052, 'eval_samples_per_second': 775.856, 'eval_steps_per_second': 6.196}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/q6ab3hnx\n",
      "\n",
      "🔍 Evaluating model folder: q6ab3hnx\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/q6ab3hnx: {'eval_mcc_metric': 0.7463880380697544, 'eval_loss': 0.1934467852115631, 'eval_Accuracy': 0.8673469387755102, 'eval_AUC-ROC': 0.9471037509253183, 'eval_Precision': 0.8515282295264454, 'eval_Recall': 0.6568764134754077, 'eval_F1-score': 0.6912447962506661, 'eval_runtime': 2.9537, 'eval_samples_per_second': 763.117, 'eval_steps_per_second': 6.094}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/jbxogsm2\n",
      "\n",
      "🔍 Evaluating model folder: jbxogsm2\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/jbxogsm2: {'eval_mcc_metric': 0.7455775650378911, 'eval_loss': 0.20152515172958374, 'eval_Accuracy': 0.8646850044365573, 'eval_AUC-ROC': 0.9270770721930675, 'eval_Precision': 0.6816616896853674, 'eval_Recall': 0.694608822153824, 'eval_F1-score': 0.6822061180937639, 'eval_runtime': 3.1684, 'eval_samples_per_second': 711.393, 'eval_steps_per_second': 5.681}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/gi5kecmi\n",
      "\n",
      "🔍 Evaluating model folder: gi5kecmi\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/gi5kecmi: {'eval_mcc_metric': 0.7416033240064804, 'eval_loss': 0.18928112089633942, 'eval_Accuracy': 0.8646850044365573, 'eval_AUC-ROC': 0.9494673914241305, 'eval_Precision': 0.7175521682937753, 'eval_Recall': 0.6539430110345903, 'eval_F1-score': 0.6754301456803111, 'eval_runtime': 3.1611, 'eval_samples_per_second': 713.04, 'eval_steps_per_second': 5.694}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/u9d6q2lo\n",
      "\n",
      "🔍 Evaluating model folder: u9d6q2lo\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/u9d6q2lo: {'eval_mcc_metric': 0.7167517251598591, 'eval_loss': 0.2149065136909485, 'eval_Accuracy': 0.8504880212954747, 'eval_AUC-ROC': 0.9020879677136187, 'eval_Precision': 0.722033614061327, 'eval_Recall': 0.6381268330734974, 'eval_F1-score': 0.658734379036996, 'eval_runtime': 2.9835, 'eval_samples_per_second': 755.483, 'eval_steps_per_second': 6.033}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/wt5mgigm\n",
      "\n",
      "🔍 Evaluating model folder: wt5mgigm\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/wt5mgigm: {'eval_mcc_metric': 0.7556197394027053, 'eval_loss': 0.18587109446525574, 'eval_Accuracy': 0.8708961845607809, 'eval_AUC-ROC': 0.9508821803277767, 'eval_Precision': 0.724016556454012, 'eval_Recall': 0.6982829700966494, 'eval_F1-score': 0.7098701024520846, 'eval_runtime': 3.237, 'eval_samples_per_second': 696.318, 'eval_steps_per_second': 5.561}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/532smrg2\n",
      "\n",
      "🔍 Evaluating model folder: 532smrg2\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1701486/3232351798.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/532smrg2: {'eval_mcc_metric': 0.7463711208398681, 'eval_loss': 0.199150949716568, 'eval_Accuracy': 0.8655723158828749, 'eval_AUC-ROC': 0.9486051174719886, 'eval_Precision': 0.7437086216358899, 'eval_Recall': 0.7269904582435949, 'eval_F1-score': 0.7321706546551738, 'eval_runtime': 3.2236, 'eval_samples_per_second': 699.208, 'eval_steps_per_second': 5.584}\n",
      "Evaluating model: ./models_Mlm_100_Chemberta/gbyjoy6e\n",
      "\n",
      "🔍 Evaluating model folder: gbyjoy6e\n",
      "📦 Adapter file being loaded: training_args.bin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for ./models_Mlm_100_Chemberta/gbyjoy6e: {'eval_mcc_metric': 0.7299963331063521, 'eval_loss': 0.2147645652294159, 'eval_Accuracy': 0.8553682342502218, 'eval_AUC-ROC': 0.8971433581892038, 'eval_Precision': 0.7331086934361826, 'eval_Recall': 0.6519809960114861, 'eval_F1-score': 0.6698237032430857, 'eval_runtime': 2.9481, 'eval_samples_per_second': 764.549, 'eval_steps_per_second': 6.106}\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel  \n",
    "import os\n",
    "\n",
    "models_dir = \"./models_Mlm_100_Chemberta\"\n",
    "\n",
    "for ckpt in os.listdir(models_dir):\n",
    "    model_path = os.path.join(models_dir, ckpt)\n",
    "    \n",
    "    if os.path.isdir(model_path):\n",
    "        adapter_config_path = os.path.join(model_path, \"adapter_config.json\")\n",
    "        \n",
    "        if not os.path.exists(adapter_config_path):\n",
    "            print(f\"⚠️ Missing 'adapter_config.json' in {model_path}\")\n",
    "\n",
    "# Evaluate each saved model\n",
    "\n",
    "valid_checkpoints = [\n",
    "    os.path.join(models_dir, ckpt)\n",
    "    for ckpt in os.listdir(models_dir)\n",
    "    if os.path.isdir(os.path.join(models_dir, ckpt)) and \n",
    "       os.path.exists(os.path.join(models_dir, ckpt, \"adapter_config.json\"))\n",
    "]\n",
    "\n",
    "print(\"Valid Model Checkpoints:\", valid_checkpoints)\n",
    "\n",
    "for model_path in valid_checkpoints:\n",
    "    print(f\"Evaluating model: {model_path}\")\n",
    "    checkpoint_name = os.path.basename(model_path)\n",
    "    \n",
    "\n",
    "    model_files = os.listdir(model_path)\n",
    "\n",
    "     # Identify key model files (e.g., adapter weights)\n",
    "    adapter_files = [f for f in model_files if f.endswith(\".bin\")]\n",
    "    adapter_file_used = adapter_files[0] if adapter_files else \"❌ No .bin file found\"\n",
    "\n",
    "    print(f\"\\n🔍 Evaluating model folder: {checkpoint_name}\")\n",
    "    print(f\"📦 Adapter file being loaded: {adapter_file_used}\")\n",
    "\n",
    "    adapter_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    adapter_model.eval()\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=adapter_model,\n",
    "        args=training_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    test_results_clin = trainer.evaluate()\n",
    "    print(f\"Test Results for {model_path}: {test_results_clin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best of all Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best of 77M MLM Model: \n",
    "\n",
    "'eval_mcc_metric': 0.743591092208974, 'eval_loss': 0.19872386753559113, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8646850044365573, 'eval_AUC-ROC': 0.9008098793247992, 'eval_Precision': 0.8414375896683746, 'eval_Recall': 0.6564035372933563, 'eval_F1-score': 0.6835677143059256,\n",
    "\n",
    "### Best of 10M MLM Model: \n",
    "\n",
    "{'eval_mcc_metric': 0.7783196473274908, 'eval_loss': 0.15798771381378174, 'eval_Accuracy': 0.883318544809228, 'eval_AUC-ROC': 0.9487068975033708, 'eval_Precision': 0.7174260972868147, 'eval_Recall': 0.7161449075205971, 'eval_F1-score': 0.714283627299657}\n",
    "\n",
    "### Best of 77M MTR\n",
    "\n",
    "'eval_mcc_metric': 0.7882497847517572, 'eval_loss': 0.14135079085826874, 'eval_Accuracy': 0.8873114463176575, 'eval_AUC-ROC': 0.9720325211171271, 'eval_Precision': 0.8327510094500526, 'eval_Recall': 0.8274682544764185, 'eval_F1-score': 0.8297152462405265\n",
    "\n",
    "\n",
    "### Best of 10M MTR\n",
    "'eval_mcc_metric': 0.8020882141160608, 'eval_loss': 0.1354757696390152, 'eval_model_preparation_time': 0.0055, 'eval_Accuracy': 0.8948535936113576, 'eval_AUC-ROC': 0.9712263554530509, 'eval_Precision': 0.8163329237327627, 'eval_Recall': 0.8065458720498409, 'eval_F1-score': 0.8099227080402003,\n",
    "\n",
    "'eval_mcc_metric': 0.7903027900598758, 'eval_loss': 0.13793590664863586, 'eval_model_preparation_time': 0.0043, 'eval_Accuracy': 0.888642413487134, 'eval_AUC-ROC': 0.9712670821306159, 'eval_Precision': 0.8297460295827033, 'eval_Recall': 0.7985800549192325, 'eval_F1-score': 0.8120801935825286,\n",
    "\n",
    "### Best of 5M MTR\n",
    "\n",
    "'eval_mcc_metric': 0.7922832370919513, 'eval_loss': 0.13667497038841248, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8895297249334516, 'eval_AUC-ROC': 0.958335401296123, 'eval_Precision': 0.767476398219038, 'eval_Recall': 0.7331228786790375, 'eval_F1-score': 0.7469094345185043"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  With the model chemberta_v1_ the performance was worse relatively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Merge the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'DeepChem/ChemBERTa-10M-MLM',\n",
    "    num_labels=5,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel  \n",
    "\n",
    "adapter_model = PeftModel.from_pretrained(base_model, \"/home/raghvendra2/Molformer_Finetuning/models_Mlm_10_Chemberta/ad5xf1ya/checkpoint-1500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_chemberta_10M_MLM= adapter_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model to Chemberta finetuned model lora 100M MTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/raghvendra2/Molformer_Finetuning/chemberta_final_model_lora_100M_MTR\"\n",
    "\n",
    "final_model_chemberta_77M_MTR.save_pretrained(save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model for 77M MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/raghvendra2/Molformer_Finetuning/chemberta_final_model_lora_77M_MLM\"\n",
    "\n",
    "final_model_chemberta_77M_MLM.save_pretrained(save_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save Model for 10M MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/raghvendra2/Molformer_Finetuning/chemberta_final_model_lora_10M_MLM\"\n",
    "\n",
    "final_model_chemberta_10M_MLM.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model for 10M MTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/raghvendra2/Molformer_Finetuning/chemberta_final_model_lora_10M_MTR\"\n",
    "\n",
    "final_model_chemberta_10M_MTR.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model for 5M MTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/raghvendra2/Molformer_Finetuning/chemberta_final_model_lora_5M_MTR\"\n",
    "\n",
    "final_model_chemberta_5M_MTR.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 3,429,365\n",
      "Trainable Parameters: 149,765\n"
     ]
    }
   ],
   "source": [
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in final_model_chemberta_77M_MTR.parameters())\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in final_model_chemberta_77M_MTR.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83454725\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in base_model.parameters())\n",
    "print(total_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Molformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
