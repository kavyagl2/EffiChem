{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clintox LoRA Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_clin=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/clintox_train.csv')\n",
    "val_clin=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/clintox_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clin_sub=train_clin.drop(['FDA_APPROVED','smiles'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tokenizer and Classsification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_clin = AutoTokenizer.from_pretrained(\n",
    "    \"ibm/MoLFormer-XL-both-10pct\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load the model with a classification head\n",
    "model_clin = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"ibm/MoLFormer-XL-both-10pct\",\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list_clin = train_clin['smiles'].tolist()\n",
    "smiles_val_clin=val_clin['smiles'].tolist()\n",
    "train_tokenized_clin=tokenizer_clin(smiles_list_clin)\n",
    "val_tokenized_clin=tokenizer_clin(smiles_val_clin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset_clin = Dataset.from_dict(train_tokenized_clin)\n",
    "val_dataset_clin = Dataset.from_dict(val_tokenized_clin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_clin = train_clin['CT_TOX'].tolist() # Assuming tasks start from column 1\n",
    "val_labels_clin = val_clin['CT_TOX'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_clin = train_dataset_clin.add_column(\"labels\", train_labels_clin)\n",
    "val_dataset_clin = val_dataset_clin.add_column(\"labels\", val_labels_clin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from evaluate import load\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load():\n",
    "    train_clin=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/clintox_train.csv')\n",
    "    val_clin=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/clintox_test.csv')\n",
    "\n",
    "    return train_clin, val_clin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data_process,tokenizer_clin):\n",
    "\n",
    "    smiles_list_clin = data_process['smiles'].tolist()\n",
    "    tokenized_clin=tokenizer_clin(smiles_list_clin)\n",
    "    \n",
    "    \n",
    "    dataset_clin = Dataset.from_dict(tokenized_clin)\n",
    "    \n",
    "\n",
    "    labels_clin = data_process['CT_TOX'].tolist() # Assuming tasks start from column 1\n",
    "    \n",
    "    dataset_clin = dataset_clin.add_column(\"labels\", labels_clin)\n",
    "    \n",
    "\n",
    "    return dataset_clin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "def lora_config(r,lora_alpha,dropout):\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",  # Sequence classification task\n",
    "        r=r,  # Rank of LoRA matrices\n",
    "        lora_alpha=lora_alpha,  # Scaling factor double of rank( from the rule of thumb)\n",
    "        target_modules='all-linear',\n",
    "        lora_dropout=dropout  # Dropout rate\n",
    "        #init_lora_weights=\"gaussian\"\n",
    "    )\n",
    "\n",
    "    return lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class_weights= [1-(train_dataset_clin['labels'].count(0)/len(train_dataset_clin['labels'])),\n",
    "                           1-(train_dataset_clin['labels'].count(1)/len(train_dataset_clin['labels']))]\n",
    "\n",
    "class_weights = torch.from_numpy(np.array(class_weights)).float().to(\"cuda\")\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Extract labels\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # compute custom loss (suppose one has 2 labels with different weights)\n",
    "        loss_func = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_func(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef\n",
    "\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "mcc_metric= load(\"matthews_correlation\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    probabilities = softmax(logits, axis=1)[:, 1]  # Get probabilities for class 1\n",
    "    predictions = np.argmax(logits, axis=1)  # Choose the most likely class\n",
    "    \n",
    "\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        \"eval_mcc_metric\": mcc,\n",
    "        \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"AUC-ROC\": roc_auc_score(labels, probabilities),  # AUC-ROC requires probabilities\n",
    "        \"Precision\": precision_score(labels, predictions),\n",
    "        \"Recall\": recall_score(labels, predictions),\n",
    "        \"F1-score\": f1_score(labels, predictions)\n",
    "    } \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize W&B with sweep\n",
    "def run_training():\n",
    "    run = wandb.init(project=\"Clintox Hyperparameter Tuning\")\n",
    "    config = run.config   \n",
    "\n",
    "    # Define unique save path for each W&B run\n",
    "    save_dir = f\"./models_molformer_clintox/{wandb.run.id}\"  # Unique directory for each run\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    tokenizer_clin = AutoTokenizer.from_pretrained(\n",
    "        \"ibm/MoLFormer-XL-both-10pct\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Load data\n",
    "    train_data, val_data = data_load()\n",
    "    training_data = data_prep(train_data, tokenizer_clin)\n",
    "    validation_data = data_prep(val_data, tokenizer_clin)\n",
    "\n",
    "    # Load base model\n",
    "    model_clin = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"ibm/MoLFormer-XL-both-10pct\",\n",
    "        num_labels=2,\n",
    "        problem_type=\"single_label_classification\",    \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Apply LoRA\n",
    "    peft_config = lora_config(config.r, config.lora_alpha, config.dropout)\n",
    "    lora_model = get_peft_model(model_clin, peft_config)\n",
    "    \n",
    "    lora_model.print_trainable_parameters()\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=config.lr,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=20,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",  # Save model at each epoch\n",
    "        logging_dir=f\"./logs_clin/{wandb.run.id}\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        report_to=\"wandb\",\n",
    "        save_total_limit=5,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_mcc_metric\"\n",
    "    )\n",
    "\n",
    "    accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = softmax(logits, axis=1)[:, 1]  \n",
    "        predictions = np.argmax(logits, axis=1)  \n",
    "        mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "        return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities),\n",
    "            \"Precision\": precision_score(labels, predictions),\n",
    "            \"Recall\": recall_score(labels, predictions),\n",
    "            \"F1-score\": f1_score(labels, predictions)\n",
    "        }\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=training_data,\n",
    "        eval_dataset=validation_data,\n",
    "        tokenizer=tokenizer_clin,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save model and tokenizer for this run\n",
    "    trainer.save_model(save_dir)\n",
    "    tokenizer_clin.save_pretrained(save_dir)\n",
    "    \n",
    "    print(f\"Model saved to {save_dir}\")\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ambvlx5s\n",
      "Sweep URL: https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kbo8a99q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.6312099053672378e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mharodharsha21\u001b[0m (\u001b[33mharodharsha21-iit-ropar\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_080234-kbo8a99q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/kbo8a99q' target=\"_blank\">divine-sweep-1</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/kbo8a99q' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/kbo8a99q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,506,498 || all params: 54,310,148 || trainable%: 15.6628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1868290/36713449.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-10 08:02:43,126] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='304' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [304/760 01:10 < 01:47, 4.26 it/s, Epoch 8/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.459713</td>\n",
       "      <td>0.704154</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.975148</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.702703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.269452</td>\n",
       "      <td>0.813760</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.827586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.424100</td>\n",
       "      <td>0.167435</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.424100</td>\n",
       "      <td>0.136626</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995858</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.424100</td>\n",
       "      <td>0.154945</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.992899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>0.182133</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>0.239986</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.091200</td>\n",
       "      <td>0.279319</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.989349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_molformer_clintox/kbo8a99q\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▇▇█▇▇▇▆</td></tr><tr><td>eval/Accuracy</td><td>▁▆██████</td></tr><tr><td>eval/F1-score</td><td>▁▅██████</td></tr><tr><td>eval/Precision</td><td>▁▄▇█████</td></tr><tr><td>eval/Recall</td><td>█▅▅▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>█▄▂▁▁▂▃▄</td></tr><tr><td>eval/mcc_metric</td><td>▁▅██████</td></tr><tr><td>eval/runtime</td><td>█▂▁▁▂▂▃▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▇██▇▇▅█</td></tr><tr><td>eval/steps_per_second</td><td>▁▇██▇▇▅█</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▄▅▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▄▅▅▆▇███</td></tr><tr><td>train/grad_norm</td><td>█▁▁</td></tr><tr><td>train/learning_rate</td><td>█▅▁</td></tr><tr><td>train/loss</td><td>█▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.98935</td></tr><tr><td>eval/Accuracy</td><td>0.98601</td></tr><tr><td>eval/F1-score</td><td>0.91667</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.27932</td></tr><tr><td>eval/mcc_metric</td><td>0.91287</td></tr><tr><td>eval/runtime</td><td>0.4258</td></tr><tr><td>eval/samples_per_second</td><td>335.865</td></tr><tr><td>eval/steps_per_second</td><td>11.744</td></tr><tr><td>total_flos</td><td>436606970091552.0</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>304</td></tr><tr><td>train/grad_norm</td><td>0.24665</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.0912</td></tr><tr><td>train_loss</td><td>0.20786</td></tr><tr><td>train_runtime</td><td>71.6846</td></tr><tr><td>train_samples_per_second</td><td>330.615</td></tr><tr><td>train_steps_per_second</td><td>10.602</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-sweep-1</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/kbo8a99q' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/kbo8a99q</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_080234-kbo8a99q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lm8vser5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.0421017672822343e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_080417-lm8vser5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/lm8vser5' target=\"_blank\">daily-sweep-2</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/lm8vser5' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/lm8vser5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,640,458 || all params: 47,213,588 || trainable%: 3.4745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1868290/36713449.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='266' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [266/760 00:56 < 01:45, 4.69 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.524808</td>\n",
       "      <td>0.656103</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.956213</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.399298</td>\n",
       "      <td>0.957095</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>0.998817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.504600</td>\n",
       "      <td>0.333783</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.997633</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.504600</td>\n",
       "      <td>0.271847</td>\n",
       "      <td>0.957095</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.504600</td>\n",
       "      <td>0.239523</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.997633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.281200</td>\n",
       "      <td>0.211013</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.998817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.281200</td>\n",
       "      <td>0.174120</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.998817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_molformer_clintox/lm8vser5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁██████</td></tr><tr><td>eval/Accuracy</td><td>▁█▇█▇▇▇</td></tr><tr><td>eval/F1-score</td><td>▁█▇█▇▇▇</td></tr><tr><td>eval/Precision</td><td>▁█▇████</td></tr><tr><td>eval/Recall</td><td>████▁▁▁</td></tr><tr><td>eval/loss</td><td>█▅▄▃▂▂▁</td></tr><tr><td>eval/mcc_metric</td><td>▁█▇█▇▇▇</td></tr><tr><td>eval/runtime</td><td>▆▄█▄▁▆▁</td></tr><tr><td>eval/samples_per_second</td><td>▃▅▁▅█▃█</td></tr><tr><td>eval/steps_per_second</td><td>▃▅▁▅█▃█</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▅▆▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▅▆▆▇██</td></tr><tr><td>train/grad_norm</td><td>█▁</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99882</td></tr><tr><td>eval/Accuracy</td><td>0.98601</td></tr><tr><td>eval/F1-score</td><td>0.91667</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.17412</td></tr><tr><td>eval/mcc_metric</td><td>0.91287</td></tr><tr><td>eval/runtime</td><td>0.3994</td></tr><tr><td>eval/samples_per_second</td><td>358.062</td></tr><tr><td>eval/steps_per_second</td><td>12.52</td></tr><tr><td>total_flos</td><td>329940572317704.0</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>266</td></tr><tr><td>train/grad_norm</td><td>1.18418</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.2812</td></tr><tr><td>train_loss</td><td>0.34487</td></tr><tr><td>train_runtime</td><td>56.5518</td></tr><tr><td>train_samples_per_second</td><td>419.085</td></tr><tr><td>train_steps_per_second</td><td>13.439</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">daily-sweep-2</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/lm8vser5' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/lm8vser5</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_080417-lm8vser5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o1vbzyf5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.5261371145863336e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_080530-o1vbzyf5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/o1vbzyf5' target=\"_blank\">robust-sweep-3</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/o1vbzyf5' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/o1vbzyf5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,844,610 || all params: 50,525,316 || trainable%: 9.5885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1868290/36713449.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='266' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [266/760 00:58 < 01:49, 4.50 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.472708</td>\n",
       "      <td>0.643952</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.954438</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.325774</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.987574</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.446300</td>\n",
       "      <td>0.223620</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.446300</td>\n",
       "      <td>0.148105</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995858</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.446300</td>\n",
       "      <td>0.160287</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.163700</td>\n",
       "      <td>0.170840</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.163700</td>\n",
       "      <td>0.145772</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.996450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_molformer_clintox/o1vbzyf5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▇▇████</td></tr><tr><td>eval/Accuracy</td><td>▁██████</td></tr><tr><td>eval/F1-score</td><td>▁██████</td></tr><tr><td>eval/Precision</td><td>▁▇▇▇███</td></tr><tr><td>eval/Recall</td><td>▁███▁▁▁</td></tr><tr><td>eval/loss</td><td>█▅▃▁▁▂▁</td></tr><tr><td>eval/mcc_metric</td><td>▁██████</td></tr><tr><td>eval/runtime</td><td>▁▄▄▆▇█▇</td></tr><tr><td>eval/samples_per_second</td><td>█▅▅▃▂▁▂</td></tr><tr><td>eval/steps_per_second</td><td>█▅▅▃▂▁▂</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▅▆▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▅▆▆▇██</td></tr><tr><td>train/grad_norm</td><td>█▁</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99645</td></tr><tr><td>eval/Accuracy</td><td>0.98601</td></tr><tr><td>eval/F1-score</td><td>0.91667</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.14577</td></tr><tr><td>eval/mcc_metric</td><td>0.91287</td></tr><tr><td>eval/runtime</td><td>0.4148</td></tr><tr><td>eval/samples_per_second</td><td>344.716</td></tr><tr><td>eval/steps_per_second</td><td>12.053</td></tr><tr><td>total_flos</td><td>354008495946600.0</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>266</td></tr><tr><td>train/grad_norm</td><td>0.2237</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.1637</td></tr><tr><td>train_loss</td><td>0.25987</td></tr><tr><td>train_runtime</td><td>58.9759</td></tr><tr><td>train_samples_per_second</td><td>401.859</td></tr><tr><td>train_steps_per_second</td><td>12.887</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">robust-sweep-3</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/o1vbzyf5' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/o1vbzyf5</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_080530-o1vbzyf5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2diwcmfr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.4895769470124064e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_080643-2diwcmfr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/2diwcmfr' target=\"_blank\">rich-sweep-4</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/2diwcmfr' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/2diwcmfr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,506,498 || all params: 54,310,148 || trainable%: 15.6628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1868290/36713449.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='456' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [456/760 01:46 < 01:11, 4.26 it/s, Epoch 12/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.499392</td>\n",
       "      <td>0.558934</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.941420</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.377013</td>\n",
       "      <td>0.844375</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.985207</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.480300</td>\n",
       "      <td>0.291138</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.480300</td>\n",
       "      <td>0.213623</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.998225</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.480300</td>\n",
       "      <td>0.194900</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.996450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.185717</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.148291</td>\n",
       "      <td>0.957095</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>0.996450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.153600</td>\n",
       "      <td>0.146876</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.997041</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.153600</td>\n",
       "      <td>0.153820</td>\n",
       "      <td>0.957095</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>0.992899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.153600</td>\n",
       "      <td>0.131516</td>\n",
       "      <td>0.957095</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>0.996450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.141007</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.997041</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.124600</td>\n",
       "      <td>0.166688</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_molformer_clintox/2diwcmfr\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▆▇█████▇███</td></tr><tr><td>eval/Accuracy</td><td>▁▇██████████</td></tr><tr><td>eval/F1-score</td><td>▁▆▇▇▇▇█▇██▇▇</td></tr><tr><td>eval/Precision</td><td>▁▆▇▇████████</td></tr><tr><td>eval/Recall</td><td>▁███▁▁█▁██▁▁</td></tr><tr><td>eval/loss</td><td>█▆▄▃▂▂▁▁▁▁▁▂</td></tr><tr><td>eval/mcc_metric</td><td>▁▆▇▇▇▇█▇██▇▇</td></tr><tr><td>eval/runtime</td><td>█▂▂▁▃▅▂▂▆▄▆▅</td></tr><tr><td>eval/samples_per_second</td><td>▁▇▇█▆▄▇▇▃▅▃▄</td></tr><tr><td>eval/steps_per_second</td><td>▁▇▇█▆▄▇▇▃▅▃▄</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▄▄▄▅▅▅▆▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▄▄▄▅▅▅▆▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>█▂▁▁</td></tr><tr><td>train/learning_rate</td><td>█▆▃▁</td></tr><tr><td>train/loss</td><td>█▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99527</td></tr><tr><td>eval/Accuracy</td><td>0.98601</td></tr><tr><td>eval/F1-score</td><td>0.91667</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.16669</td></tr><tr><td>eval/mcc_metric</td><td>0.91287</td></tr><tr><td>eval/runtime</td><td>0.4268</td></tr><tr><td>eval/samples_per_second</td><td>335.017</td></tr><tr><td>eval/steps_per_second</td><td>11.714</td></tr><tr><td>total_flos</td><td>657670596765624.0</td></tr><tr><td>train/epoch</td><td>12</td></tr><tr><td>train/global_step</td><td>456</td></tr><tr><td>train/grad_norm</td><td>0.52241</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.1246</td></tr><tr><td>train_loss</td><td>0.22947</td></tr><tr><td>train_runtime</td><td>106.8531</td></tr><tr><td>train_samples_per_second</td><td>221.8</td></tr><tr><td>train_steps_per_second</td><td>7.113</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rich-sweep-4</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/2diwcmfr' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/2diwcmfr</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_080643-2diwcmfr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3wzclq6q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.4142564104975132e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_080842-3wzclq6q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/3wzclq6q' target=\"_blank\">lemon-sweep-5</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/3wzclq6q' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/3wzclq6q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,098,194 || all params: 47,686,692 || trainable%: 4.4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1868290/36713449.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='304' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [304/760 01:03 < 01:36, 4.73 it/s, Epoch 8/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.433198</td>\n",
       "      <td>0.813760</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.963905</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.827586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.247336</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.368700</td>\n",
       "      <td>0.172291</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.368700</td>\n",
       "      <td>0.211937</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.368700</td>\n",
       "      <td>0.289031</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.988757</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.312574</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.477216</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.986391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.569535</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.981657</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_molformer_clintox/3wzclq6q\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▇██▇█▆▅</td></tr><tr><td>eval/Accuracy</td><td>▁▆██████</td></tr><tr><td>eval/F1-score</td><td>▁▅██████</td></tr><tr><td>eval/Precision</td><td>▁▆██████</td></tr><tr><td>eval/Recall</td><td>█▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>▆▂▁▂▃▃▆█</td></tr><tr><td>eval/mcc_metric</td><td>▁▅██████</td></tr><tr><td>eval/runtime</td><td>▁▁█▃█▇▄▇</td></tr><tr><td>eval/samples_per_second</td><td>██▁▆▁▂▅▂</td></tr><tr><td>eval/steps_per_second</td><td>██▁▆▁▂▅▂</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▄▅▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▄▅▅▆▇███</td></tr><tr><td>train/grad_norm</td><td>█▁▁</td></tr><tr><td>train/learning_rate</td><td>█▅▁</td></tr><tr><td>train/loss</td><td>█▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.98166</td></tr><tr><td>eval/Accuracy</td><td>0.98601</td></tr><tr><td>eval/F1-score</td><td>0.91667</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.56953</td></tr><tr><td>eval/mcc_metric</td><td>0.91287</td></tr><tr><td>eval/runtime</td><td>0.4123</td></tr><tr><td>eval/samples_per_second</td><td>346.809</td></tr><tr><td>eval/steps_per_second</td><td>12.126</td></tr><tr><td>total_flos</td><td>381520110440736.0</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>304</td></tr><tr><td>train/grad_norm</td><td>0.43364</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.125</td></tr><tr><td>train_loss</td><td>0.19698</td></tr><tr><td>train_runtime</td><td>64.0993</td></tr><tr><td>train_samples_per_second</td><td>369.739</td></tr><tr><td>train_steps_per_second</td><td>11.857</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lemon-sweep-5</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/3wzclq6q' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/3wzclq6q</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_080842-3wzclq6q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jvf2j5zv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.5141899736019033e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_081000-jvf2j5zv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/jvf2j5zv' target=\"_blank\">charmed-sweep-6</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/jvf2j5zv' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/jvf2j5zv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,640,458 || all params: 47,213,588 || trainable%: 3.4745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1868290/36713449.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='266' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [266/760 00:54 < 01:42, 4.84 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.457875</td>\n",
       "      <td>0.844375</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.333438</td>\n",
       "      <td>0.957095</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>0.997633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.449500</td>\n",
       "      <td>0.239397</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.999408</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.449500</td>\n",
       "      <td>0.182941</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.449500</td>\n",
       "      <td>0.151454</td>\n",
       "      <td>0.957095</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>0.998817</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.191700</td>\n",
       "      <td>0.154435</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.191700</td>\n",
       "      <td>0.139757</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.999408</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_molformer_clintox/jvf2j5zv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▇█▆█▇</td></tr><tr><td>eval/Accuracy</td><td>▁█▆▆█▆▆</td></tr><tr><td>eval/F1-score</td><td>▁█▅▅█▅▅</td></tr><tr><td>eval/Precision</td><td>▁█▅████</td></tr><tr><td>eval/Recall</td><td>███▁█▁▁</td></tr><tr><td>eval/loss</td><td>█▅▃▂▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁█▅▅█▅▅</td></tr><tr><td>eval/runtime</td><td>▃▁▃▂▆▇█</td></tr><tr><td>eval/samples_per_second</td><td>▆█▆▇▃▂▁</td></tr><tr><td>eval/steps_per_second</td><td>▆█▆▇▃▂▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▅▆▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▅▆▆▇██</td></tr><tr><td>train/grad_norm</td><td>█▁</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99941</td></tr><tr><td>eval/Accuracy</td><td>0.98601</td></tr><tr><td>eval/F1-score</td><td>0.91667</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.13976</td></tr><tr><td>eval/mcc_metric</td><td>0.91287</td></tr><tr><td>eval/runtime</td><td>0.4105</td></tr><tr><td>eval/samples_per_second</td><td>348.335</td></tr><tr><td>eval/steps_per_second</td><td>12.18</td></tr><tr><td>total_flos</td><td>329940572317704.0</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>266</td></tr><tr><td>train/grad_norm</td><td>0.43581</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.1917</td></tr><tr><td>train_loss</td><td>0.27574</td></tr><tr><td>train_runtime</td><td>54.8094</td></tr><tr><td>train_samples_per_second</td><td>432.408</td></tr><tr><td>train_steps_per_second</td><td>13.866</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">charmed-sweep-6</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/jvf2j5zv' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/jvf2j5zv</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_081000-jvf2j5zv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gulowmgl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.2023686914961584e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_081107-gulowmgl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/gulowmgl' target=\"_blank\">wise-sweep-7</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/gulowmgl' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/gulowmgl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,013,666 || all params: 48,632,900 || trainable%: 6.1968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1868290/36713449.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='418' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [418/760 01:31 < 01:15, 4.55 it/s, Epoch 11/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.529954</td>\n",
       "      <td>0.505984</td>\n",
       "      <td>0.860140</td>\n",
       "      <td>0.914793</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.523810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.423347</td>\n",
       "      <td>0.785757</td>\n",
       "      <td>0.958042</td>\n",
       "      <td>0.980473</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.512600</td>\n",
       "      <td>0.341780</td>\n",
       "      <td>0.878058</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.988757</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.512600</td>\n",
       "      <td>0.263308</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.998225</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.512600</td>\n",
       "      <td>0.234602</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995858</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.277800</td>\n",
       "      <td>0.216456</td>\n",
       "      <td>0.957095</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.277800</td>\n",
       "      <td>0.176623</td>\n",
       "      <td>0.957095</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>0.997633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.185400</td>\n",
       "      <td>0.162396</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.997633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.185400</td>\n",
       "      <td>0.165579</td>\n",
       "      <td>0.957095</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>0.992899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.185400</td>\n",
       "      <td>0.141140</td>\n",
       "      <td>0.957095</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>0.996450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.142070</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.998225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_molformer_clintox/gulowmgl\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▇▇████████</td></tr><tr><td>eval/Accuracy</td><td>▁▆▇████████</td></tr><tr><td>eval/F1-score</td><td>▁▅▇▇▇██▇██▇</td></tr><tr><td>eval/Precision</td><td>▁▅▆▇▇██████</td></tr><tr><td>eval/Recall</td><td>▁██████▁██▁</td></tr><tr><td>eval/loss</td><td>█▆▅▃▃▂▂▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▅▇▇▇██▇██▇</td></tr><tr><td>eval/runtime</td><td>▄▇█▇█▅▂▄▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▅▂▁▂▁▄▆▅███</td></tr><tr><td>eval/steps_per_second</td><td>▅▂▁▂▁▄▆▅███</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▄▄▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▄▄▄▅▆▆▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▃▁▁</td></tr><tr><td>train/learning_rate</td><td>█▆▃▁</td></tr><tr><td>train/loss</td><td>█▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99822</td></tr><tr><td>eval/Accuracy</td><td>0.98601</td></tr><tr><td>eval/F1-score</td><td>0.91667</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.14207</td></tr><tr><td>eval/mcc_metric</td><td>0.91287</td></tr><tr><td>eval/runtime</td><td>0.4013</td></tr><tr><td>eval/samples_per_second</td><td>356.361</td></tr><tr><td>eval/steps_per_second</td><td>12.46</td></tr><tr><td>total_flos</td><td>538479341477952.0</td></tr><tr><td>train/epoch</td><td>11</td></tr><tr><td>train/global_step</td><td>418</td></tr><tr><td>train/grad_norm</td><td>0.75469</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.1437</td></tr><tr><td>train_loss</td><td>0.27201</td></tr><tr><td>train_runtime</td><td>91.5851</td></tr><tr><td>train_samples_per_second</td><td>258.776</td></tr><tr><td>train_steps_per_second</td><td>8.298</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wise-sweep-7</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/gulowmgl' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/gulowmgl</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_081107-gulowmgl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: k121h7u1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.2561499111615004e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_081256-k121h7u1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/k121h7u1' target=\"_blank\">effortless-sweep-8</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/k121h7u1' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/k121h7u1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,640,458 || all params: 47,213,588 || trainable%: 3.4745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1868290/36713449.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='418' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [418/760 01:30 < 01:14, 4.62 it/s, Epoch 11/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.581817</td>\n",
       "      <td>0.498914</td>\n",
       "      <td>0.832168</td>\n",
       "      <td>0.899408</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.410739</td>\n",
       "      <td>0.785757</td>\n",
       "      <td>0.958042</td>\n",
       "      <td>0.974556</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.525100</td>\n",
       "      <td>0.316396</td>\n",
       "      <td>0.813760</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.987574</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.827586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.525100</td>\n",
       "      <td>0.223903</td>\n",
       "      <td>0.844375</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.525100</td>\n",
       "      <td>0.198002</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.188338</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.178192</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>0.179138</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.996450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>0.186915</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>0.209079</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.250119</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_molformer_clintox/k121h7u1\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▆▇████████</td></tr><tr><td>eval/Accuracy</td><td>▁▇▇▇▇██████</td></tr><tr><td>eval/F1-score</td><td>▁▆▇▇▇█▇████</td></tr><tr><td>eval/Precision</td><td>▁▅▅▆▆█▇████</td></tr><tr><td>eval/Recall</td><td>████▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>█▅▃▂▁▁▁▁▁▂▂</td></tr><tr><td>eval/mcc_metric</td><td>▁▆▆▇▇█▇████</td></tr><tr><td>eval/runtime</td><td>▄▄▆▇▇▆█▄▄▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▅▅▃▂▂▃▁▅▅██</td></tr><tr><td>eval/steps_per_second</td><td>▅▅▃▂▂▃▁▅▅██</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▄▄▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▄▄▄▅▆▆▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▁▁▃</td></tr><tr><td>train/learning_rate</td><td>█▆▃▁</td></tr><tr><td>train/loss</td><td>█▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99349</td></tr><tr><td>eval/Accuracy</td><td>0.98601</td></tr><tr><td>eval/F1-score</td><td>0.91667</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.25012</td></tr><tr><td>eval/mcc_metric</td><td>0.91287</td></tr><tr><td>eval/runtime</td><td>0.3964</td></tr><tr><td>eval/samples_per_second</td><td>360.786</td></tr><tr><td>eval/steps_per_second</td><td>12.615</td></tr><tr><td>total_flos</td><td>522155368631616.0</td></tr><tr><td>train/epoch</td><td>11</td></tr><tr><td>train/global_step</td><td>418</td></tr><tr><td>train/grad_norm</td><td>1.29894</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.1081</td></tr><tr><td>train_loss</td><td>0.2392</td></tr><tr><td>train_runtime</td><td>90.2795</td></tr><tr><td>train_samples_per_second</td><td>262.518</td></tr><tr><td>train_steps_per_second</td><td>8.418</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">effortless-sweep-8</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/k121h7u1' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/k121h7u1</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_081256-k121h7u1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: btpw4450 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.823268967202021e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_081439-btpw4450</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/btpw4450' target=\"_blank\">graceful-sweep-9</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/btpw4450' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/btpw4450</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,013,666 || all params: 48,632,900 || trainable%: 6.1968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1868290/36713449.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='266' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [266/760 00:56 < 01:45, 4.68 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.414801</td>\n",
       "      <td>0.796205</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.978698</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.814815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.191054</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.989941</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.353100</td>\n",
       "      <td>0.179716</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.992899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.353100</td>\n",
       "      <td>0.155668</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.992899</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.353100</td>\n",
       "      <td>0.442334</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.982249</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.098200</td>\n",
       "      <td>0.330261</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.098200</td>\n",
       "      <td>0.374909</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.988757</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_molformer_clintox/btpw4450\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▇██▃▇▆</td></tr><tr><td>eval/Accuracy</td><td>▁█████▆</td></tr><tr><td>eval/F1-score</td><td>▁█████▅</td></tr><tr><td>eval/Precision</td><td>▁▅█▅██▅</td></tr><tr><td>eval/Recall</td><td>▁█▁█▁▁▁</td></tr><tr><td>eval/loss</td><td>▇▂▂▁█▅▆</td></tr><tr><td>eval/mcc_metric</td><td>▁█████▅</td></tr><tr><td>eval/runtime</td><td>▆▅▇█▁▄▅</td></tr><tr><td>eval/samples_per_second</td><td>▃▄▂▁█▅▄</td></tr><tr><td>eval/steps_per_second</td><td>▃▄▂▁█▅▄</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▅▆▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▅▆▆▇██</td></tr><tr><td>train/grad_norm</td><td>█▁</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.98876</td></tr><tr><td>eval/Accuracy</td><td>0.97902</td></tr><tr><td>eval/F1-score</td><td>0.88</td></tr><tr><td>eval/Precision</td><td>0.91667</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.37491</td></tr><tr><td>eval/mcc_metric</td><td>0.86936</td></tr><tr><td>eval/runtime</td><td>0.3966</td></tr><tr><td>eval/samples_per_second</td><td>360.556</td></tr><tr><td>eval/steps_per_second</td><td>12.607</td></tr><tr><td>total_flos</td><td>340255396730088.0</td></tr><tr><td>train/epoch</td><td>7</td></tr><tr><td>train/global_step</td><td>266</td></tr><tr><td>train/grad_norm</td><td>0.04047</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.0982</td></tr><tr><td>train_loss</td><td>0.19396</td></tr><tr><td>train_runtime</td><td>56.7113</td></tr><tr><td>train_samples_per_second</td><td>417.906</td></tr><tr><td>train_steps_per_second</td><td>13.401</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-sweep-9</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/btpw4450' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/btpw4450</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_081439-btpw4450/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g66590oz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.361425656452323e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_081556-g66590oz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/g66590oz' target=\"_blank\">atomic-sweep-10</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/ambvlx5s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/g66590oz' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/g66590oz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,844,610 || all params: 50,525,316 || trainable%: 9.5885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1868290/36713449.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='304' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [304/760 01:05 < 01:39, 4.59 it/s, Epoch 8/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.492128</td>\n",
       "      <td>0.558934</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.946154</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.355241</td>\n",
       "      <td>0.844375</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.985799</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.467000</td>\n",
       "      <td>0.253257</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.989941</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.467000</td>\n",
       "      <td>0.169533</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995858</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.467000</td>\n",
       "      <td>0.163836</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.185500</td>\n",
       "      <td>0.170438</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.995266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.185500</td>\n",
       "      <td>0.139319</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.996450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.125500</td>\n",
       "      <td>0.155448</td>\n",
       "      <td>0.912871</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.997041</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_molformer_clintox/g66590oz\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▆▇█████</td></tr><tr><td>eval/Accuracy</td><td>▁▇██████</td></tr><tr><td>eval/F1-score</td><td>▁▇██████</td></tr><tr><td>eval/Precision</td><td>▁▆▇▇████</td></tr><tr><td>eval/Recall</td><td>▁███▁▁▁▁</td></tr><tr><td>eval/loss</td><td>█▅▃▂▁▂▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▇██████</td></tr><tr><td>eval/runtime</td><td>▂▁▅▂▅▂▂█</td></tr><tr><td>eval/samples_per_second</td><td>▇█▄█▄▇▇▁</td></tr><tr><td>eval/steps_per_second</td><td>▇█▄█▄▇▇▁</td></tr><tr><td>train/epoch</td><td>▁▂▃▃▄▅▅▆▇███</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▄▅▅▆▇███</td></tr><tr><td>train/grad_norm</td><td>█▁▁</td></tr><tr><td>train/learning_rate</td><td>█▅▁</td></tr><tr><td>train/loss</td><td>█▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99704</td></tr><tr><td>eval/Accuracy</td><td>0.98601</td></tr><tr><td>eval/F1-score</td><td>0.91667</td></tr><tr><td>eval/Precision</td><td>1</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.15545</td></tr><tr><td>eval/mcc_metric</td><td>0.91287</td></tr><tr><td>eval/runtime</td><td>0.4007</td></tr><tr><td>eval/samples_per_second</td><td>356.918</td></tr><tr><td>eval/steps_per_second</td><td>12.48</td></tr><tr><td>total_flos</td><td>405128764576800.0</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>304</td></tr><tr><td>train/grad_norm</td><td>0.4608</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.1255</td></tr><tr><td>train_loss</td><td>0.25617</td></tr><tr><td>train_runtime</td><td>65.9972</td></tr><tr><td>train_samples_per_second</td><td>359.106</td></tr><tr><td>train_steps_per_second</td><td>11.516</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-sweep-10</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/g66590oz' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/g66590oz</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_081556-g66590oz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_runtime': 73.726774537, '_step': 11, '_timestamp': 1744265830.85145, '_wandb': {'runtime': 74}, 'eval/AUC-ROC': 0.9970414201183432, 'eval/Accuracy': 0.986013986013986, 'eval/F1-score': 0.9166666666666666, 'eval/Precision': 1, 'eval/Recall': 0.8461538461538461, 'eval/loss': 0.15544822812080383, 'eval/mcc_metric': 0.9128709291752768, 'eval/runtime': 0.4007, 'eval/samples_per_second': 356.918, 'eval/steps_per_second': 12.48, 'total_flos': 405128764576800, 'train/epoch': 8, 'train/global_step': 304, 'train/grad_norm': 0.46080031991004944, 'train/learning_rate': 8.24020792063248e-06, 'train/loss': 0.1255, 'train_loss': 0.2561702096197558, 'train_runtime': 65.9972, 'train_samples_per_second': 359.106, 'train_steps_per_second': 11.516}\n",
      "{'r': 16, 'lr': 1.823268967202021e-05, 'bf16': False, 'fp16': False, 'fsdp': [], 'seed': 42, 'tf32': None, 'debug': [], 'optim': 'adamw_torch', 'top_k': 50, 'top_p': 1, 'prefix': None, 'do_eval': True, 'dropout': 0.2, 'no_cuda': False, 'tp_size': 0, 'use_cpu': False, 'auto_map': {'AutoModel': 'ibm/MoLFormer-XL-both-10pct--modeling_molformer.MolformerModel', 'AutoConfig': 'ibm/MoLFormer-XL-both-10pct--configuration_molformer.MolformerConfig', 'AutoModelForMaskedLM': 'ibm/MoLFormer-XL-both-10pct--modeling_molformer.MolformerForMaskedLM', 'AutoModelForSequenceClassification': 'ibm/MoLFormer-XL-both-10pct--modeling_molformer.MolformerForSequenceClassification'}, 'do_train': False, 'id2label': {'0': 'LABEL_0', '1': 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'run_name': './models_molformer_clintox/btpw4450', 'use_ipex': False, 'adafactor': False, 'data_seed': None, 'deepspeed': None, 'do_sample': False, 'hub_token': '<HUB_TOKEN>', 'log_level': 'passive', 'max_steps': -1, 'num_beams': 1, 'ray_scope': 'last', 'report_to': ['wandb'], 'typical_p': 1, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'do_predict': False, 'eval_delay': 0, 'eval_steps': None, 'hidden_act': 'gelu', 'is_decoder': False, 'local_rank': 0, 'lora_alpha': 128, 'max_length': 20, 'min_length': 0, 'model_type': 'molformer', 'optim_args': None, 'output_dir': './models_molformer_clintox/btpw4450', 'past_index': -1, 'save_steps': 500, 'vocab_size': 2362, 'ddp_backend': None, 'ddp_timeout': 1800, 'fsdp_config': {'xla': False, 'xla_fsdp_v2': False, 'min_num_params': 0, 'xla_fsdp_grad_ckpt': False}, 'hidden_size': 768, 'label_names': None, 'logging_dir': './logs_clin/btpw4450', 'peft_config': {'default': {'r': 16, 'bias': 'none', 'revision': None, 'use_dora': False, 'lora_bias': False, 'peft_type': 'LORA', 'task_type': 'SEQ_CLS', 'eva_config': None, 'lora_alpha': 128, 'use_rslora': False, 'auto_mapping': None, 'lora_dropout': 0.2, 'megatron_core': 'megatron.core', 'fan_in_fan_out': False, 'inference_mode': False, 'layers_pattern': None, 'runtime_config': {'ephemeral_gpu_offload': False}, 'target_modules': ['value', 'query', 'dense', 'dense2', 'out_proj', 'key'], 'exclude_modules': None, 'megatron_config': None, 'modules_to_save': ['classifier', 'score'], 'init_lora_weights': True, 'layer_replication': None, 'layers_to_transform': None, 'base_model_name_or_path': 'ibm/MoLFormer-XL-both-10pct'}}, 'push_to_hub': False, 'return_dict': True, 'temperature': 1, 'torch_dtype': 'float32', 'torchdynamo': None, 'torchscript': False, 'adam_epsilon': 1e-08, 'bos_token_id': None, 'disable_tqdm': False, 'eos_token_id': None, 'fp16_backend': 'auto', 'hub_model_id': None, 'hub_strategy': 'every_save', 'pad_token_id': 2, 'problem_type': 'single_label_classification', 'sep_token_id': None, 'use_bfloat16': False, 'warmup_ratio': 0, 'warmup_steps': 0, 'weight_decay': 0.01, '_name_or_path': 'ibm/MoLFormer-XL-both-10pct', 'architectures': ['MolformerForMaskedLM'], 'bad_words_ids': None, 'eval_on_start': False, 'eval_strategy': 'epoch', 'jit_mode_eval': False, 'learning_rate': 1.823268967202021e-05, 'logging_steps': 100, 'max_grad_norm': 1, 'mp_parameters': '', 'output_scores': False, 'save_strategy': 'epoch', 'split_batches': None, 'torch_compile': False, 'tpu_num_cores': None, 'bf16_full_eval': False, 'early_stopping': False, 'fp16_full_eval': False, 'fp16_opt_level': 'O1', 'layer_norm_eps': 1e-12, 'length_penalty': 1, 'tf_legacy_loss': False, 'use_mps_device': False, 'finetuning_task': None, 'group_by_length': False, 'hub_always_push': False, 'num_beam_groups': 1, 'save_only_model': False, 'suppress_tokens': None, 'tokenizer_class': None, 'dispatch_batches': None, 'full_determinism': False, 'hub_private_repo': None, 'ignore_data_skip': False, 'log_on_each_node': True, 'logging_strategy': 'steps', 'num_train_epochs': 20, 'save_safetensors': True, 'save_total_limit': 5, 'use_liger_kernel': False, 'ddp_bucket_cap_mb': None, 'diversity_penalty': 0, 'greater_is_better': True, 'initializer_range': 0.02, 'intermediate_size': 768, 'log_level_replica': 'warning', 'lr_scheduler_type': 'linear', 'num_hidden_layers': 12, 'output_attentions': False, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'save_on_each_node': False, 'tpu_metrics_debug': False, 'accelerator_config': {'even_batches': True, 'non_blocking': False, 'split_batches': False, 'dispatch_batches': None, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None}, 'batch_eval_metrics': False, 'deterministic_eval': False, 'feature_map_kernel': 'relu', 'is_encoder_decoder': False, 'length_column_name': 'length', 'logging_first_step': False, 'repetition_penalty': 1, 'torch_compile_mode': None, 'add_cross_attention': False, 'evaluation_strategy': 'epoch', 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'fsdp_min_num_params': 0, 'hidden_dropout_prob': 0.1, 'include_for_metrics': [], 'neftune_noise_alpha': None, 'num_attention_heads': 12, 'num_random_features': 32, 'skip_memory_metrics': True, 'tie_encoder_decoder': False, 'tie_word_embeddings': False, 'auto_find_batch_size': False, 'dataloader_drop_last': False, 'linear_attention_eps': 1e-06, 'model/num_parameters': 48632900, 'no_repeat_ngram_size': 0, 'num_return_sequences': 1, 'optim_target_modules': None, 'output_hidden_states': False, 'overwrite_output_dir': False, 'prediction_loss_only': False, 'push_to_hub_model_id': None, 'task_specific_params': None, 'transformers_version': '4.50.0', 'begin_suppress_tokens': None, 'dataloader_pin_memory': True, 'ddp_broadcast_buffers': None, 'metric_for_best_model': 'eval_mcc_metric', 'remove_invalid_values': False, 'remove_unused_columns': True, 'torch_compile_backend': None, 'dataloader_num_workers': 0, 'decoder_start_token_id': None, 'embedding_dropout_prob': 0.2, 'eval_do_concat_batches': True, 'eval_use_gather_object': False, 'gradient_checkpointing': False, 'half_precision_backend': 'auto', 'label_smoothing_factor': 0, 'load_best_model_at_end': True, 'logging_nan_inf_filter': True, 'resume_from_checkpoint': None, 'chunk_size_feed_forward': 0, 'classifier_dropout_prob': None, 'eval_accumulation_steps': None, 'max_position_embeddings': 202, 'per_gpu_eval_batch_size': None, 'return_dict_in_generate': False, 'torch_empty_cache_steps': None, 'per_gpu_train_batch_size': None, 'push_to_hub_organization': None, 'include_tokens_per_second': False, 'classifier_skip_connection': True, 'dataloader_prefetch_factor': None, 'ddp_find_unused_parameters': None, 'include_inputs_for_metrics': False, 'per_device_eval_batch_size': 32, 'use_legacy_prediction_loop': False, 'cross_attention_hidden_size': None, 'gradient_accumulation_steps': 1, 'per_device_train_batch_size': 32, '_attn_implementation_autoset': True, 'encoder_no_repeat_ngram_size': 0, 'average_tokens_across_devices': False, 'dataloader_persistent_workers': False, 'gradient_checkpointing_kwargs': None, 'include_num_input_tokens_seen': False, 'exponential_decay_length_penalty': None, 'fsdp_transformer_layer_cls_to_wrap': None, 'restore_callback_states_from_checkpoint': False}\n"
     ]
    }
   ],
   "source": [
    " # Define the sweep configuration\n",
    "def main():\n",
    "\n",
    "    sweep_config = {\n",
    "    \"name\": \"Clintox Hyperparameter Tuning\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"goal\": \"maximize\",\n",
    "        \"name\": \"eval/mcc_metric\"\n",
    "        },\n",
    "    \"parameters\": {\n",
    "\n",
    "        \"lr\": {\n",
    "\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 1e-5,\n",
    "            \"max\": 2e-5\n",
    "        },\n",
    "        \"r\": {\n",
    "            \"values\": [4, 8, 16, 32,64]\n",
    "        },\n",
    "        \"lora_alpha\": {\n",
    "            \"values\": [8, 16, 32, 64,128]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"values\": [0.0, 0.1, 0.2]\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"huggingface\")\n",
    "    wandb.agent(sweep_id, function=run_training, count=10)\n",
    "\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(f\"huggingface/{sweep_id}\")\n",
    "    print(sweep.runs[0].summary_metrics)\n",
    "\n",
    "    runs_with_eval_loss = [run for run in sweep.runs if 'eval/mcc_metric' in run.summary_metrics]\n",
    "\n",
    "    if runs_with_eval_loss:\n",
    "        best_run = sorted(runs_with_eval_loss, key=lambda run: run.summary_metrics['eval/mcc_metric'],reverse=False)[0]\n",
    "    else:\n",
    "        raise ValueError(\"No runs found with 'eval/mcc_metric' metric.\")\n",
    "\n",
    "    best_hyperparameters = best_run.config\n",
    "    print(best_hyperparameters)\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"ibm/MoLFormer-XL-both-10pct\",\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True,\n",
    "    deterministic_eval=True\n",
    ")\n",
    "\n",
    "tokenizer_clin = AutoTokenizer.from_pretrained(\n",
    "    \"ibm/MoLFormer-XL-both-10pct\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n"
     ]
    }
   ],
   "source": [
    "adapter_model = PeftModel.from_pretrained(base_model, '/home/raghvendra2/Molformer_Finetuning/models/q3mia2tx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on trained model\n",
    "\n",
    "- bad performance- best_clintox_w__model\n",
    "\n",
    "- best performance: best_clintox_model and checkpoint-150\n",
    "\n",
    "- with 0.2 didnt replicate the same model, lets check for 0.3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data_clin=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/clintox_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "smiles_test_clin = test_data_clin['smiles'].tolist()\n",
    "\n",
    "test_tokenized_clin =tokenizer_clin(smiles_test_clin)\n",
    "\n",
    "test_dataset_clin = Dataset.from_dict(test_tokenized_clin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_clin = test_data_clin['CT_TOX'].tolist() \n",
    "\n",
    "\n",
    "test_dataset_clin = test_dataset_clin.add_column(\"labels\", test_labels_clin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = softmax(logits, axis=1)[:, 1]  # Get probabilities for class 1\n",
    "        predictions = np.argmax(logits, axis=1)  # Choose the most likely class\n",
    "        mcc = matthews_corrcoef(labels, predictions)\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities),  # AUC-ROC requires probabilities\n",
    "            \"Precision\": precision_score(labels, predictions),\n",
    "            \"Recall\": recall_score(labels, predictions),\n",
    "            \"F1-score\": f1_score(labels, predictions)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"./test_results_clintox_wandb\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    report_to=\"none\",  # Disable logging to W&B for test\n",
    "    seed=42,  # Ensures reproducibility\n",
    "\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "\"ibm/MoLFormer-XL-both-10pct\",\n",
    "trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adapter_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m WeightedLossTrainer(\n\u001b[0;32m----> 2\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[43madapter_model\u001b[49m,\n\u001b[1;32m      3\u001b[0m         args\u001b[38;5;241m=\u001b[39meval_args,\n\u001b[1;32m      4\u001b[0m         eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset_clin,\n\u001b[1;32m      5\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m      6\u001b[0m         compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m      7\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adapter_model' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = WeightedLossTrainer(\n",
    "        model=adapter_model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=test_dataset_clin,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/9 00:00 < 00:00, 24.01 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for model2: {'eval_mcc_metric': 0.9128709291752768, 'eval_loss': 0.23767508566379547, 'eval_model_preparation_time': 0.0144, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.993491124260355, 'eval_Precision': 1.0, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 0.3836, 'eval_samples_per_second': 372.794, 'eval_steps_per_second': 23.463}\n"
     ]
    }
   ],
   "source": [
    "test_results_clin = trainer.evaluate()\n",
    "\n",
    "print(\"Test Results for model2:\", test_results_clin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Valid nested checkpoints found: ['./models_molformer_clintox/kbo8a99q/checkpoint-266', './models_molformer_clintox/kbo8a99q/checkpoint-304', './models_molformer_clintox/kbo8a99q/checkpoint-114', './models_molformer_clintox/kbo8a99q/checkpoint-228', './models_molformer_clintox/kbo8a99q/checkpoint-190', './models_molformer_clintox/gulowmgl/checkpoint-304', './models_molformer_clintox/gulowmgl/checkpoint-228', './models_molformer_clintox/gulowmgl/checkpoint-342', './models_molformer_clintox/gulowmgl/checkpoint-380', './models_molformer_clintox/gulowmgl/checkpoint-418', './models_molformer_clintox/jvf2j5zv/checkpoint-76', './models_molformer_clintox/jvf2j5zv/checkpoint-266', './models_molformer_clintox/jvf2j5zv/checkpoint-228', './models_molformer_clintox/jvf2j5zv/checkpoint-152', './models_molformer_clintox/jvf2j5zv/checkpoint-190', './models_molformer_clintox/g66590oz/checkpoint-266', './models_molformer_clintox/g66590oz/checkpoint-304', './models_molformer_clintox/g66590oz/checkpoint-114', './models_molformer_clintox/g66590oz/checkpoint-228', './models_molformer_clintox/g66590oz/checkpoint-190', './models_molformer_clintox/o1vbzyf5/checkpoint-76', './models_molformer_clintox/o1vbzyf5/checkpoint-266', './models_molformer_clintox/o1vbzyf5/checkpoint-228', './models_molformer_clintox/o1vbzyf5/checkpoint-152', './models_molformer_clintox/o1vbzyf5/checkpoint-190', './models_molformer_clintox/3wzclq6q/checkpoint-266', './models_molformer_clintox/3wzclq6q/checkpoint-304', './models_molformer_clintox/3wzclq6q/checkpoint-114', './models_molformer_clintox/3wzclq6q/checkpoint-228', './models_molformer_clintox/3wzclq6q/checkpoint-190', './models_molformer_clintox/lm8vser5/checkpoint-76', './models_molformer_clintox/lm8vser5/checkpoint-266', './models_molformer_clintox/lm8vser5/checkpoint-228', './models_molformer_clintox/lm8vser5/checkpoint-152', './models_molformer_clintox/lm8vser5/checkpoint-190', './models_molformer_clintox/btpw4450/checkpoint-76', './models_molformer_clintox/btpw4450/checkpoint-266', './models_molformer_clintox/btpw4450/checkpoint-228', './models_molformer_clintox/btpw4450/checkpoint-152', './models_molformer_clintox/btpw4450/checkpoint-190', './models_molformer_clintox/k121h7u1/checkpoint-304', './models_molformer_clintox/k121h7u1/checkpoint-228', './models_molformer_clintox/k121h7u1/checkpoint-342', './models_molformer_clintox/k121h7u1/checkpoint-380', './models_molformer_clintox/k121h7u1/checkpoint-418', './models_molformer_clintox/2diwcmfr/checkpoint-266', './models_molformer_clintox/2diwcmfr/checkpoint-456', './models_molformer_clintox/2diwcmfr/checkpoint-342', './models_molformer_clintox/2diwcmfr/checkpoint-380', './models_molformer_clintox/2diwcmfr/checkpoint-418']\n",
      "\n",
      "🔍 Evaluating model: kbo8a99q/checkpoint-266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kbo8a99q/checkpoint-266\n",
      "\n",
      "🔍 Evaluating model: kbo8a99q/checkpoint-304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kbo8a99q/checkpoint-304\n",
      "\n",
      "🔍 Evaluating model: kbo8a99q/checkpoint-114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kbo8a99q/checkpoint-114\n",
      "\n",
      "🔍 Evaluating model: kbo8a99q/checkpoint-228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kbo8a99q/checkpoint-228\n",
      "\n",
      "🔍 Evaluating model: kbo8a99q/checkpoint-190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kbo8a99q/checkpoint-190\n",
      "\n",
      "🔍 Evaluating model: gulowmgl/checkpoint-304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for gulowmgl/checkpoint-304\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9570948408340445, 'eval_loss': 0.15103395283222198, 'eval_model_preparation_time': 0.013, 'eval_Accuracy': 0.993006993006993, 'eval_AUC-ROC': 0.9976331360946745, 'eval_Precision': 1.0, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.96, 'eval_runtime': 0.38, 'eval_samples_per_second': 376.282, 'eval_steps_per_second': 23.682}\n",
      "\n",
      "🔍 Evaluating model: gulowmgl/checkpoint-228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for gulowmgl/checkpoint-228\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9570948408340445, 'eval_loss': 0.18789832293987274, 'eval_model_preparation_time': 0.0132, 'eval_Accuracy': 0.993006993006993, 'eval_AUC-ROC': 0.9976331360946745, 'eval_Precision': 1.0, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.96, 'eval_runtime': 0.3793, 'eval_samples_per_second': 376.961, 'eval_steps_per_second': 23.725}\n",
      "\n",
      "🔍 Evaluating model: gulowmgl/checkpoint-342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for gulowmgl/checkpoint-342\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9570948408340445, 'eval_loss': 0.14021161198616028, 'eval_model_preparation_time': 0.0128, 'eval_Accuracy': 0.993006993006993, 'eval_AUC-ROC': 0.9982248520710059, 'eval_Precision': 1.0, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.96, 'eval_runtime': 0.3805, 'eval_samples_per_second': 375.846, 'eval_steps_per_second': 23.655}\n",
      "\n",
      "🔍 Evaluating model: gulowmgl/checkpoint-380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for gulowmgl/checkpoint-380\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9570948408340445, 'eval_loss': 0.12716948986053467, 'eval_model_preparation_time': 0.0129, 'eval_Accuracy': 0.993006993006993, 'eval_AUC-ROC': 0.9976331360946745, 'eval_Precision': 1.0, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.96, 'eval_runtime': 0.381, 'eval_samples_per_second': 375.313, 'eval_steps_per_second': 23.621}\n",
      "\n",
      "🔍 Evaluating model: gulowmgl/checkpoint-418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for gulowmgl/checkpoint-418\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9128709291752768, 'eval_loss': 0.12895627319812775, 'eval_model_preparation_time': 0.0131, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.9976331360946745, 'eval_Precision': 1.0, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 0.383, 'eval_samples_per_second': 373.345, 'eval_steps_per_second': 23.497}\n",
      "\n",
      "🔍 Evaluating model: jvf2j5zv/checkpoint-76\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping jvf2j5zv/checkpoint-76\n",
      "\n",
      "🔍 Evaluating model: jvf2j5zv/checkpoint-266\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for jvf2j5zv/checkpoint-266\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9128709291752768, 'eval_loss': 0.13314110040664673, 'eval_model_preparation_time': 0.013, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.9988165680473373, 'eval_Precision': 1.0, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 0.3788, 'eval_samples_per_second': 377.483, 'eval_steps_per_second': 23.758}\n",
      "\n",
      "🔍 Evaluating model: jvf2j5zv/checkpoint-228\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for jvf2j5zv/checkpoint-228\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9128709291752768, 'eval_loss': 0.14308039844036102, 'eval_model_preparation_time': 0.0129, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.9988165680473373, 'eval_Precision': 1.0, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 0.3783, 'eval_samples_per_second': 377.995, 'eval_steps_per_second': 23.79}\n",
      "\n",
      "🔍 Evaluating model: jvf2j5zv/checkpoint-152\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for jvf2j5zv/checkpoint-152\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9128709291752768, 'eval_loss': 0.17804013192653656, 'eval_model_preparation_time': 0.0129, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.998224852071006, 'eval_Precision': 1.0, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 0.38, 'eval_samples_per_second': 376.322, 'eval_steps_per_second': 23.685}\n",
      "\n",
      "🔍 Evaluating model: jvf2j5zv/checkpoint-190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for jvf2j5zv/checkpoint-190\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9128709291752768, 'eval_loss': 0.14329198002815247, 'eval_model_preparation_time': 0.0129, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.9988165680473373, 'eval_Precision': 1.0, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 0.3787, 'eval_samples_per_second': 377.588, 'eval_steps_per_second': 23.764}\n",
      "\n",
      "🔍 Evaluating model: g66590oz/checkpoint-266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping g66590oz/checkpoint-266\n",
      "\n",
      "🔍 Evaluating model: g66590oz/checkpoint-304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping g66590oz/checkpoint-304\n",
      "\n",
      "🔍 Evaluating model: g66590oz/checkpoint-114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping g66590oz/checkpoint-114\n",
      "\n",
      "🔍 Evaluating model: g66590oz/checkpoint-228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping g66590oz/checkpoint-228\n",
      "\n",
      "🔍 Evaluating model: g66590oz/checkpoint-190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping g66590oz/checkpoint-190\n",
      "\n",
      "🔍 Evaluating model: o1vbzyf5/checkpoint-76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping o1vbzyf5/checkpoint-76\n",
      "\n",
      "🔍 Evaluating model: o1vbzyf5/checkpoint-266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping o1vbzyf5/checkpoint-266\n",
      "\n",
      "🔍 Evaluating model: o1vbzyf5/checkpoint-228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping o1vbzyf5/checkpoint-228\n",
      "\n",
      "🔍 Evaluating model: o1vbzyf5/checkpoint-152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping o1vbzyf5/checkpoint-152\n",
      "\n",
      "🔍 Evaluating model: o1vbzyf5/checkpoint-190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping o1vbzyf5/checkpoint-190\n",
      "\n",
      "🔍 Evaluating model: 3wzclq6q/checkpoint-266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 3wzclq6q/checkpoint-266\n",
      "\n",
      "🔍 Evaluating model: 3wzclq6q/checkpoint-304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 3wzclq6q/checkpoint-304\n",
      "\n",
      "🔍 Evaluating model: 3wzclq6q/checkpoint-114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 3wzclq6q/checkpoint-114\n",
      "\n",
      "🔍 Evaluating model: 3wzclq6q/checkpoint-228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 3wzclq6q/checkpoint-228\n",
      "\n",
      "🔍 Evaluating model: 3wzclq6q/checkpoint-190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 3wzclq6q/checkpoint-190\n",
      "\n",
      "🔍 Evaluating model: lm8vser5/checkpoint-76\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping lm8vser5/checkpoint-76\n",
      "\n",
      "🔍 Evaluating model: lm8vser5/checkpoint-266\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for lm8vser5/checkpoint-266\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9128709291752768, 'eval_loss': 0.16216017305850983, 'eval_model_preparation_time': 0.0135, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.9988165680473373, 'eval_Precision': 1.0, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 0.3793, 'eval_samples_per_second': 377.045, 'eval_steps_per_second': 23.73}\n",
      "\n",
      "🔍 Evaluating model: lm8vser5/checkpoint-228\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for lm8vser5/checkpoint-228\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9128709291752768, 'eval_loss': 0.18674971163272858, 'eval_model_preparation_time': 0.0129, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.998224852071006, 'eval_Precision': 1.0, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 0.3793, 'eval_samples_per_second': 376.996, 'eval_steps_per_second': 23.727}\n",
      "\n",
      "🔍 Evaluating model: lm8vser5/checkpoint-152\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for lm8vser5/checkpoint-152\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9128709291752768, 'eval_loss': 0.26440030336380005, 'eval_model_preparation_time': 0.013, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.9976331360946745, 'eval_Precision': 1.0, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 0.3794, 'eval_samples_per_second': 376.943, 'eval_steps_per_second': 23.724}\n",
      "\n",
      "🔍 Evaluating model: lm8vser5/checkpoint-190\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for lm8vser5/checkpoint-190\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9128709291752768, 'eval_loss': 0.21533715724945068, 'eval_model_preparation_time': 0.0128, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.9976331360946746, 'eval_Precision': 1.0, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 0.3805, 'eval_samples_per_second': 375.772, 'eval_steps_per_second': 23.65}\n",
      "\n",
      "🔍 Evaluating model: btpw4450/checkpoint-76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping btpw4450/checkpoint-76\n",
      "\n",
      "🔍 Evaluating model: btpw4450/checkpoint-266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping btpw4450/checkpoint-266\n",
      "\n",
      "🔍 Evaluating model: btpw4450/checkpoint-228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping btpw4450/checkpoint-228\n",
      "\n",
      "🔍 Evaluating model: btpw4450/checkpoint-152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping btpw4450/checkpoint-152\n",
      "\n",
      "🔍 Evaluating model: btpw4450/checkpoint-190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping btpw4450/checkpoint-190\n",
      "\n",
      "🔍 Evaluating model: k121h7u1/checkpoint-304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping k121h7u1/checkpoint-304\n",
      "\n",
      "🔍 Evaluating model: k121h7u1/checkpoint-228\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping k121h7u1/checkpoint-228\n",
      "\n",
      "🔍 Evaluating model: k121h7u1/checkpoint-342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping k121h7u1/checkpoint-342\n",
      "\n",
      "🔍 Evaluating model: k121h7u1/checkpoint-380\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping k121h7u1/checkpoint-380\n",
      "\n",
      "🔍 Evaluating model: k121h7u1/checkpoint-418\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping k121h7u1/checkpoint-418\n",
      "\n",
      "🔍 Evaluating model: 2diwcmfr/checkpoint-266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for 2diwcmfr/checkpoint-266\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9570948408340445, 'eval_loss': 0.13525691628456116, 'eval_model_preparation_time': 0.0129, 'eval_Accuracy': 0.993006993006993, 'eval_AUC-ROC': 0.9982248520710059, 'eval_Precision': 1.0, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.96, 'eval_runtime': 0.3913, 'eval_samples_per_second': 365.484, 'eval_steps_per_second': 23.002}\n",
      "\n",
      "🔍 Evaluating model: 2diwcmfr/checkpoint-456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for 2diwcmfr/checkpoint-456\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9128709291752768, 'eval_loss': 0.12335965037345886, 'eval_model_preparation_time': 0.0131, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.9982248520710059, 'eval_Precision': 1.0, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 0.3928, 'eval_samples_per_second': 364.084, 'eval_steps_per_second': 22.914}\n",
      "\n",
      "🔍 Evaluating model: 2diwcmfr/checkpoint-342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for 2diwcmfr/checkpoint-342\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9128709291752768, 'eval_loss': 0.1257503479719162, 'eval_model_preparation_time': 0.013, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.9982248520710059, 'eval_Precision': 1.0, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 0.3912, 'eval_samples_per_second': 365.547, 'eval_steps_per_second': 23.006}\n",
      "\n",
      "🔍 Evaluating model: 2diwcmfr/checkpoint-380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for 2diwcmfr/checkpoint-380\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9570948408340445, 'eval_loss': 0.11523086577653885, 'eval_model_preparation_time': 0.0136, 'eval_Accuracy': 0.993006993006993, 'eval_AUC-ROC': 0.9976331360946745, 'eval_Precision': 1.0, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.96, 'eval_runtime': 0.3919, 'eval_samples_per_second': 364.857, 'eval_steps_per_second': 22.963}\n",
      "\n",
      "🔍 Evaluating model: 2diwcmfr/checkpoint-418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1868290/1147041880.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for 2diwcmfr/checkpoint-418\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9128709291752768, 'eval_loss': 0.1249915137887001, 'eval_model_preparation_time': 0.0132, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.9982248520710059, 'eval_Precision': 1.0, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 0.3942, 'eval_samples_per_second': 362.729, 'eval_steps_per_second': 22.829}\n"
     ]
    }
   ],
   "source": [
    "# List all checkpoints inside models directory\n",
    "import os\n",
    "from peft import PeftModel\n",
    "\n",
    "models_dir = \"./models_molformer_clintox\"\n",
    "\n",
    "def find_all_checkpoints(base_dir):\n",
    "    all_checkpoints = []\n",
    "    for folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for subfolder in os.listdir(folder_path):\n",
    "                subfolder_path = os.path.join(folder_path, subfolder)\n",
    "                if os.path.isdir(subfolder_path) and subfolder.startswith(\"checkpoint-\"):\n",
    "                    if os.path.exists(os.path.join(subfolder_path, \"adapter_config.json\")):\n",
    "                        all_checkpoints.append(subfolder_path)\n",
    "    return all_checkpoints\n",
    "\n",
    "valid_checkpoints = find_all_checkpoints(models_dir)\n",
    "print(\"🧠 Valid nested checkpoints found:\", valid_checkpoints)\n",
    "\n",
    "for checkpoint_path in valid_checkpoints:\n",
    "    checkpoint_name = os.path.basename(checkpoint_path)\n",
    "    parent_folder = os.path.basename(os.path.dirname(checkpoint_path))\n",
    "\n",
    "    print(f\"\\n🔍 Evaluating model: {parent_folder}/{checkpoint_name}\")\n",
    "\n",
    "    adapter_model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    adapter_model.eval()\n",
    "\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=adapter_model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=test_dataset_clin,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "\n",
    "    test_results = trainer.evaluate()\n",
    "    auc_score = test_results[\"eval_AUC-ROC\"]\n",
    "    if auc_score > 0.997:\n",
    "        print(f\"✅ AUC_ROC > 0.99 for {parent_folder}/{checkpoint_name}\")\n",
    "        print(f\"📌 Test Results: {test_results}\")\n",
    "    else:\n",
    "        print(f\"❌ Skipping {parent_folder}/{checkpoint_name}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best model: lm8vser5/checkpoint-266\n",
    "\n",
    "trainable params: 1,640,458 || all params: 47,213,588 || trainable%: 3.4745\n",
    "\n",
    "📌 Test Results: {'eval_mcc_metric': 0.9128709291752768, 'eval_loss': 0.16216017305850983, 'eval_model_preparation_time': 0.0135, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.9988165680473373, 'eval_Precision': 1.0, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 0.3793, 'eval_samples_per_second': 377.045, 'eval_steps_per_second': 23.73}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Merge the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'ibm/MoLFormer-XL-both-10pct',\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel  \n",
    "\n",
    "adapter_model = PeftModel.from_pretrained(base_model, \"/home/raghvendra2/Molformer_Finetuning/models_molformer_clintox/lm8vser5/checkpoint-266\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_clintox_molformer= adapter_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model to Chemberta finetuned model lora 100M MTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/raghvendra2/Molformer_Finetuning/Clintox_Final_Molformer_model\"\n",
    "\n",
    "final_model_clintox_molformer.save_pretrained(save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Molformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
