{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Chemberata Model for Clintox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-10 10:26:45.568083: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-10 10:26:45.588560: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744273605.614111 1883300 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744273605.621907 1883300 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744273605.641323 1883300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744273605.641340 1883300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744273605.641343 1883300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744273605.641344 1883300 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-10 10:26:45.647431: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "import evaluate\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Tokenizer and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Chemberta 77M MLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "### Chemberta 77M MTR Model\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_clin = AutoTokenizer.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-5M-MTR\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load the model with a classification head\n",
    "model_clin = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-5M-MTR\",\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_clin=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/clintox_train.csv')\n",
    "val_clin=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/clintox_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list_clin = train_clin['smiles'].tolist()\n",
    "smiles_val_clin=val_clin['smiles'].tolist()\n",
    "train_tokenized_clin=tokenizer_clin(smiles_list_clin)\n",
    "val_tokenized_clin=tokenizer_clin(smiles_val_clin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset_clin = Dataset.from_dict(train_tokenized_clin)\n",
    "val_dataset_clin = Dataset.from_dict(val_tokenized_clin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_clin = train_clin['CT_TOX'].tolist() # Assuming tasks start from column 1\n",
    "val_labels_clin = val_clin['CT_TOX'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_clin = train_dataset_clin.add_column(\"labels\", train_labels_clin)\n",
    "val_dataset_clin = val_dataset_clin.add_column(\"labels\", val_labels_clin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load():\n",
    "    train_clin=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/clintox_train.csv')\n",
    "    val_clin=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/clintox_test.csv')\n",
    "\n",
    "    return train_clin, val_clin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data_process,tokenizer_clin):\n",
    "\n",
    "    smiles_list_clin = data_process['smiles'].tolist()\n",
    "    tokenized_clin=tokenizer_clin(smiles_list_clin)\n",
    "    \n",
    "    \n",
    "    dataset_clin = Dataset.from_dict(tokenized_clin)\n",
    "    \n",
    "\n",
    "    labels_clin = data_process['CT_TOX'].tolist() # Assuming tasks start from column 1\n",
    "    \n",
    "    dataset_clin = dataset_clin.add_column(\"labels\", labels_clin)\n",
    "    \n",
    "\n",
    "    return dataset_clin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "def lora_config(r, lora_alpha, dropout):\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",  # Sequence classification task\n",
    "        r=r,  # Rank of LoRA matrices\n",
    "        lora_alpha=lora_alpha,  # Scaling factor double of rank( from the rule of thumb)\n",
    "        target_modules='all-linear',\n",
    "        lora_dropout=dropout  # Dropout rate\n",
    "        #init_lora_weights=\"gaussian\"\n",
    "    )\n",
    "\n",
    "    return lora_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class_weights= [1-(train_dataset_clin['labels'].count(0)/len(train_dataset_clin['labels'])),\n",
    "                           1-(train_dataset_clin['labels'].count(1)/len(train_dataset_clin['labels']))]\n",
    "\n",
    "class_weights = torch.from_numpy(np.array(class_weights)).float().to(\"cuda\")\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Extract labels\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # compute custom loss (suppose one has 2 labels with different weights)\n",
    "        loss_func = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_func(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "# initialize wandb with sweep \n",
    "def run_training():\n",
    "\n",
    "    run = wandb.init(project=\"Clintox Hyperparameter Tuning\")\n",
    "    config = run.config\n",
    "    \n",
    "\n",
    "    tokenizer_clin = AutoTokenizer.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-5M-MTR\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    save_dir = f\"./clintox_models_5MTR/{wandb.run.id}\"  # Unique directory for each run\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "#Data\n",
    "\n",
    "    train_data, val_data=data_load()\n",
    "    training_data=data_prep(train_data,tokenizer_clin)\n",
    "    validation_data=data_prep(val_data,tokenizer_clin)    \n",
    "\n",
    "    \n",
    "\n",
    "# Load the model with a classification head\n",
    "    model_clin = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"DeepChem/ChemBERTa-5M-MTR\",\n",
    "        num_labels=2,\n",
    "        problem_type=\"single_label_classification\",    \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    peft_config = lora_config(config.r, config.lora_alpha, config.dropout)\n",
    "    lora_model = get_peft_model(model_clin, peft_config)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=save_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=config.lr,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_clin\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"wandb\",\n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_mcc_metric\"\n",
    "    )\n",
    "\n",
    "\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = softmax(logits, axis=1)[:, 1]  # Get probabilities for class 1\n",
    "        predictions = np.argmax(logits, axis=1)  # Choose the most likely class\n",
    "        mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "        return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities),  # AUC-ROC requires probabilities\n",
    "            \"Precision\": precision_score(labels, predictions),\n",
    "            \"Recall\": recall_score(labels, predictions),\n",
    "            \"F1-score\": f1_score(labels, predictions)\n",
    "        }\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    trainer= WeightedLossTrainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_data,\n",
    "    eval_dataset=validation_data,\n",
    "    tokenizer=tokenizer_clin,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(save_dir)\n",
    "        \n",
    "    print(f\"Model saved to {save_dir}\")\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: sdvave8e\n",
      "Sweep URL: https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mc7f44lm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.5625314124316165e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mharodharsha21\u001b[0m (\u001b[33mharodharsha21-iit-ropar\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_103920-mc7f44lm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/mc7f44lm' target=\"_blank\">laced-sweep-1</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/mc7f44lm' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/mc7f44lm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1883300/4194757592.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-10 10:39:26,808] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='494' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [494/760 00:28 < 00:15, 17.18 it/s, Epoch 13/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.667858</td>\n",
       "      <td>0.234196</td>\n",
       "      <td>0.643357</td>\n",
       "      <td>0.779882</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.281690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.636900</td>\n",
       "      <td>0.337409</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.857396</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.370370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.653800</td>\n",
       "      <td>0.606263</td>\n",
       "      <td>0.494244</td>\n",
       "      <td>0.853147</td>\n",
       "      <td>0.898817</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.653800</td>\n",
       "      <td>0.573562</td>\n",
       "      <td>0.531150</td>\n",
       "      <td>0.874126</td>\n",
       "      <td>0.931361</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.653800</td>\n",
       "      <td>0.539563</td>\n",
       "      <td>0.544683</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>0.953254</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.564103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.570400</td>\n",
       "      <td>0.506029</td>\n",
       "      <td>0.573975</td>\n",
       "      <td>0.895105</td>\n",
       "      <td>0.963905</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.594595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.570400</td>\n",
       "      <td>0.471814</td>\n",
       "      <td>0.589891</td>\n",
       "      <td>0.902098</td>\n",
       "      <td>0.968639</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.479400</td>\n",
       "      <td>0.437992</td>\n",
       "      <td>0.656103</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.973373</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.479400</td>\n",
       "      <td>0.405729</td>\n",
       "      <td>0.656103</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.975148</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.479400</td>\n",
       "      <td>0.375672</td>\n",
       "      <td>0.656103</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.975740</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>0.350344</td>\n",
       "      <td>0.656103</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.975740</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>0.327895</td>\n",
       "      <td>0.656103</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.976331</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>0.309195</td>\n",
       "      <td>0.656103</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.978107</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./clintox_models_5MTR/mc7f44lm\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▄▅▆▇▇███████</td></tr><tr><td>eval/Accuracy</td><td>▁▄▆▇▇▇███████</td></tr><tr><td>eval/F1-score</td><td>▁▃▅▆▆▇▇██████</td></tr><tr><td>eval/Precision</td><td>▁▂▅▆▆▇▇██████</td></tr><tr><td>eval/Recall</td><td>▁▁▅▅▅▅▅██████</td></tr><tr><td>eval/loss</td><td>█▇▇▆▅▅▄▄▃▂▂▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▃▅▆▆▇▇██████</td></tr><tr><td>eval/runtime</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█████▇█▇████</td></tr><tr><td>eval/steps_per_second</td><td>▁█████▇█▇████</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▃▃▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▃▃▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▁▇█▅</td></tr><tr><td>train/learning_rate</td><td>█▆▃▁</td></tr><tr><td>train/loss</td><td>█▆▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.97811</td></tr><tr><td>eval/Accuracy</td><td>0.91608</td></tr><tr><td>eval/F1-score</td><td>0.66667</td></tr><tr><td>eval/Precision</td><td>0.52174</td></tr><tr><td>eval/Recall</td><td>0.92308</td></tr><tr><td>eval/loss</td><td>0.30919</td></tr><tr><td>eval/mcc_metric</td><td>0.6561</td></tr><tr><td>eval/runtime</td><td>0.1166</td></tr><tr><td>eval/samples_per_second</td><td>1225.896</td></tr><tr><td>eval/steps_per_second</td><td>42.863</td></tr><tr><td>total_flos</td><td>45156283107624.0</td></tr><tr><td>train/epoch</td><td>13</td></tr><tr><td>train/global_step</td><td>494</td></tr><tr><td>train/grad_norm</td><td>1.01748</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.3927</td></tr><tr><td>train_loss</td><td>0.48641</td></tr><tr><td>train_runtime</td><td>29.0522</td></tr><tr><td>train_samples_per_second</td><td>815.774</td></tr><tr><td>train_steps_per_second</td><td>26.16</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">laced-sweep-1</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/mc7f44lm' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/mc7f44lm</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_103920-mc7f44lm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8nooftvw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.3801082904795e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_104012-8nooftvw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8nooftvw' target=\"_blank\">upbeat-sweep-2</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8nooftvw' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8nooftvw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1883300/4194757592.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='646' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [646/760 00:37 < 00:06, 17.10 it/s, Epoch 17/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.667631</td>\n",
       "      <td>0.296590</td>\n",
       "      <td>0.720280</td>\n",
       "      <td>0.810651</td>\n",
       "      <td>0.212766</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.645106</td>\n",
       "      <td>0.407160</td>\n",
       "      <td>0.790210</td>\n",
       "      <td>0.881065</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.423077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.660300</td>\n",
       "      <td>0.623923</td>\n",
       "      <td>0.451925</td>\n",
       "      <td>0.825175</td>\n",
       "      <td>0.917751</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.468085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.660300</td>\n",
       "      <td>0.602924</td>\n",
       "      <td>0.494244</td>\n",
       "      <td>0.853147</td>\n",
       "      <td>0.936686</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.660300</td>\n",
       "      <td>0.582488</td>\n",
       "      <td>0.494244</td>\n",
       "      <td>0.853147</td>\n",
       "      <td>0.946746</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.603100</td>\n",
       "      <td>0.563357</td>\n",
       "      <td>0.505984</td>\n",
       "      <td>0.860140</td>\n",
       "      <td>0.952071</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.523810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.603100</td>\n",
       "      <td>0.544299</td>\n",
       "      <td>0.531150</td>\n",
       "      <td>0.874126</td>\n",
       "      <td>0.955621</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.548500</td>\n",
       "      <td>0.525051</td>\n",
       "      <td>0.531150</td>\n",
       "      <td>0.874126</td>\n",
       "      <td>0.960355</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.548500</td>\n",
       "      <td>0.506505</td>\n",
       "      <td>0.544683</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>0.963905</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.564103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.548500</td>\n",
       "      <td>0.488868</td>\n",
       "      <td>0.544683</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>0.964497</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.564103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.497300</td>\n",
       "      <td>0.473300</td>\n",
       "      <td>0.544683</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>0.966272</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.564103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.497300</td>\n",
       "      <td>0.458765</td>\n",
       "      <td>0.558934</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.968639</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.497300</td>\n",
       "      <td>0.445504</td>\n",
       "      <td>0.558934</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.968047</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.455900</td>\n",
       "      <td>0.434119</td>\n",
       "      <td>0.558934</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.455900</td>\n",
       "      <td>0.423930</td>\n",
       "      <td>0.558934</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.970414</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.410600</td>\n",
       "      <td>0.415738</td>\n",
       "      <td>0.558934</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.971006</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.410600</td>\n",
       "      <td>0.409648</td>\n",
       "      <td>0.558934</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.971006</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./clintox_models_5MTR/8nooftvw\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▄▆▇▇▇▇██████████</td></tr><tr><td>eval/Accuracy</td><td>▁▄▅▇▇▇▇▇█████████</td></tr><tr><td>eval/F1-score</td><td>▁▄▅▆▆▆▇▇█████████</td></tr><tr><td>eval/Precision</td><td>▁▃▄▆▆▆▇▇▇▇▇██████</td></tr><tr><td>eval/Recall</td><td>▁████████████████</td></tr><tr><td>eval/loss</td><td>█▇▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▄▅▆▆▇▇▇█████████</td></tr><tr><td>eval/runtime</td><td>▄█▃▃▃▄▄▃▃▄▃▃▁▃▄▂▁</td></tr><tr><td>eval/samples_per_second</td><td>▅▁▆▆▆▅▅▆▆▅▆▆█▆▅▇█</td></tr><tr><td>eval/steps_per_second</td><td>▅▁▆▆▆▅▅▆▆▅▆▆█▆▅▇█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁█▆▃▆▅</td></tr><tr><td>train/learning_rate</td><td>█▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>█▆▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.97101</td></tr><tr><td>eval/Accuracy</td><td>0.88811</td></tr><tr><td>eval/F1-score</td><td>0.57895</td></tr><tr><td>eval/Precision</td><td>0.44</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.40965</td></tr><tr><td>eval/mcc_metric</td><td>0.55893</td></tr><tr><td>eval/runtime</td><td>0.1159</td></tr><tr><td>eval/samples_per_second</td><td>1234.157</td></tr><tr><td>eval/steps_per_second</td><td>43.152</td></tr><tr><td>total_flos</td><td>58812291289584.0</td></tr><tr><td>train/epoch</td><td>17</td></tr><tr><td>train/global_step</td><td>646</td></tr><tr><td>train/grad_norm</td><td>0.88218</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4106</td></tr><tr><td>train_loss</td><td>0.5211</td></tr><tr><td>train_runtime</td><td>37.7435</td></tr><tr><td>train_samples_per_second</td><td>627.923</td></tr><tr><td>train_steps_per_second</td><td>20.136</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upbeat-sweep-2</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8nooftvw' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8nooftvw</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_104012-8nooftvw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vthdn47v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.3846754312549576e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_104109-vthdn47v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/vthdn47v' target=\"_blank\">divine-sweep-3</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/vthdn47v' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/vthdn47v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1883300/4194757592.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='760' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [760/760 00:44, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.656298</td>\n",
       "      <td>0.292770</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.891124</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.303797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.631917</td>\n",
       "      <td>0.380266</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.940237</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.655800</td>\n",
       "      <td>0.607393</td>\n",
       "      <td>0.519615</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.956213</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.655800</td>\n",
       "      <td>0.581655</td>\n",
       "      <td>0.530595</td>\n",
       "      <td>0.853147</td>\n",
       "      <td>0.962722</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.655800</td>\n",
       "      <td>0.555064</td>\n",
       "      <td>0.553988</td>\n",
       "      <td>0.867133</td>\n",
       "      <td>0.972189</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.588500</td>\n",
       "      <td>0.528209</td>\n",
       "      <td>0.579569</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>0.973964</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.585366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.588500</td>\n",
       "      <td>0.500440</td>\n",
       "      <td>0.579569</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>0.975148</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.585366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.516600</td>\n",
       "      <td>0.471939</td>\n",
       "      <td>0.593306</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.975740</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.516600</td>\n",
       "      <td>0.444017</td>\n",
       "      <td>0.622978</td>\n",
       "      <td>0.902098</td>\n",
       "      <td>0.976331</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.516600</td>\n",
       "      <td>0.417040</td>\n",
       "      <td>0.622978</td>\n",
       "      <td>0.902098</td>\n",
       "      <td>0.977515</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.442200</td>\n",
       "      <td>0.393622</td>\n",
       "      <td>0.622978</td>\n",
       "      <td>0.902098</td>\n",
       "      <td>0.978698</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.442200</td>\n",
       "      <td>0.372225</td>\n",
       "      <td>0.656103</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.979290</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.442200</td>\n",
       "      <td>0.353765</td>\n",
       "      <td>0.656103</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.979290</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.381100</td>\n",
       "      <td>0.338238</td>\n",
       "      <td>0.656103</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.381100</td>\n",
       "      <td>0.324682</td>\n",
       "      <td>0.656103</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.981065</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.325300</td>\n",
       "      <td>0.314216</td>\n",
       "      <td>0.656103</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.981065</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.325300</td>\n",
       "      <td>0.306396</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.981657</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.325300</td>\n",
       "      <td>0.301073</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.981657</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.314500</td>\n",
       "      <td>0.298039</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.981657</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.314500</td>\n",
       "      <td>0.296930</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.981657</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./clintox_models_5MTR/vthdn47v\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▆▇▇▇▇█████████████</td></tr><tr><td>eval/Accuracy</td><td>▁▄▆▆▇▇▇▇████████████</td></tr><tr><td>eval/F1-score</td><td>▁▂▅▅▆▆▆▆▇▇▇█████████</td></tr><tr><td>eval/Precision</td><td>▁▂▅▅▅▆▆▆▇▇▇█████████</td></tr><tr><td>eval/Recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>██▇▇▆▆▅▄▄▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▃▅▅▆▆▆▇▇▇▇█████████</td></tr><tr><td>eval/runtime</td><td>▂▂▄▄▂▃▄▅▆▆▃▄▄▄█▃▁▂▂▁</td></tr><tr><td>eval/samples_per_second</td><td>▆▇▅▅▇▆▄▄▃▃▆▅▅▅▁▆█▇▇█</td></tr><tr><td>eval/steps_per_second</td><td>▆▇▅▅▇▆▄▄▃▃▆▅▅▅▁▆█▇▇█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁█▆▄▇██</td></tr><tr><td>train/learning_rate</td><td>█▇▆▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▇▅▄▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.98166</td></tr><tr><td>eval/Accuracy</td><td>0.92308</td></tr><tr><td>eval/F1-score</td><td>0.68571</td></tr><tr><td>eval/Precision</td><td>0.54545</td></tr><tr><td>eval/Recall</td><td>0.92308</td></tr><tr><td>eval/loss</td><td>0.29693</td></tr><tr><td>eval/mcc_metric</td><td>0.6742</td></tr><tr><td>eval/runtime</td><td>0.1157</td></tr><tr><td>eval/samples_per_second</td><td>1235.812</td></tr><tr><td>eval/steps_per_second</td><td>43.21</td></tr><tr><td>total_flos</td><td>72122343547128.0</td></tr><tr><td>train/epoch</td><td>20</td></tr><tr><td>train/global_step</td><td>760</td></tr><tr><td>train/grad_norm</td><td>1.07427</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3145</td></tr><tr><td>train_loss</td><td>0.44754</td></tr><tr><td>train_runtime</td><td>44.9195</td></tr><tr><td>train_samples_per_second</td><td>527.61</td></tr><tr><td>train_steps_per_second</td><td>16.919</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-sweep-3</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/vthdn47v' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/vthdn47v</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_104109-vthdn47v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wlgpdy7b with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.1180080096838924e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_104207-wlgpdy7b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/wlgpdy7b' target=\"_blank\">wise-sweep-4</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/wlgpdy7b' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/wlgpdy7b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1883300/4194757592.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='646' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [646/760 00:42 < 00:07, 15.09 it/s, Epoch 17/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.661399</td>\n",
       "      <td>0.423972</td>\n",
       "      <td>0.832168</td>\n",
       "      <td>0.899408</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.632249</td>\n",
       "      <td>0.553988</td>\n",
       "      <td>0.867133</td>\n",
       "      <td>0.962722</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.654100</td>\n",
       "      <td>0.599104</td>\n",
       "      <td>0.607752</td>\n",
       "      <td>0.895105</td>\n",
       "      <td>0.974556</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.654100</td>\n",
       "      <td>0.557657</td>\n",
       "      <td>0.622978</td>\n",
       "      <td>0.902098</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.654100</td>\n",
       "      <td>0.507960</td>\n",
       "      <td>0.639064</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.980473</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.560900</td>\n",
       "      <td>0.453885</td>\n",
       "      <td>0.639064</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.560900</td>\n",
       "      <td>0.395651</td>\n",
       "      <td>0.639064</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.422100</td>\n",
       "      <td>0.338696</td>\n",
       "      <td>0.639064</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.980473</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.422100</td>\n",
       "      <td>0.288334</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.981657</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.422100</td>\n",
       "      <td>0.246778</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.982840</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.279700</td>\n",
       "      <td>0.216738</td>\n",
       "      <td>0.693479</td>\n",
       "      <td>0.930070</td>\n",
       "      <td>0.986391</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.279700</td>\n",
       "      <td>0.196113</td>\n",
       "      <td>0.714086</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.986982</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.279700</td>\n",
       "      <td>0.182071</td>\n",
       "      <td>0.714086</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.988757</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.200200</td>\n",
       "      <td>0.173122</td>\n",
       "      <td>0.714086</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.989349</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.200200</td>\n",
       "      <td>0.166478</td>\n",
       "      <td>0.714086</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.989941</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.158100</td>\n",
       "      <td>0.162127</td>\n",
       "      <td>0.714086</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.158100</td>\n",
       "      <td>0.158956</td>\n",
       "      <td>0.714086</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./clintox_models_5MTR/wlgpdy7b\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▆▇▇▇▇▇▇▇▇███████</td></tr><tr><td>eval/Accuracy</td><td>▁▃▅▆▆▆▆▆▇▇███████</td></tr><tr><td>eval/F1-score</td><td>▁▄▅▆▆▆▆▆▇▇▇██████</td></tr><tr><td>eval/Precision</td><td>▁▃▅▅▅▅▅▅▇▇▇██████</td></tr><tr><td>eval/Recall</td><td>▁████████████████</td></tr><tr><td>eval/loss</td><td>██▇▇▆▅▄▄▃▂▂▂▁▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▄▅▆▆▆▆▆▇▇███████</td></tr><tr><td>eval/runtime</td><td>█▄▃▁▄▃▁▂▄▂▄▅▃▅▃▃▃</td></tr><tr><td>eval/samples_per_second</td><td>▁▅▆█▅▆█▇▅▇▅▄▆▄▅▆▆</td></tr><tr><td>eval/steps_per_second</td><td>▁▅▆█▅▆█▇▅▇▅▄▆▄▅▆▆</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▆▇▃▂█</td></tr><tr><td>train/learning_rate</td><td>█▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>█▇▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99112</td></tr><tr><td>eval/Accuracy</td><td>0.93706</td></tr><tr><td>eval/F1-score</td><td>0.72727</td></tr><tr><td>eval/Precision</td><td>0.6</td></tr><tr><td>eval/Recall</td><td>0.92308</td></tr><tr><td>eval/loss</td><td>0.15896</td></tr><tr><td>eval/mcc_metric</td><td>0.71409</td></tr><tr><td>eval/runtime</td><td>0.1225</td></tr><tr><td>eval/samples_per_second</td><td>1167.116</td></tr><tr><td>eval/steps_per_second</td><td>40.808</td></tr><tr><td>total_flos</td><td>94543052721264.0</td></tr><tr><td>train/epoch</td><td>17</td></tr><tr><td>train/global_step</td><td>646</td></tr><tr><td>train/grad_norm</td><td>1.48764</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1581</td></tr><tr><td>train_loss</td><td>0.36372</td></tr><tr><td>train_runtime</td><td>42.7481</td></tr><tr><td>train_samples_per_second</td><td>554.411</td></tr><tr><td>train_steps_per_second</td><td>17.779</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wise-sweep-4</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/wlgpdy7b' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/wlgpdy7b</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_104207-wlgpdy7b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q33we7xu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.4145154504529838e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_104259-q33we7xu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/q33we7xu' target=\"_blank\">ruby-sweep-5</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/q33we7xu' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/q33we7xu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1883300/4194757592.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='684' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [684/760 00:42 < 00:04, 16.23 it/s, Epoch 18/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.646277</td>\n",
       "      <td>0.327912</td>\n",
       "      <td>0.664336</td>\n",
       "      <td>0.928994</td>\n",
       "      <td>0.203390</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.606038</td>\n",
       "      <td>0.530595</td>\n",
       "      <td>0.853147</td>\n",
       "      <td>0.966272</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.641600</td>\n",
       "      <td>0.555754</td>\n",
       "      <td>0.579569</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>0.975740</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.585366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.641600</td>\n",
       "      <td>0.493843</td>\n",
       "      <td>0.593306</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.978698</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.641600</td>\n",
       "      <td>0.424409</td>\n",
       "      <td>0.639064</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.507500</td>\n",
       "      <td>0.357092</td>\n",
       "      <td>0.693479</td>\n",
       "      <td>0.930070</td>\n",
       "      <td>0.981065</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.507500</td>\n",
       "      <td>0.296266</td>\n",
       "      <td>0.693479</td>\n",
       "      <td>0.930070</td>\n",
       "      <td>0.984024</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>0.247358</td>\n",
       "      <td>0.693479</td>\n",
       "      <td>0.930070</td>\n",
       "      <td>0.985799</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>0.211979</td>\n",
       "      <td>0.693479</td>\n",
       "      <td>0.930070</td>\n",
       "      <td>0.988757</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>0.186486</td>\n",
       "      <td>0.736192</td>\n",
       "      <td>0.944056</td>\n",
       "      <td>0.990533</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.172104</td>\n",
       "      <td>0.736192</td>\n",
       "      <td>0.944056</td>\n",
       "      <td>0.990533</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.164658</td>\n",
       "      <td>0.844375</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.990533</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.158891</td>\n",
       "      <td>0.878058</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.990533</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.159300</td>\n",
       "      <td>0.160454</td>\n",
       "      <td>0.878058</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.990533</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.159300</td>\n",
       "      <td>0.163912</td>\n",
       "      <td>0.878058</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.132900</td>\n",
       "      <td>0.166115</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.132900</td>\n",
       "      <td>0.166949</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.132900</td>\n",
       "      <td>0.167989</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./clintox_models_5MTR/q33we7xu\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▆▇▇▇▇▇██████████</td></tr><tr><td>eval/Accuracy</td><td>▁▅▆▆▆▇▇▇▇▇▇███████</td></tr><tr><td>eval/F1-score</td><td>▁▄▄▄▅▆▆▆▆▆▆████▇▇▇</td></tr><tr><td>eval/Precision</td><td>▁▃▃▄▄▅▅▅▅▆▆▇██████</td></tr><tr><td>eval/Recall</td><td>███████████████▁▁▁</td></tr><tr><td>eval/loss</td><td>█▇▇▆▅▄▃▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▄▄▄▅▆▆▆▆▆▆████▇▇▇</td></tr><tr><td>eval/runtime</td><td>▁▄▁▅▄▇▃▅█▆▅▆▅▅▅▁▃▄</td></tr><tr><td>eval/samples_per_second</td><td>█▅█▄▅▂▆▄▁▃▄▃▄▄▄█▆▅</td></tr><tr><td>eval/steps_per_second</td><td>█▅█▄▅▂▆▄▁▃▄▃▄▄▄█▆▅</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▃▄▄▂█</td></tr><tr><td>train/learning_rate</td><td>█▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99112</td></tr><tr><td>eval/Accuracy</td><td>0.97203</td></tr><tr><td>eval/F1-score</td><td>0.84615</td></tr><tr><td>eval/Precision</td><td>0.84615</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.16799</td></tr><tr><td>eval/mcc_metric</td><td>0.83077</td></tr><tr><td>eval/runtime</td><td>0.1171</td></tr><tr><td>eval/samples_per_second</td><td>1220.796</td></tr><tr><td>eval/steps_per_second</td><td>42.685</td></tr><tr><td>total_flos</td><td>64970765572944.0</td></tr><tr><td>train/epoch</td><td>18</td></tr><tr><td>train/global_step</td><td>684</td></tr><tr><td>train/grad_norm</td><td>5.03945</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1329</td></tr><tr><td>train_loss</td><td>0.30885</td></tr><tr><td>train_runtime</td><td>42.0919</td></tr><tr><td>train_samples_per_second</td><td>563.054</td></tr><tr><td>train_steps_per_second</td><td>18.056</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-sweep-5</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/q33we7xu' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/q33we7xu</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_104259-q33we7xu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8mbsk0ip with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.4078246398358415e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_104351-8mbsk0ip</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8mbsk0ip' target=\"_blank\">warm-sweep-6</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8mbsk0ip' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8mbsk0ip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1883300/4194757592.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='722' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [722/760 00:43 < 00:02, 16.73 it/s, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.661186</td>\n",
       "      <td>0.352541</td>\n",
       "      <td>0.776224</td>\n",
       "      <td>0.847929</td>\n",
       "      <td>0.256410</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.625229</td>\n",
       "      <td>0.553988</td>\n",
       "      <td>0.867133</td>\n",
       "      <td>0.933136</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.651600</td>\n",
       "      <td>0.581144</td>\n",
       "      <td>0.579569</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>0.956213</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.585366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.651600</td>\n",
       "      <td>0.522313</td>\n",
       "      <td>0.579569</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>0.966864</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.585366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.651600</td>\n",
       "      <td>0.453382</td>\n",
       "      <td>0.607752</td>\n",
       "      <td>0.895105</td>\n",
       "      <td>0.976331</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.527600</td>\n",
       "      <td>0.381740</td>\n",
       "      <td>0.639064</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.977515</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.527600</td>\n",
       "      <td>0.312300</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.982249</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.693479</td>\n",
       "      <td>0.930070</td>\n",
       "      <td>0.984024</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.213846</td>\n",
       "      <td>0.693479</td>\n",
       "      <td>0.930070</td>\n",
       "      <td>0.986391</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.186410</td>\n",
       "      <td>0.714086</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.986982</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.201400</td>\n",
       "      <td>0.171279</td>\n",
       "      <td>0.736192</td>\n",
       "      <td>0.944056</td>\n",
       "      <td>0.988757</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.201400</td>\n",
       "      <td>0.165177</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.988757</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.201400</td>\n",
       "      <td>0.160189</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.988757</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>0.161821</td>\n",
       "      <td>0.785757</td>\n",
       "      <td>0.958042</td>\n",
       "      <td>0.989349</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.149300</td>\n",
       "      <td>0.164305</td>\n",
       "      <td>0.785757</td>\n",
       "      <td>0.958042</td>\n",
       "      <td>0.989941</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.124700</td>\n",
       "      <td>0.166267</td>\n",
       "      <td>0.736593</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.989941</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.124700</td>\n",
       "      <td>0.166392</td>\n",
       "      <td>0.736593</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.989349</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.124700</td>\n",
       "      <td>0.167403</td>\n",
       "      <td>0.736593</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.989349</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.141800</td>\n",
       "      <td>0.168508</td>\n",
       "      <td>0.736593</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.989349</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.758621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./clintox_models_5MTR/8mbsk0ip\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▆▇▇▇█████████████</td></tr><tr><td>eval/Accuracy</td><td>▁▄▅▅▆▆▇▇▇▇▇████████</td></tr><tr><td>eval/F1-score</td><td>▁▄▄▄▅▅▆▆▆▇▇████▇▇▇▇</td></tr><tr><td>eval/Precision</td><td>▁▃▄▄▄▅▆▆▆▆▇▇▇██████</td></tr><tr><td>eval/Recall</td><td>▁██████████████▅▅▅▅</td></tr><tr><td>eval/loss</td><td>█▇▇▆▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▄▅▅▅▆▆▇▇▇▇████▇▇▇▇</td></tr><tr><td>eval/runtime</td><td>▆▄▂█▅▃▃▅▇▆▄▃▄▃▂▂▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▃▅▇▁▄▅▆▄▂▃▅▆▅▆▇▇███</td></tr><tr><td>eval/steps_per_second</td><td>▃▅▇▁▄▅▆▄▂▃▅▆▅▆▇▇███</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▁▄▅▃▁█▂</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.98935</td></tr><tr><td>eval/Accuracy</td><td>0.95105</td></tr><tr><td>eval/F1-score</td><td>0.75862</td></tr><tr><td>eval/Precision</td><td>0.6875</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.16851</td></tr><tr><td>eval/mcc_metric</td><td>0.73659</td></tr><tr><td>eval/runtime</td><td>0.1157</td></tr><tr><td>eval/samples_per_second</td><td>1235.54</td></tr><tr><td>eval/steps_per_second</td><td>43.201</td></tr><tr><td>total_flos</td><td>73865211563520.0</td></tr><tr><td>train/epoch</td><td>19</td></tr><tr><td>train/global_step</td><td>722</td></tr><tr><td>train/grad_norm</td><td>1.45581</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1418</td></tr><tr><td>train_loss</td><td>0.29987</td></tr><tr><td>train_runtime</td><td>43.1035</td></tr><tr><td>train_samples_per_second</td><td>549.839</td></tr><tr><td>train_steps_per_second</td><td>17.632</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">warm-sweep-6</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8mbsk0ip' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8mbsk0ip</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_104351-8mbsk0ip/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pycdsmqi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.4764811831586508e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_104443-pycdsmqi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/pycdsmqi' target=\"_blank\">revived-sweep-7</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/pycdsmqi' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/pycdsmqi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1883300/4194757592.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='760' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [760/760 00:45, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.650573</td>\n",
       "      <td>0.257447</td>\n",
       "      <td>0.489510</td>\n",
       "      <td>0.957396</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.262626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.609546</td>\n",
       "      <td>0.498779</td>\n",
       "      <td>0.804196</td>\n",
       "      <td>0.972189</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.481481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.645900</td>\n",
       "      <td>0.556805</td>\n",
       "      <td>0.626980</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.974556</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.619048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.645900</td>\n",
       "      <td>0.486980</td>\n",
       "      <td>0.622978</td>\n",
       "      <td>0.902098</td>\n",
       "      <td>0.977515</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.645900</td>\n",
       "      <td>0.405525</td>\n",
       "      <td>0.639064</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.981657</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.496600</td>\n",
       "      <td>0.325156</td>\n",
       "      <td>0.656103</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.980473</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.496600</td>\n",
       "      <td>0.255542</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.982840</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.292500</td>\n",
       "      <td>0.206902</td>\n",
       "      <td>0.693479</td>\n",
       "      <td>0.930070</td>\n",
       "      <td>0.984024</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.292500</td>\n",
       "      <td>0.176737</td>\n",
       "      <td>0.693479</td>\n",
       "      <td>0.930070</td>\n",
       "      <td>0.987574</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.292500</td>\n",
       "      <td>0.159454</td>\n",
       "      <td>0.714086</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.989941</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.173600</td>\n",
       "      <td>0.150176</td>\n",
       "      <td>0.714086</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.173600</td>\n",
       "      <td>0.146856</td>\n",
       "      <td>0.736192</td>\n",
       "      <td>0.944056</td>\n",
       "      <td>0.992308</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.173600</td>\n",
       "      <td>0.143108</td>\n",
       "      <td>0.736192</td>\n",
       "      <td>0.944056</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.140600</td>\n",
       "      <td>0.143275</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.992899</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.140600</td>\n",
       "      <td>0.144607</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.145888</td>\n",
       "      <td>0.785757</td>\n",
       "      <td>0.958042</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.145880</td>\n",
       "      <td>0.785757</td>\n",
       "      <td>0.958042</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.146441</td>\n",
       "      <td>0.785757</td>\n",
       "      <td>0.958042</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>0.147088</td>\n",
       "      <td>0.785757</td>\n",
       "      <td>0.958042</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>0.147311</td>\n",
       "      <td>0.785757</td>\n",
       "      <td>0.958042</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./clintox_models_5MTR/pycdsmqi\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▄▄▅▆▅▆▆▇▇██████████</td></tr><tr><td>eval/Accuracy</td><td>▁▆▇▇▇▇▇█████████████</td></tr><tr><td>eval/F1-score</td><td>▁▄▆▆▆▆▇▇▇▇▇▇▇███████</td></tr><tr><td>eval/Precision</td><td>▁▃▅▅▅▆▆▆▆▇▇▇▇███████</td></tr><tr><td>eval/Recall</td><td>███▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>█▇▇▆▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▄▆▆▆▆▇▇▇▇▇▇▇███████</td></tr><tr><td>eval/runtime</td><td>▂▃▁▂▂▂▂▂▁▂▂▅▂▂▂▁▁▃▁█</td></tr><tr><td>eval/samples_per_second</td><td>▇▆█▇▇▇▇▇█▇▇▄▇▇▇██▆█▁</td></tr><tr><td>eval/steps_per_second</td><td>▇▆█▇▇▇▇▇█▇▇▄▇▇▇██▆█▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▂▃▃▁█▁</td></tr><tr><td>train/learning_rate</td><td>█▇▆▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▃▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99349</td></tr><tr><td>eval/Accuracy</td><td>0.95804</td></tr><tr><td>eval/F1-score</td><td>0.8</td></tr><tr><td>eval/Precision</td><td>0.70588</td></tr><tr><td>eval/Recall</td><td>0.92308</td></tr><tr><td>eval/loss</td><td>0.14731</td></tr><tr><td>eval/mcc_metric</td><td>0.78576</td></tr><tr><td>eval/runtime</td><td>0.1278</td></tr><tr><td>eval/samples_per_second</td><td>1118.781</td></tr><tr><td>eval/steps_per_second</td><td>39.118</td></tr><tr><td>total_flos</td><td>77737211773560.0</td></tr><tr><td>train/epoch</td><td>20</td></tr><tr><td>train/global_step</td><td>760</td></tr><tr><td>train/grad_norm</td><td>1.14039</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1297</td></tr><tr><td>train_loss</td><td>0.27363</td></tr><tr><td>train_runtime</td><td>45.1412</td></tr><tr><td>train_samples_per_second</td><td>525.019</td></tr><tr><td>train_steps_per_second</td><td>16.836</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">revived-sweep-7</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/pycdsmqi' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/pycdsmqi</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_104443-pycdsmqi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mcnkpsf4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.4864696197692065e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_104540-mcnkpsf4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/mcnkpsf4' target=\"_blank\">gallant-sweep-8</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/mcnkpsf4' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/mcnkpsf4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1883300/4194757592.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='722' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [722/760 00:41 < 00:02, 17.15 it/s, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.650465</td>\n",
       "      <td>0.394785</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.921893</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.425532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.608283</td>\n",
       "      <td>0.509065</td>\n",
       "      <td>0.839161</td>\n",
       "      <td>0.959172</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.510638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.640200</td>\n",
       "      <td>0.556142</td>\n",
       "      <td>0.593306</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.966272</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.640200</td>\n",
       "      <td>0.488958</td>\n",
       "      <td>0.639064</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.974556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.640200</td>\n",
       "      <td>0.414685</td>\n",
       "      <td>0.639064</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>0.341837</td>\n",
       "      <td>0.639064</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.981657</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>0.275648</td>\n",
       "      <td>0.639064</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.983432</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.303500</td>\n",
       "      <td>0.225172</td>\n",
       "      <td>0.693479</td>\n",
       "      <td>0.930070</td>\n",
       "      <td>0.986982</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.303500</td>\n",
       "      <td>0.191079</td>\n",
       "      <td>0.714086</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.990533</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.303500</td>\n",
       "      <td>0.169976</td>\n",
       "      <td>0.736192</td>\n",
       "      <td>0.944056</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.183600</td>\n",
       "      <td>0.160512</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.183600</td>\n",
       "      <td>0.160077</td>\n",
       "      <td>0.844375</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.183600</td>\n",
       "      <td>0.157574</td>\n",
       "      <td>0.796205</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.990533</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.814815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>0.162951</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>0.170198</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.174153</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.175457</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.177501</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.178936</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./clintox_models_5MTR/mcnkpsf4\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▅▆▇▇▇████████████</td></tr><tr><td>eval/Accuracy</td><td>▁▂▄▅▅▅▅▆▆▇▇█▇██████</td></tr><tr><td>eval/F1-score</td><td>▁▂▄▄▄▄▄▅▆▆▆█▇██████</td></tr><tr><td>eval/Precision</td><td>▁▂▃▃▃▃▃▄▄▅▅▇▇██████</td></tr><tr><td>eval/Recall</td><td>▁███████████▅▅▅▅▅▅▅</td></tr><tr><td>eval/loss</td><td>█▇▇▆▅▄▃▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▃▄▅▅▅▅▅▆▆▆█▇██████</td></tr><tr><td>eval/runtime</td><td>▃▁▄▄▅▄▄▄▄▄▆▇█▄▆▂▃▄▄</td></tr><tr><td>eval/samples_per_second</td><td>▅█▅▅▄▅▅▅▅▅▃▂▁▄▃▇▆▅▅</td></tr><tr><td>eval/steps_per_second</td><td>▅█▅▅▄▅▅▅▅▅▃▂▁▄▃▇▆▅▅</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▁▃▄▄▁█▃</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▃▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99172</td></tr><tr><td>eval/Accuracy</td><td>0.97902</td></tr><tr><td>eval/F1-score</td><td>0.88</td></tr><tr><td>eval/Precision</td><td>0.91667</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.17894</td></tr><tr><td>eval/mcc_metric</td><td>0.86936</td></tr><tr><td>eval/runtime</td><td>0.1172</td></tr><tr><td>eval/samples_per_second</td><td>1219.984</td></tr><tr><td>eval/steps_per_second</td><td>42.657</td></tr><tr><td>total_flos</td><td>68530013400576.0</td></tr><tr><td>train/epoch</td><td>19</td></tr><tr><td>train/global_step</td><td>722</td></tr><tr><td>train/grad_norm</td><td>2.51947</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1458</td></tr><tr><td>train_loss</td><td>0.28536</td></tr><tr><td>train_runtime</td><td>42.0437</td></tr><tr><td>train_samples_per_second</td><td>563.7</td></tr><tr><td>train_steps_per_second</td><td>18.076</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gallant-sweep-8</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/mcnkpsf4' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/mcnkpsf4</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_104540-mcnkpsf4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0jhf6w1q with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.3869899364693637e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_104632-0jhf6w1q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/0jhf6w1q' target=\"_blank\">fallen-sweep-9</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/0jhf6w1q' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/0jhf6w1q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1883300/4194757592.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='760' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [760/760 00:44, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.653733</td>\n",
       "      <td>0.257447</td>\n",
       "      <td>0.489510</td>\n",
       "      <td>0.959172</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.262626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.616599</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.977515</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.650400</td>\n",
       "      <td>0.571946</td>\n",
       "      <td>0.626980</td>\n",
       "      <td>0.888112</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.619048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.650400</td>\n",
       "      <td>0.513417</td>\n",
       "      <td>0.687023</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.981065</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.650400</td>\n",
       "      <td>0.445117</td>\n",
       "      <td>0.687023</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.983432</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.521400</td>\n",
       "      <td>0.373625</td>\n",
       "      <td>0.704154</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.984615</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.702703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.521400</td>\n",
       "      <td>0.303931</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.985207</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.342900</td>\n",
       "      <td>0.245972</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.986982</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.342900</td>\n",
       "      <td>0.203196</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.989941</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.342900</td>\n",
       "      <td>0.173439</td>\n",
       "      <td>0.714086</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.154028</td>\n",
       "      <td>0.714086</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.143404</td>\n",
       "      <td>0.714086</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.993491</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.136003</td>\n",
       "      <td>0.714086</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.134453</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.133778</td>\n",
       "      <td>0.785757</td>\n",
       "      <td>0.958042</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.133633</td>\n",
       "      <td>0.785757</td>\n",
       "      <td>0.958042</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.133508</td>\n",
       "      <td>0.844375</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.134025</td>\n",
       "      <td>0.844375</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.994083</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.134658</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.134898</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.923077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./clintox_models_5MTR/0jhf6w1q\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▅▅▆▆▆▆▇▇██████████</td></tr><tr><td>eval/Accuracy</td><td>▁▅▇▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>eval/F1-score</td><td>▁▃▅▅▅▆▅▅▅▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>eval/Precision</td><td>▁▂▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇██</td></tr><tr><td>eval/Recall</td><td>██████▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>██▇▆▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▃▅▆▆▆▅▅▅▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>eval/runtime</td><td>▁▃▆█▂▁▄▁▆▃▂▆▆▂▆▁▂▁▅▄</td></tr><tr><td>eval/samples_per_second</td><td>█▆▃▁▇█▅█▃▆▇▃▃█▃█▇█▄▅</td></tr><tr><td>eval/steps_per_second</td><td>█▆▃▁▇█▅█▃▆▇▃▃█▃█▇█▄▅</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▃▄▃▁█▂</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99467</td></tr><tr><td>eval/Accuracy</td><td>0.98601</td></tr><tr><td>eval/F1-score</td><td>0.92308</td></tr><tr><td>eval/Precision</td><td>0.92308</td></tr><tr><td>eval/Recall</td><td>0.92308</td></tr><tr><td>eval/loss</td><td>0.1349</td></tr><tr><td>eval/mcc_metric</td><td>0.91538</td></tr><tr><td>eval/runtime</td><td>0.1181</td></tr><tr><td>eval/samples_per_second</td><td>1210.702</td></tr><tr><td>eval/steps_per_second</td><td>42.332</td></tr><tr><td>total_flos</td><td>72122343547128.0</td></tr><tr><td>train/epoch</td><td>20</td></tr><tr><td>train/global_step</td><td>760</td></tr><tr><td>train/grad_norm</td><td>1.9843</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1465</td></tr><tr><td>train_loss</td><td>0.29501</td></tr><tr><td>train_runtime</td><td>44.6876</td></tr><tr><td>train_samples_per_second</td><td>530.349</td></tr><tr><td>train_steps_per_second</td><td>17.007</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fallen-sweep-9</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/0jhf6w1q' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/0jhf6w1q</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_104632-0jhf6w1q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8t48fjg0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.4105904731579142e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Clintox Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/wandb/run-20250410_104729-8t48fjg0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8t48fjg0' target=\"_blank\">silvery-sweep-10</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/sdvave8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8t48fjg0' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8t48fjg0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1883300/4194757592.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer= WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='760' max='760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [760/760 00:45, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.652594</td>\n",
       "      <td>0.404145</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.916568</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.613191</td>\n",
       "      <td>0.519615</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.957396</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.521739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.565282</td>\n",
       "      <td>0.579569</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>0.966864</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.585366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.503981</td>\n",
       "      <td>0.639064</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.971006</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.435484</td>\n",
       "      <td>0.639064</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.366995</td>\n",
       "      <td>0.639064</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.979290</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.302012</td>\n",
       "      <td>0.639064</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.982249</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.648649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.329200</td>\n",
       "      <td>0.248797</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.984024</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.329200</td>\n",
       "      <td>0.209436</td>\n",
       "      <td>0.714086</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.989349</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.329200</td>\n",
       "      <td>0.182966</td>\n",
       "      <td>0.714086</td>\n",
       "      <td>0.937063</td>\n",
       "      <td>0.990533</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.167668</td>\n",
       "      <td>0.736192</td>\n",
       "      <td>0.944056</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.161608</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.951049</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.156677</td>\n",
       "      <td>0.785757</td>\n",
       "      <td>0.958042</td>\n",
       "      <td>0.991716</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.158732</td>\n",
       "      <td>0.764989</td>\n",
       "      <td>0.958042</td>\n",
       "      <td>0.990533</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.162937</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.990533</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.165680</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.166389</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.167680</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.168750</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.169218</td>\n",
       "      <td>0.869361</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.991124</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./clintox_models_5MTR/8t48fjg0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▆▆▇▇▇▇████████████</td></tr><tr><td>eval/Accuracy</td><td>▁▂▄▅▅▅▅▆▆▆▆▇▇▇██████</td></tr><tr><td>eval/F1-score</td><td>▁▂▃▄▄▄▄▅▆▆▆▆▇▇██████</td></tr><tr><td>eval/Precision</td><td>▁▂▂▃▃▃▃▄▄▄▅▅▆▆██████</td></tr><tr><td>eval/Recall</td><td>▁████████████▅▅▅▅▅▅▅</td></tr><tr><td>eval/loss</td><td>█▇▇▆▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▃▄▅▅▅▅▅▆▆▆▆▇▆██████</td></tr><tr><td>eval/runtime</td><td>▁▄▄▇▄▃▄▅▇▃█▃▆▇▇▄▂▅▄▂</td></tr><tr><td>eval/samples_per_second</td><td>█▅▄▂▅▆▅▄▂▆▁▆▃▂▂▄▆▄▄▇</td></tr><tr><td>eval/steps_per_second</td><td>█▅▄▂▅▆▅▄▂▆▁▆▃▂▂▄▆▄▄▇</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▃▄▄▁█▃</td></tr><tr><td>train/learning_rate</td><td>█▇▆▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.99112</td></tr><tr><td>eval/Accuracy</td><td>0.97902</td></tr><tr><td>eval/F1-score</td><td>0.88</td></tr><tr><td>eval/Precision</td><td>0.91667</td></tr><tr><td>eval/Recall</td><td>0.84615</td></tr><tr><td>eval/loss</td><td>0.16922</td></tr><tr><td>eval/mcc_metric</td><td>0.86936</td></tr><tr><td>eval/runtime</td><td>0.1167</td></tr><tr><td>eval/samples_per_second</td><td>1225.155</td></tr><tr><td>eval/steps_per_second</td><td>42.838</td></tr><tr><td>total_flos</td><td>72122343547128.0</td></tr><tr><td>train/epoch</td><td>20</td></tr><tr><td>train/global_step</td><td>760</td></tr><tr><td>train/grad_norm</td><td>2.63126</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1468</td></tr><tr><td>train_loss</td><td>0.28801</td></tr><tr><td>train_runtime</td><td>45.3928</td></tr><tr><td>train_samples_per_second</td><td>522.109</td></tr><tr><td>train_steps_per_second</td><td>16.743</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silvery-sweep-10</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8t48fjg0' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8t48fjg0</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250410_104729-8t48fjg0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_runtime': 50.616286505, '_step': 27, '_timestamp': 1744274900.3053336, '_wandb': {'runtime': 51}, 'eval/AUC-ROC': 0.9911242603550297, 'eval/Accuracy': 0.9790209790209792, 'eval/F1-score': 0.88, 'eval/Precision': 0.9166666666666666, 'eval/Recall': 0.8461538461538461, 'eval/loss': 0.1692180186510086, 'eval/mcc_metric': 0.8693611470909237, 'eval/runtime': 0.1167, 'eval/samples_per_second': 1225.155, 'eval/steps_per_second': 42.838, 'total_flos': 72122343547128, 'train/epoch': 20, 'train/global_step': 760, 'train/grad_norm': 2.6312620639801025, 'train/learning_rate': 1.113624057756248e-06, 'train/loss': 0.1468, 'train_loss': 0.2880080223083496, 'train_runtime': 45.3928, 'train_samples_per_second': 522.109, 'train_steps_per_second': 16.743}\n",
      "{'r': 8, 'lr': 1.3801082904795e-05, 'bf16': False, 'fp16': False, 'fsdp': [], 'seed': 42, 'tf32': None, 'debug': [], 'optim': 'adamw_torch', 'top_k': 50, 'top_p': 1, 'is_gpu': True, 'prefix': None, 'do_eval': True, 'dropout': 0.1, 'no_cuda': False, 'tp_size': 0, 'use_cpu': False, 'do_train': False, 'id2label': {'0': 'LABEL_0', '1': 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'norm_std': [2.9210526350021033, 1.5294133532822063, 2.9209947673330334, 0.21956154740898992, 0.22097666681598951, 160.4856642380458, 151.38170855657367, 160.3304390667665, 60.484857692625106, 0.181038611279414, 0, 0, 0, 0, 0.24851193112366385, 0.317494124851492, 0.37175815103599535, 0.6098706561111424, 539.8195290502504, 8.140940922894863, 6.600767667198695, 6.700942921964325, 5.536318526756788, 4.020569431789569, 4.316039675035455, 3.229701298304296, 4.058753110098356, 2.399274478688092, 4.590084765547685, 1.8657465201411236, 8.197075845395899, 1.3989800795766576, 8.727770321711972, 4.719034225006412, 3.6844834579923407, 66.65125255607474, 11.022808176926915, 9.88512023443511, 5.895101555004671, 6.0315631910071374, 4.465786134186721, 8.73293454096314, 7.292192943139112, 5.798809757257198, 5.458840154330179, 5.34562222799046, 28.624753237838465, 22.7685485030176, 13.735506972569182, 12.75558914023291, 12.647297666063738, 16.73803715869515, 1.3236865505015507, 8.012917117258175, 6.328266302270954, 30.80439768300023, 14.510669158473307, 33.76748799216324, 0, 8.851153866015428, 8.222102882220607, 7.329351085680612, 4.87773057457412, 10.796349487508555, 24.55359833254403, 10.33295824604808, 8.986884190324291, 26.77991276665104, 29.521288543995215, 4.077418430037268, 11.23487898363004, 0, 50.277243284807206, 19.12173183245714, 9.819697177666312, 1.4201437981599128, 12.511435257208836, 14.212538029397628, 16.973978925056553, 19.21649041911615, 15.092240504961104, 19.889237093009676, 25.80872442073538, 9.254317550453823, 19.013243564373347, 3.6841568734614953, 17.690679185577395, 10.27595457263499, 3.3283202642652645, 2.8773795244438474, 9.228734822190496, 5.106296483962912, 4.008127533955226, 2.3345092198667503, 0.23958883840178577, 11.48532061063049, 2.0042680181777808, 3.411142707197923, 0.7103265443180337, 0.8009597262862117, 1.0630493791282618, 1.249503799091361, 0.8592211073826755, 1.4909738617970665, 2.8049912821495706, 1.5692082041123123, 3.718886071238216, 4.918753910447648, 0.6213838320183964, 0.6971589290933399, 0.9385507839118636, 1.7370945619837506, 2.7759468746763334, 43.91556441471313, 0.2929625321198007, 0.6742399816263887, 0.6447563579731193, 0.26136083143708466, 0.1703202147866646, 1.3696411924562566, 0.3394696140137124, 0.26977939457438505, 0.3350074869447194, 0.3408584597974497, 1.2690580420372088, 1.2684116362885036, 0.1297126917051003, 0.06304965563156611, 0.17914965229828922, 1.485673805113914, 1.1656052934139842, 0.5018632205797633, 0.15576643470973517, 0.2883562378800223, 0.3774901929558512, 0.3394696140137124, 0.07983606764988928, 0.1030741645577756, 0.11692041889415362, 1.0010868912132271, 0.7705779932112281, 1.157481598590082, 0.13507534533122212, 0.8359812306885952, 0.7600865243553028, 0.04757124327808961, 0.07183232513905516, 0.03513570421263404, 1.239225396368063, 0.015097985029438592, 1.3364349277900949, 0.013378265133341392, 0.032663541616103894, 0.060970137226002974, 0.44400840883756576, 1.159532265122051, 0.198246590935912, 0.1491817288215558, 1.28126795861232, 0.143114919141507, 0.11579880303510388, 0.25012811724209466, 0.1830406121462275, 0.03504726333553974, 0.015295758691880374, 0.3034514997274073, 0.2749689545601939, 0.04859983910409953, 0.09878498419533764, 0.5707110234042025, 0.17028898672063034, 0.24456026600763192, 0.21322057789532145, 0.1917343827305721, 0.13591391704896466, 0.03519702423260403, 0.1108018278371122, 0.0680510883818226, 0.5264724473438641, 0.2602735481879015, 0.25847912916802446, 0.10886360159063148, 0.1002693464072736, 0.35113436163289397, 0.2260341350934195, 0.16874580630684471, 0, 0.4146998571400424, 0.5347143492505464, 0.3137422508894841, 0.27962501103110715, 0.1547563582555832, 0.08130444916739461, 0.08949068223889126, 0.225304925348536, 0.014421012861987593, 0.2736413019822887, 2.253629375384596, 0.22817317920167496], 'run_name': './clintox_models_5MTR/8nooftvw', 'use_ipex': False, 'adafactor': False, 'data_seed': None, 'deepspeed': None, 'do_sample': False, 'hub_token': '<HUB_TOKEN>', 'log_level': 'passive', 'max_steps': -1, 'norm_mean': [11.199569164274653, -0.9728601944583676, 11.199595401578872, 0.1914454376660732, 0.608589373135307, 365.064017672, 342.24912812000014, 364.6033136038417, 134.06547, 0.004249, 0, 0, 0, 0, 1.1861084842221647, 1.890967178564785, 2.519587985439997, 2.0112818114267816, 795.5621221754437, 18.14439203724506, 14.536240385432391, 15.215140271072489, 12.068994414289726, 8.453657900068215, 9.114162139055054, 6.434168605708085, 7.215103879809845, 4.436200487997215, 5.109730699855831, 3.055231525907226, 3.6252747118486264, -2.202564923376624, 18.195385007867852, 7.970699358994477, 4.5379164631837545, 150.95250337667272, 13.184208966483704, 8.814008658052902, 3.81918390789873, 3.4969386790830774, 2.9222201316693712, 2.644444123964607, 6.408740449956927, 4.95314480536345, 2.6263770771853108, 2.4113616526384853, 26.24052195128434, 37.102909834641714, 19.89943953042712, 16.353848799228413, 15.638332143998122, 21.706094849865753, 0.28727529762970366, 8.054432014422119, 3.2648099385428853, 32.629006626588726, 16.26551059790217, 47.70605007162041, 0, 5.325837027308287, 9.698460925314944, 5.573601891254677, 2.581492771453006, 7.312496194388467, 33.07539073817076, 10.718462271839512, 6.99277406210818, 31.684923475431933, 36.92162447084414, 1.2074202610211655, 5.110701506051421, 0, 71.04050338999998, 9.57750975344203, 10.066085526965992, 0.07691213090851719, 13.38923196114951, 16.862422387837878, 21.382953923695233, 15.651918121909311, 14.440634953378058, 19.13130604146014, 22.114944705243296, 8.183429061888226, 13.699768012021506, 2.1212691930096144, 17.474216494453906, 7.846769617492272, 2.6683841482907034, 0.11868201225906092, 9.064881467380092, 2.659801877718109, 4.055917032498944, 0.259848432909807, 0.413963629624058, 25.186704, 1.79722, 5.353545, 0.272499, 0.562898, 0.835397, 1.236854, 0.729917, 1.966771, 4.216321, 1.414081, 6.486208, 5.688314, 0.205632, 0.409204, 0.614836, 2.802168, 2.7549044689500004, 97.31541557350002, 0.069051, 0.151924, 0.130758, 0.06279, 0.027038, 0.999062, 0.096951, 0.042862, 0.096089, 0.100163, 1.033857, 1.034286, 0.016206, 0.00357, 0.016776, 1.488795, 0.915699, 0.232236, 0.012241, 0.074885, 0.131561, 0.096951, 0.004026, 0.009835, 0.011646, 0.250196, 0.131237, 0.768633, 0.015927, 0.539599, 0.451885, 0.001726, 0.003335, 0.001218, 1.236474, 0.000226, 0.555529, 0.000149, 0.001046, 0.002578, 0.126995, 0.732216, 0.037978, 0.019179, 0.720141, 0.018951, 0.013025, 0.059523, 0.027553, 0.000831, 0.0002, 0.073914, 0.061694, 0.002249, 0.007716, 0.236426, 0.0287, 0.05231, 0.041425, 0.033421, 0.017275, 0.001082, 0.011915, 0.004249, 0.196769, 0.039316, 0.038686, 0.00409, 0.003615, 0.116124, 0.051192, 0.025177, 0, 0.161908, 0.315775, 0.087229, 0.079586, 0.023227, 0.005966, 0.007901, 0.050376, 0.000186, 0.065723, 0.380193, 0.051566], 'num_beams': 1, 'ray_scope': 'last', 'report_to': ['wandb'], 'typical_p': 1, 'use_cache': True, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'do_predict': False, 'eval_delay': 0, 'eval_steps': None, 'hidden_act': 'gelu', 'is_decoder': False, 'local_rank': 0, 'lora_alpha': 16, 'max_length': 20, 'min_length': 0, 'model_type': 'roberta', 'optim_args': None, 'output_dir': './clintox_models_5MTR/8nooftvw', 'past_index': -1, 'save_steps': 500, 'vocab_size': 600, 'ddp_backend': None, 'ddp_timeout': 1800, 'fsdp_config': {'xla': False, 'xla_fsdp_v2': False, 'min_num_params': 0, 'xla_fsdp_grad_ckpt': False}, 'hidden_size': 384, 'label_names': None, 'logging_dir': './logs_clin', 'peft_config': {'default': {'r': 8, 'bias': 'none', 'revision': None, 'use_dora': False, 'lora_bias': False, 'peft_type': 'LORA', 'task_type': 'SEQ_CLS', 'eva_config': None, 'lora_alpha': 16, 'use_rslora': False, 'auto_mapping': None, 'lora_dropout': 0.1, 'megatron_core': 'megatron.core', 'fan_in_fan_out': False, 'inference_mode': False, 'layers_pattern': None, 'runtime_config': {'ephemeral_gpu_offload': False}, 'target_modules': ['dense', 'out_proj', 'key', 'query', 'value'], 'exclude_modules': None, 'megatron_config': None, 'modules_to_save': ['classifier', 'score'], 'init_lora_weights': True, 'layer_replication': None, 'layers_to_transform': None, 'base_model_name_or_path': 'DeepChem/ChemBERTa-5M-MTR'}}, 'push_to_hub': False, 'return_dict': True, 'temperature': 1, 'torch_dtype': 'float32', 'torchdynamo': None, 'torchscript': False, 'adam_epsilon': 1e-08, 'bos_token_id': 0, 'disable_tqdm': False, 'eos_token_id': 2, 'fp16_backend': 'auto', 'hub_model_id': None, 'hub_strategy': 'every_save', 'pad_token_id': 1, 'problem_type': 'single_label_classification', 'sep_token_id': None, 'use_bfloat16': False, 'warmup_ratio': 0, 'warmup_steps': 0, 'weight_decay': 0.01, '_name_or_path': 'DeepChem/ChemBERTa-5M-MTR', 'architectures': ['RobertaForRegression'], 'bad_words_ids': None, 'eval_on_start': False, 'eval_strategy': 'epoch', 'jit_mode_eval': False, 'learning_rate': 1.3801082904795e-05, 'logging_steps': 100, 'max_grad_norm': 1, 'mp_parameters': '', 'output_scores': False, 'save_strategy': 'epoch', 'split_batches': None, 'torch_compile': False, 'tpu_num_cores': None, 'bf16_full_eval': False, 'early_stopping': False, 'fp16_full_eval': False, 'fp16_opt_level': 'O1', 'layer_norm_eps': 1e-12, 'length_penalty': 1, 'tf_legacy_loss': False, 'use_mps_device': False, 'finetuning_task': None, 'group_by_length': False, 'hub_always_push': False, 'num_beam_groups': 1, 'save_only_model': False, 'suppress_tokens': None, 'tokenizer_class': None, 'type_vocab_size': 1, 'dispatch_batches': None, 'full_determinism': False, 'hub_private_repo': None, 'ignore_data_skip': False, 'log_on_each_node': True, 'logging_strategy': 'steps', 'num_train_epochs': 20, 'save_safetensors': True, 'save_total_limit': 5, 'use_liger_kernel': False, 'ddp_bucket_cap_mb': None, 'diversity_penalty': 0, 'greater_is_better': True, 'initializer_range': 0.02, 'intermediate_size': 464, 'log_level_replica': 'warning', 'lr_scheduler_type': 'linear', 'num_hidden_layers': 3, 'output_attentions': False, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'save_on_each_node': False, 'tpu_metrics_debug': False, 'accelerator_config': {'even_batches': True, 'non_blocking': False, 'split_batches': False, 'dispatch_batches': None, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None}, 'batch_eval_metrics': False, 'classifier_dropout': None, 'is_encoder_decoder': False, 'length_column_name': 'length', 'logging_first_step': False, 'repetition_penalty': 1, 'torch_compile_mode': None, 'add_cross_attention': False, 'evaluation_strategy': 'epoch', 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'fsdp_min_num_params': 0, 'hidden_dropout_prob': 0.144, 'include_for_metrics': [], 'neftune_noise_alpha': None, 'num_attention_heads': 12, 'skip_memory_metrics': True, 'tie_encoder_decoder': False, 'tie_word_embeddings': True, 'auto_find_batch_size': False, 'dataloader_drop_last': False, 'model/num_parameters': 3709716, 'no_repeat_ngram_size': 0, 'num_return_sequences': 1, 'optim_target_modules': None, 'output_hidden_states': False, 'overwrite_output_dir': False, 'prediction_loss_only': False, 'push_to_hub_model_id': None, 'task_specific_params': None, 'transformers_version': '4.50.0', 'begin_suppress_tokens': None, 'dataloader_pin_memory': True, 'ddp_broadcast_buffers': None, 'metric_for_best_model': 'eval_mcc_metric', 'remove_invalid_values': False, 'remove_unused_columns': True, 'torch_compile_backend': None, 'dataloader_num_workers': 0, 'decoder_start_token_id': None, 'eval_do_concat_batches': True, 'eval_use_gather_object': False, 'gradient_checkpointing': False, 'half_precision_backend': 'auto', 'label_smoothing_factor': 0, 'load_best_model_at_end': True, 'logging_nan_inf_filter': True, 'resume_from_checkpoint': None, 'chunk_size_feed_forward': 0, 'eval_accumulation_steps': None, 'max_position_embeddings': 515, 'per_gpu_eval_batch_size': None, 'position_embedding_type': 'absolute', 'return_dict_in_generate': False, 'torch_empty_cache_steps': None, 'per_gpu_train_batch_size': None, 'push_to_hub_organization': None, 'include_tokens_per_second': False, 'dataloader_prefetch_factor': None, 'ddp_find_unused_parameters': None, 'include_inputs_for_metrics': False, 'per_device_eval_batch_size': 32, 'use_legacy_prediction_loop': False, 'cross_attention_hidden_size': None, 'gradient_accumulation_steps': 1, 'per_device_train_batch_size': 32, '_attn_implementation_autoset': True, 'attention_probs_dropout_prob': 0.109, 'encoder_no_repeat_ngram_size': 0, 'average_tokens_across_devices': False, 'dataloader_persistent_workers': False, 'gradient_checkpointing_kwargs': None, 'include_num_input_tokens_seen': False, 'exponential_decay_length_penalty': None, 'fsdp_transformer_layer_cls_to_wrap': None, 'restore_callback_states_from_checkpoint': False}\n"
     ]
    }
   ],
   "source": [
    " # Define the sweep configuration\n",
    "def main():\n",
    "\n",
    "    sweep_config = {\n",
    "    \"name\": \"Clintox Hyperparameter Tuning\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"goal\": \"maximize\",\n",
    "        \"name\": \"eval/mcc_metric\"\n",
    "        },\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\n",
    "        \"distribution\": \"uniform\",\n",
    "                \"min\": 1e-5,\n",
    "                \"max\": 2e-5\n",
    "        },\n",
    "        \"r\": {\n",
    "            \"values\": [8,16,32,64,128]\n",
    "        },\n",
    "        \"lora_alpha\": {\n",
    "            \"values\": [16,32,64, 128]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"values\": [0.1, 0.2]\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"huggingface\")\n",
    "    wandb.agent(sweep_id, function=run_training, count=10)\n",
    "\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(f\"huggingface/{sweep_id}\")\n",
    "    print(sweep.runs[0].summary_metrics)\n",
    "\n",
    "    runs_with_eval_loss = [run for run in sweep.runs if 'eval/mcc_metric' in run.summary_metrics]\n",
    "\n",
    "    if runs_with_eval_loss:\n",
    "        best_run = sorted(runs_with_eval_loss, key=lambda run: run.summary_metrics['eval/mcc_metric'],reverse=False)[0]\n",
    "    else:\n",
    "        raise ValueError(\"No runs found with 'eval/mcc_metric' metric.\")\n",
    "\n",
    "    best_hyperparameters = best_run.config\n",
    "    print(best_hyperparameters)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model with a classification head\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-5M-MTR\",\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True,\n",
    "    \n",
    ")\n",
    "\n",
    "tokenizer_clin = AutoTokenizer.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-5M-MTR\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "adapter_model = PeftModel.from_pretrained(base_model, \"/home/raghvendra2/Molformer_Finetuning/clintox_models_77mtr_re/ry12c9lj/checkpoint-600\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data_clin=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/clintox_test.csv')\n",
    "\n",
    "smiles_test_clin = test_data_clin['smiles'].tolist()\n",
    "\n",
    "test_tokenized_clin =tokenizer_clin(smiles_test_clin)\n",
    "\n",
    "test_dataset_clin = Dataset.from_dict(test_tokenized_clin)\n",
    "\n",
    "test_labels_clin = test_data_clin['CT_TOX'].tolist() # Assuming tasks start from column 1\n",
    "\n",
    "\n",
    "test_dataset_clin = test_dataset_clin.add_column(\"labels\", test_labels_clin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = softmax(logits, axis=1)[:, 1]  # Get probabilities for class 1\n",
    "        predictions = np.argmax(logits, axis=1)  # Choose the most likely class\n",
    "        mcc = matthews_corrcoef(labels, predictions)\n",
    "        \n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities),  # AUC-ROC requires probabilities\n",
    "            \"Precision\": precision_score(labels, predictions),\n",
    "            \"Recall\": recall_score(labels, predictions),\n",
    "            \"F1-score\": f1_score(labels, predictions)\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_results_clintox2\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    report_to=\"none\",  # Disable logging to W&B for test\n",
    "    seed=42,  # Ensures reproducibility\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For model tyfuunip 77MLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob:  [0.4401219  0.4399929  0.44660157 0.45053703 0.44525385 0.4580119\n",
      " 0.46075583 0.46209157 0.5033227  0.45664516 0.4532183  0.45829377\n",
      " 0.4509178  0.4523891  0.45217815 0.51477736 0.45290527 0.45750266\n",
      " 0.45052534 0.45100486 0.45547527 0.45262635 0.46124303 0.4483394\n",
      " 0.4528426  0.4561485  0.4517518  0.5295794  0.46028766 0.4568578\n",
      " 0.46080524 0.45632154 0.42576033 0.43297708 0.43159267 0.44898543\n",
      " 0.4636876  0.48366705 0.4761501  0.4557099  0.47625202 0.48437208\n",
      " 0.4790405  0.4397568  0.44090763 0.4459546  0.44566756 0.44490114\n",
      " 0.44374675 0.45518142 0.45188746 0.46135843 0.44551456 0.46952462\n",
      " 0.4488264  0.4528965  0.43407086 0.44923997 0.45246115 0.44849825\n",
      " 0.4377801  0.44890004 0.4528525  0.45194796 0.45177567 0.45127347\n",
      " 0.44365412 0.43731663 0.43618667 0.4472212  0.4513229  0.45774305\n",
      " 0.5249134  0.46220636 0.45378727 0.4513828  0.45773304 0.4541731\n",
      " 0.45600662 0.4529879  0.45823517 0.45732948 0.45369816 0.4455454\n",
      " 0.47273836 0.43125102 0.45793718 0.46125695 0.45517468 0.45505613\n",
      " 0.51701516 0.4388055  0.42799357 0.5258266  0.4493172  0.51815224\n",
      " 0.5104785  0.4367464  0.47835085 0.443024   0.4369357  0.47722545\n",
      " 0.52699864 0.4386961  0.43363076 0.43925685 0.45400333 0.43066153\n",
      " 0.4539179  0.45655847 0.44100696 0.47188658 0.45407364 0.45411402\n",
      " 0.43694556 0.46289724 0.5235957  0.43424913 0.44978464 0.45924214\n",
      " 0.44617444 0.49354917 0.43273857 0.4273133  0.43078527 0.48363838\n",
      " 0.44055483 0.43429103 0.45293206 0.45366266 0.4615435  0.45479867\n",
      " 0.5302409  0.4574436  0.44068784 0.43890974 0.44558832 0.4424323\n",
      " 0.4434255  0.44963998 0.44946343 0.44692686 0.4395926 ]\n",
      "Predictions: [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Test Results for model2: {'eval_mcc_metric': 0.8215838362577491, 'eval_loss': 0.6328266859054565, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.972027972027972, 'eval_AUC-ROC': 0.991715976331361, 'eval_Precision': 0.9090909090909091, 'eval_Recall': 0.7692307692307693, 'eval_F1-score': 0.8333333333333334, 'eval_runtime': 0.1425, 'eval_samples_per_second': 1003.771, 'eval_steps_per_second': 63.174}\n"
     ]
    }
   ],
   "source": [
    "test_results_clin = trainer.evaluate()\n",
    "\n",
    "print(\"Test Results for model :\", test_results_clin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For model 77M MLM 1m1rdktq , result similar to Molformer best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob:  [2.2636575e-03 2.1273368e-03 9.4913028e-04 1.7895816e-03 1.2322032e-03\n",
      " 2.2662901e-03 2.3632217e-03 2.2377539e-03 4.9768624e-01 1.2857310e-03\n",
      " 9.4042718e-04 1.4716162e-03 8.0880936e-04 8.5863500e-04 7.8731659e-04\n",
      " 9.9693120e-01 4.1663432e-03 9.9744927e-04 7.2505849e-04 7.9317833e-04\n",
      " 8.2890649e-04 7.5041869e-04 1.1137150e-03 7.4008160e-04 7.7598670e-04\n",
      " 1.2245920e-03 9.3314360e-04 9.9880087e-01 9.4802497e-04 8.9376868e-04\n",
      " 1.0875771e-03 8.1587117e-04 1.2799466e-03 3.2281652e-03 1.7236605e-03\n",
      " 7.7165634e-04 5.2515180e-03 3.7917960e-02 1.4309039e-02 6.1209425e-03\n",
      " 1.0245502e-02 2.7188790e-01 3.3269875e-02 1.7050576e-03 1.8145476e-03\n",
      " 2.6818756e-03 2.2128250e-03 2.2739777e-03 4.6588895e-03 2.6700431e-03\n",
      " 1.2666737e-03 2.0346346e-03 9.2744321e-04 2.9990047e-03 1.4759921e-03\n",
      " 1.8043651e-03 7.3090330e-04 9.7787206e-04 8.8727387e-04 1.0206160e-03\n",
      " 1.6029282e-03 1.3530731e-03 1.5052761e-03 1.5822970e-03 1.3714175e-03\n",
      " 2.1536497e-03 1.1692258e-03 8.6176163e-04 9.8773255e-04 6.5377005e-04\n",
      " 1.4149855e-03 9.8820182e-04 9.9176461e-01 2.5038728e-03 1.3070294e-03\n",
      " 7.8161596e-04 9.4700395e-04 8.4429316e-04 1.0058394e-03 7.0312800e-04\n",
      " 1.1448768e-03 1.7186349e-03 2.0105096e-03 1.5964275e-03 3.4675665e-02\n",
      " 2.7243984e-03 2.2797275e-03 2.1614188e-03 3.2201156e-03 2.0449562e-03\n",
      " 9.7446805e-01 6.4384053e-04 8.3570922e-04 9.9759030e-01 2.4720987e-03\n",
      " 9.8583513e-01 8.5475695e-01 8.3572319e-04 2.0458360e-01 9.8723196e-04\n",
      " 6.4391689e-04 2.0638296e-02 9.9830973e-01 5.4228352e-04 6.5763702e-04\n",
      " 1.2696818e-03 1.5864166e-03 1.2092747e-03 1.3314644e-02 3.0892189e-03\n",
      " 8.0923288e-04 8.9254323e-03 6.6732634e-03 3.2410584e-03 1.5508581e-03\n",
      " 1.6691314e-02 9.9801660e-01 6.7755958e-04 1.1553240e-03 2.8945066e-03\n",
      " 2.4003971e-03 9.4310659e-01 1.7441316e-03 9.9680806e-04 1.1069044e-03\n",
      " 2.7023276e-02 8.0328411e-04 5.2275523e-03 7.6282833e-04 1.1384744e-03\n",
      " 1.2829705e-03 3.8465210e-03 9.9824595e-01 9.8847665e-02 1.2840073e-03\n",
      " 1.0191688e-03 8.1537658e-04 9.9893473e-04 8.4063213e-04 1.1210094e-03\n",
      " 9.4194221e-04 1.0063483e-03 1.6344674e-03]\n",
      "Predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Test Results for model : {'eval_mcc_metric': 0.9128709291752768, 'eval_loss': 0.31356415152549744, 'eval_model_preparation_time': 0.0042, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.9893491124260355, 'eval_Precision': 1.0, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 0.1439, 'eval_samples_per_second': 993.611, 'eval_steps_per_second': 62.535}\n"
     ]
    }
   ],
   "source": [
    "test_results_clin = trainer.evaluate()\n",
    "\n",
    "print(\"Test Results for model :\", test_results_clin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob:  [0.49189648 0.50437635 0.49710193 0.4827579  0.5103262  0.47776976\n",
      " 0.5090503  0.5069377  0.48913968 0.5189021  0.5079457  0.49596068\n",
      " 0.5211956  0.4894604  0.50843394 0.49846822 0.48656672 0.5055379\n",
      " 0.48387793 0.4938234  0.50845313 0.504188   0.49817413 0.52219564\n",
      " 0.52048296 0.4922291  0.53118414 0.47201216 0.5077607  0.5244536\n",
      " 0.4820047  0.50981706 0.50513047 0.5067421  0.4950132  0.4992591\n",
      " 0.47697246 0.5135397  0.49186352 0.48615742 0.47971562 0.46490225\n",
      " 0.47868133 0.488366   0.486141   0.4672533  0.4727669  0.47844842\n",
      " 0.4937883  0.48017076 0.51966035 0.49457708 0.4964692  0.5018369\n",
      " 0.5183544  0.50759983 0.51033074 0.5088241  0.4970515  0.5005986\n",
      " 0.5208653  0.5099847  0.49592677 0.50016934 0.4997075  0.50380987\n",
      " 0.49675068 0.52640885 0.5133816  0.51943296 0.49700308 0.51433927\n",
      " 0.49498564 0.5046982  0.4887951  0.51450986 0.5070639  0.49390793\n",
      " 0.48778072 0.50806403 0.48961282 0.50639975 0.50782585 0.49060062\n",
      " 0.49575263 0.5130115  0.5095936  0.48658034 0.51929677 0.5024843\n",
      " 0.50804275 0.50248915 0.48701414 0.48072326 0.47826788 0.46809915\n",
      " 0.46928272 0.50853395 0.48577136 0.48169968 0.5070271  0.5158655\n",
      " 0.48537254 0.509838   0.5058953  0.50707185 0.50476843 0.4978731\n",
      " 0.49889374 0.500144   0.52669156 0.5129722  0.49173468 0.5098199\n",
      " 0.48930252 0.484235   0.4810644  0.50400007 0.50047714 0.48391148\n",
      " 0.52280575 0.49093753 0.51077485 0.50602746 0.500203   0.50356513\n",
      " 0.49860546 0.49539575 0.509928   0.4734068  0.48126337 0.50730866\n",
      " 0.5093264  0.5052407  0.4987348  0.50371283 0.4859705  0.51248074\n",
      " 0.5257139  0.48676926 0.4896353  0.5002519  0.5058259 ]\n",
      "Predictions: [0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1\n",
      " 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1\n",
      " 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 1]\n",
      "Labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "Test Results for model : {'eval_mcc_metric': -0.18144435093788364, 'eval_loss': 0.7050463557243347, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.43356643356643354, 'eval_AUC-ROC': 0.28520710059171595, 'eval_Precision': 0.04054054054054054, 'eval_Recall': 0.23076923076923078, 'eval_F1-score': 0.06896551724137931, 'eval_runtime': 0.1436, 'eval_samples_per_second': 995.519, 'eval_steps_per_second': 62.655}\n"
     ]
    }
   ],
   "source": [
    "test_results_clin = trainer.evaluate()\n",
    "\n",
    "print(\"Test Results for model :\", test_results_clin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 77MTR Model the training was poor, 0.50-0.60 precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Valid nested checkpoints found: ['./clintox_models_5MTR/0jhf6w1q/checkpoint-722', './clintox_models_5MTR/0jhf6w1q/checkpoint-608', './clintox_models_5MTR/0jhf6w1q/checkpoint-684', './clintox_models_5MTR/0jhf6w1q/checkpoint-646', './clintox_models_5MTR/0jhf6w1q/checkpoint-760', './clintox_models_5MTR/pycdsmqi/checkpoint-722', './clintox_models_5MTR/pycdsmqi/checkpoint-608', './clintox_models_5MTR/pycdsmqi/checkpoint-684', './clintox_models_5MTR/pycdsmqi/checkpoint-646', './clintox_models_5MTR/pycdsmqi/checkpoint-760', './clintox_models_5MTR/8mbsk0ip/checkpoint-722', './clintox_models_5MTR/8mbsk0ip/checkpoint-532', './clintox_models_5MTR/8mbsk0ip/checkpoint-608', './clintox_models_5MTR/8mbsk0ip/checkpoint-684', './clintox_models_5MTR/8mbsk0ip/checkpoint-646', './clintox_models_5MTR/wlgpdy7b/checkpoint-570', './clintox_models_5MTR/wlgpdy7b/checkpoint-532', './clintox_models_5MTR/wlgpdy7b/checkpoint-608', './clintox_models_5MTR/wlgpdy7b/checkpoint-646', './clintox_models_5MTR/wlgpdy7b/checkpoint-456', './clintox_models_5MTR/q33we7xu/checkpoint-494', './clintox_models_5MTR/q33we7xu/checkpoint-570', './clintox_models_5MTR/q33we7xu/checkpoint-608', './clintox_models_5MTR/q33we7xu/checkpoint-684', './clintox_models_5MTR/q33we7xu/checkpoint-646', './clintox_models_5MTR/8nooftvw/checkpoint-570', './clintox_models_5MTR/8nooftvw/checkpoint-532', './clintox_models_5MTR/8nooftvw/checkpoint-608', './clintox_models_5MTR/8nooftvw/checkpoint-646', './clintox_models_5MTR/8nooftvw/checkpoint-456', './clintox_models_5MTR/vthdn47v/checkpoint-722', './clintox_models_5MTR/vthdn47v/checkpoint-608', './clintox_models_5MTR/vthdn47v/checkpoint-684', './clintox_models_5MTR/vthdn47v/checkpoint-646', './clintox_models_5MTR/vthdn47v/checkpoint-760', './clintox_models_5MTR/8t48fjg0/checkpoint-722', './clintox_models_5MTR/8t48fjg0/checkpoint-570', './clintox_models_5MTR/8t48fjg0/checkpoint-684', './clintox_models_5MTR/8t48fjg0/checkpoint-646', './clintox_models_5MTR/8t48fjg0/checkpoint-760', './clintox_models_5MTR/mcnkpsf4/checkpoint-722', './clintox_models_5MTR/mcnkpsf4/checkpoint-532', './clintox_models_5MTR/mcnkpsf4/checkpoint-608', './clintox_models_5MTR/mcnkpsf4/checkpoint-684', './clintox_models_5MTR/mcnkpsf4/checkpoint-646', './clintox_models_5MTR/mc7f44lm/checkpoint-494', './clintox_models_5MTR/mc7f44lm/checkpoint-304', './clintox_models_5MTR/mc7f44lm/checkpoint-456', './clintox_models_5MTR/mc7f44lm/checkpoint-380', './clintox_models_5MTR/mc7f44lm/checkpoint-418']\n",
      "\n",
      "🔍 Evaluating model: 0jhf6w1q/checkpoint-722\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for 0jhf6w1q/checkpoint-722\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9153846153846154, 'eval_loss': 0.13465836644172668, 'eval_model_preparation_time': 0.0042, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.9946745562130177, 'eval_Precision': 0.9230769230769231, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.9230769230769231, 'eval_runtime': 0.1747, 'eval_samples_per_second': 818.635, 'eval_steps_per_second': 28.624}\n",
      "\n",
      "🔍 Evaluating model: 0jhf6w1q/checkpoint-608\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for 0jhf6w1q/checkpoint-608\n",
      "📌 Test Results: {'eval_mcc_metric': 0.785756725787005, 'eval_loss': 0.13363271951675415, 'eval_model_preparation_time': 0.004, 'eval_Accuracy': 0.958041958041958, 'eval_AUC-ROC': 0.9940828402366864, 'eval_Precision': 0.7058823529411765, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.8, 'eval_runtime': 0.1218, 'eval_samples_per_second': 1173.833, 'eval_steps_per_second': 41.043}\n",
      "\n",
      "🔍 Evaluating model: 0jhf6w1q/checkpoint-684\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for 0jhf6w1q/checkpoint-684\n",
      "📌 Test Results: {'eval_mcc_metric': 0.8443747686898277, 'eval_loss': 0.13402533531188965, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.972027972027972, 'eval_AUC-ROC': 0.9940828402366864, 'eval_Precision': 0.8, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.8571428571428571, 'eval_runtime': 0.1239, 'eval_samples_per_second': 1154.062, 'eval_steps_per_second': 40.352}\n",
      "\n",
      "🔍 Evaluating model: 0jhf6w1q/checkpoint-646\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for 0jhf6w1q/checkpoint-646\n",
      "📌 Test Results: {'eval_mcc_metric': 0.8443747686898277, 'eval_loss': 0.13350766897201538, 'eval_model_preparation_time': 0.0042, 'eval_Accuracy': 0.972027972027972, 'eval_AUC-ROC': 0.9940828402366864, 'eval_Precision': 0.8, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.8571428571428571, 'eval_runtime': 0.1226, 'eval_samples_per_second': 1165.986, 'eval_steps_per_second': 40.769}\n",
      "\n",
      "🔍 Evaluating model: 0jhf6w1q/checkpoint-760\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for 0jhf6w1q/checkpoint-760\n",
      "📌 Test Results: {'eval_mcc_metric': 0.9153846153846154, 'eval_loss': 0.13489842414855957, 'eval_model_preparation_time': 0.0041, 'eval_Accuracy': 0.986013986013986, 'eval_AUC-ROC': 0.9946745562130177, 'eval_Precision': 0.9230769230769231, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.9230769230769231, 'eval_runtime': 0.1227, 'eval_samples_per_second': 1165.771, 'eval_steps_per_second': 40.761}\n",
      "\n",
      "🔍 Evaluating model: pycdsmqi/checkpoint-722\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for pycdsmqi/checkpoint-722\n",
      "📌 Test Results: {'eval_mcc_metric': 0.785756725787005, 'eval_loss': 0.1470877081155777, 'eval_model_preparation_time': 0.0043, 'eval_Accuracy': 0.958041958041958, 'eval_AUC-ROC': 0.9934911242603551, 'eval_Precision': 0.7058823529411765, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.8, 'eval_runtime': 0.1219, 'eval_samples_per_second': 1172.801, 'eval_steps_per_second': 41.007}\n",
      "\n",
      "🔍 Evaluating model: pycdsmqi/checkpoint-608\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for pycdsmqi/checkpoint-608\n",
      "📌 Test Results: {'eval_mcc_metric': 0.785756725787005, 'eval_loss': 0.14588846266269684, 'eval_model_preparation_time': 0.004, 'eval_Accuracy': 0.958041958041958, 'eval_AUC-ROC': 0.9934911242603551, 'eval_Precision': 0.7058823529411765, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.8, 'eval_runtime': 0.1229, 'eval_samples_per_second': 1163.561, 'eval_steps_per_second': 40.684}\n",
      "\n",
      "🔍 Evaluating model: pycdsmqi/checkpoint-684\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for pycdsmqi/checkpoint-684\n",
      "📌 Test Results: {'eval_mcc_metric': 0.785756725787005, 'eval_loss': 0.14644131064414978, 'eval_model_preparation_time': 0.004, 'eval_Accuracy': 0.958041958041958, 'eval_AUC-ROC': 0.9934911242603551, 'eval_Precision': 0.7058823529411765, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.8, 'eval_runtime': 0.1212, 'eval_samples_per_second': 1180.283, 'eval_steps_per_second': 41.269}\n",
      "\n",
      "🔍 Evaluating model: pycdsmqi/checkpoint-646\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for pycdsmqi/checkpoint-646\n",
      "📌 Test Results: {'eval_mcc_metric': 0.785756725787005, 'eval_loss': 0.1458801031112671, 'eval_model_preparation_time': 0.004, 'eval_Accuracy': 0.958041958041958, 'eval_AUC-ROC': 0.9934911242603551, 'eval_Precision': 0.7058823529411765, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.8, 'eval_runtime': 0.1225, 'eval_samples_per_second': 1167.461, 'eval_steps_per_second': 40.82}\n",
      "\n",
      "🔍 Evaluating model: pycdsmqi/checkpoint-760\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for pycdsmqi/checkpoint-760\n",
      "📌 Test Results: {'eval_mcc_metric': 0.785756725787005, 'eval_loss': 0.14731107652187347, 'eval_model_preparation_time': 0.004, 'eval_Accuracy': 0.958041958041958, 'eval_AUC-ROC': 0.9934911242603551, 'eval_Precision': 0.7058823529411765, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.8, 'eval_runtime': 0.1234, 'eval_samples_per_second': 1158.549, 'eval_steps_per_second': 40.509}\n",
      "\n",
      "🔍 Evaluating model: 8mbsk0ip/checkpoint-722\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8mbsk0ip/checkpoint-722\n",
      "\n",
      "🔍 Evaluating model: 8mbsk0ip/checkpoint-532\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8mbsk0ip/checkpoint-532\n",
      "\n",
      "🔍 Evaluating model: 8mbsk0ip/checkpoint-608\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8mbsk0ip/checkpoint-608\n",
      "\n",
      "🔍 Evaluating model: 8mbsk0ip/checkpoint-684\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8mbsk0ip/checkpoint-684\n",
      "\n",
      "🔍 Evaluating model: 8mbsk0ip/checkpoint-646\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8mbsk0ip/checkpoint-646\n",
      "\n",
      "🔍 Evaluating model: wlgpdy7b/checkpoint-570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping wlgpdy7b/checkpoint-570\n",
      "\n",
      "🔍 Evaluating model: wlgpdy7b/checkpoint-532\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping wlgpdy7b/checkpoint-532\n",
      "\n",
      "🔍 Evaluating model: wlgpdy7b/checkpoint-608\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for wlgpdy7b/checkpoint-608\n",
      "📌 Test Results: {'eval_mcc_metric': 0.7140859186309388, 'eval_loss': 0.16212722659111023, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.9370629370629371, 'eval_AUC-ROC': 0.9911242603550295, 'eval_Precision': 0.6, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.7272727272727273, 'eval_runtime': 0.1232, 'eval_samples_per_second': 1160.825, 'eval_steps_per_second': 40.588}\n",
      "\n",
      "🔍 Evaluating model: wlgpdy7b/checkpoint-646\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for wlgpdy7b/checkpoint-646\n",
      "📌 Test Results: {'eval_mcc_metric': 0.7140859186309388, 'eval_loss': 0.15895630419254303, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.9370629370629371, 'eval_AUC-ROC': 0.9911242603550295, 'eval_Precision': 0.6, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.7272727272727273, 'eval_runtime': 0.1255, 'eval_samples_per_second': 1139.373, 'eval_steps_per_second': 39.838}\n",
      "\n",
      "🔍 Evaluating model: wlgpdy7b/checkpoint-456\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping wlgpdy7b/checkpoint-456\n",
      "\n",
      "🔍 Evaluating model: q33we7xu/checkpoint-494\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping q33we7xu/checkpoint-494\n",
      "\n",
      "🔍 Evaluating model: q33we7xu/checkpoint-570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for q33we7xu/checkpoint-570\n",
      "📌 Test Results: {'eval_mcc_metric': 0.8780578588816019, 'eval_loss': 0.16391222178936005, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.9790209790209791, 'eval_AUC-ROC': 0.9911242603550297, 'eval_Precision': 0.8571428571428571, 'eval_Recall': 0.9230769230769231, 'eval_F1-score': 0.8888888888888888, 'eval_runtime': 0.1268, 'eval_samples_per_second': 1127.423, 'eval_steps_per_second': 39.42}\n",
      "\n",
      "🔍 Evaluating model: q33we7xu/checkpoint-608\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for q33we7xu/checkpoint-608\n",
      "📌 Test Results: {'eval_mcc_metric': 0.8307692307692308, 'eval_loss': 0.16611522436141968, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.972027972027972, 'eval_AUC-ROC': 0.9911242603550297, 'eval_Precision': 0.8461538461538461, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.8461538461538461, 'eval_runtime': 0.1302, 'eval_samples_per_second': 1098.281, 'eval_steps_per_second': 38.401}\n",
      "\n",
      "🔍 Evaluating model: q33we7xu/checkpoint-684\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for q33we7xu/checkpoint-684\n",
      "📌 Test Results: {'eval_mcc_metric': 0.8307692307692308, 'eval_loss': 0.16798891127109528, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.972027972027972, 'eval_AUC-ROC': 0.9911242603550297, 'eval_Precision': 0.8461538461538461, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.8461538461538461, 'eval_runtime': 0.1315, 'eval_samples_per_second': 1087.833, 'eval_steps_per_second': 38.036}\n",
      "\n",
      "🔍 Evaluating model: q33we7xu/checkpoint-646\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for q33we7xu/checkpoint-646\n",
      "📌 Test Results: {'eval_mcc_metric': 0.8307692307692308, 'eval_loss': 0.16694866120815277, 'eval_model_preparation_time': 0.004, 'eval_Accuracy': 0.972027972027972, 'eval_AUC-ROC': 0.9911242603550297, 'eval_Precision': 0.8461538461538461, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.8461538461538461, 'eval_runtime': 0.1276, 'eval_samples_per_second': 1120.634, 'eval_steps_per_second': 39.183}\n",
      "\n",
      "🔍 Evaluating model: 8nooftvw/checkpoint-570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8nooftvw/checkpoint-570\n",
      "\n",
      "🔍 Evaluating model: 8nooftvw/checkpoint-532\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8nooftvw/checkpoint-532\n",
      "\n",
      "🔍 Evaluating model: 8nooftvw/checkpoint-608\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8nooftvw/checkpoint-608\n",
      "\n",
      "🔍 Evaluating model: 8nooftvw/checkpoint-646\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8nooftvw/checkpoint-646\n",
      "\n",
      "🔍 Evaluating model: 8nooftvw/checkpoint-456\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8nooftvw/checkpoint-456\n",
      "\n",
      "🔍 Evaluating model: vthdn47v/checkpoint-722\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping vthdn47v/checkpoint-722\n",
      "\n",
      "🔍 Evaluating model: vthdn47v/checkpoint-608\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping vthdn47v/checkpoint-608\n",
      "\n",
      "🔍 Evaluating model: vthdn47v/checkpoint-684\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping vthdn47v/checkpoint-684\n",
      "\n",
      "🔍 Evaluating model: vthdn47v/checkpoint-646\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping vthdn47v/checkpoint-646\n",
      "\n",
      "🔍 Evaluating model: vthdn47v/checkpoint-760\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping vthdn47v/checkpoint-760\n",
      "\n",
      "🔍 Evaluating model: 8t48fjg0/checkpoint-722\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for 8t48fjg0/checkpoint-722\n",
      "📌 Test Results: {'eval_mcc_metric': 0.8693611470909237, 'eval_loss': 0.16874969005584717, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.9790209790209791, 'eval_AUC-ROC': 0.9911242603550295, 'eval_Precision': 0.9166666666666666, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.88, 'eval_runtime': 0.1187, 'eval_samples_per_second': 1204.865, 'eval_steps_per_second': 42.128}\n",
      "\n",
      "🔍 Evaluating model: 8t48fjg0/checkpoint-570\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8t48fjg0/checkpoint-570\n",
      "\n",
      "🔍 Evaluating model: 8t48fjg0/checkpoint-684\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for 8t48fjg0/checkpoint-684\n",
      "📌 Test Results: {'eval_mcc_metric': 0.8693611470909237, 'eval_loss': 0.16767965257167816, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.9790209790209791, 'eval_AUC-ROC': 0.9911242603550295, 'eval_Precision': 0.9166666666666666, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.88, 'eval_runtime': 0.1175, 'eval_samples_per_second': 1217.208, 'eval_steps_per_second': 42.56}\n",
      "\n",
      "🔍 Evaluating model: 8t48fjg0/checkpoint-646\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for 8t48fjg0/checkpoint-646\n",
      "📌 Test Results: {'eval_mcc_metric': 0.8693611470909237, 'eval_loss': 0.16638915240764618, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.9790209790209791, 'eval_AUC-ROC': 0.9911242603550295, 'eval_Precision': 0.9166666666666666, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.88, 'eval_runtime': 0.1165, 'eval_samples_per_second': 1227.979, 'eval_steps_per_second': 42.936}\n",
      "\n",
      "🔍 Evaluating model: 8t48fjg0/checkpoint-760\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for 8t48fjg0/checkpoint-760\n",
      "📌 Test Results: {'eval_mcc_metric': 0.8693611470909237, 'eval_loss': 0.1692180186510086, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.9790209790209791, 'eval_AUC-ROC': 0.9911242603550295, 'eval_Precision': 0.9166666666666666, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.88, 'eval_runtime': 0.1193, 'eval_samples_per_second': 1198.461, 'eval_steps_per_second': 41.904}\n",
      "\n",
      "🔍 Evaluating model: mcnkpsf4/checkpoint-722\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for mcnkpsf4/checkpoint-722\n",
      "📌 Test Results: {'eval_mcc_metric': 0.8693611470909237, 'eval_loss': 0.17893646657466888, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.9790209790209791, 'eval_AUC-ROC': 0.991715976331361, 'eval_Precision': 0.9166666666666666, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.88, 'eval_runtime': 0.1179, 'eval_samples_per_second': 1212.964, 'eval_steps_per_second': 42.411}\n",
      "\n",
      "🔍 Evaluating model: mcnkpsf4/checkpoint-532\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for mcnkpsf4/checkpoint-532\n",
      "📌 Test Results: {'eval_mcc_metric': 0.8693611470909237, 'eval_loss': 0.1629505604505539, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.9790209790209791, 'eval_AUC-ROC': 0.9911242603550295, 'eval_Precision': 0.9166666666666666, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.88, 'eval_runtime': 0.1186, 'eval_samples_per_second': 1206.229, 'eval_steps_per_second': 42.176}\n",
      "\n",
      "🔍 Evaluating model: mcnkpsf4/checkpoint-608\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for mcnkpsf4/checkpoint-608\n",
      "📌 Test Results: {'eval_mcc_metric': 0.8693611470909237, 'eval_loss': 0.1741529256105423, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.9790209790209791, 'eval_AUC-ROC': 0.9911242603550295, 'eval_Precision': 0.9166666666666666, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.88, 'eval_runtime': 0.1289, 'eval_samples_per_second': 1109.274, 'eval_steps_per_second': 38.786}\n",
      "\n",
      "🔍 Evaluating model: mcnkpsf4/checkpoint-684\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for mcnkpsf4/checkpoint-684\n",
      "📌 Test Results: {'eval_mcc_metric': 0.8693611470909237, 'eval_loss': 0.1775006651878357, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.9790209790209791, 'eval_AUC-ROC': 0.991715976331361, 'eval_Precision': 0.9166666666666666, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.88, 'eval_runtime': 0.1195, 'eval_samples_per_second': 1196.799, 'eval_steps_per_second': 41.846}\n",
      "\n",
      "🔍 Evaluating model: mcnkpsf4/checkpoint-646\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.99 for mcnkpsf4/checkpoint-646\n",
      "📌 Test Results: {'eval_mcc_metric': 0.8693611470909237, 'eval_loss': 0.17545665800571442, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.9790209790209791, 'eval_AUC-ROC': 0.991715976331361, 'eval_Precision': 0.9166666666666666, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.88, 'eval_runtime': 0.1169, 'eval_samples_per_second': 1223.64, 'eval_steps_per_second': 42.785}\n",
      "\n",
      "🔍 Evaluating model: mc7f44lm/checkpoint-494\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping mc7f44lm/checkpoint-494\n",
      "\n",
      "🔍 Evaluating model: mc7f44lm/checkpoint-304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping mc7f44lm/checkpoint-304\n",
      "\n",
      "🔍 Evaluating model: mc7f44lm/checkpoint-456\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping mc7f44lm/checkpoint-456\n",
      "\n",
      "🔍 Evaluating model: mc7f44lm/checkpoint-380\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1883300/1173159616.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping mc7f44lm/checkpoint-380\n",
      "\n",
      "🔍 Evaluating model: mc7f44lm/checkpoint-418\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping mc7f44lm/checkpoint-418\n"
     ]
    }
   ],
   "source": [
    "# List all checkpoints inside models directory\n",
    "import os\n",
    "from peft import PeftModel\n",
    "\n",
    "models_dir = \"./clintox_models_5MTR\"\n",
    "\n",
    "def find_all_checkpoints(base_dir):\n",
    "    all_checkpoints = []\n",
    "    for folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for subfolder in os.listdir(folder_path):\n",
    "                subfolder_path = os.path.join(folder_path, subfolder)\n",
    "                if os.path.isdir(subfolder_path) and subfolder.startswith(\"checkpoint-\"):\n",
    "                    if os.path.exists(os.path.join(subfolder_path, \"adapter_config.json\")):\n",
    "                        all_checkpoints.append(subfolder_path)\n",
    "    return all_checkpoints\n",
    "\n",
    "valid_checkpoints = find_all_checkpoints(models_dir)\n",
    "print(\"🧠 Valid nested checkpoints found:\", valid_checkpoints)\n",
    "\n",
    "for checkpoint_path in valid_checkpoints:\n",
    "    checkpoint_name = os.path.basename(checkpoint_path)\n",
    "    parent_folder = os.path.basename(os.path.dirname(checkpoint_path))\n",
    "\n",
    "    print(f\"\\n🔍 Evaluating model: {parent_folder}/{checkpoint_name}\")\n",
    "\n",
    "    adapter_model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    adapter_model.eval()\n",
    "\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=adapter_model,\n",
    "        args=training_args,\n",
    "        eval_dataset=test_dataset_clin,\n",
    "        tokenizer=tokenizer_clin,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "\n",
    "    test_results = trainer.evaluate()\n",
    "    auc_score = test_results[\"eval_AUC-ROC\"]\n",
    "    \n",
    "\n",
    "    if auc_score > 0.991:\n",
    "        print(f\"✅ AUC_ROC > 0.99 for {parent_folder}/{checkpoint_name}\")\n",
    "        print(f\"📌 Test Results: {test_results}\")\n",
    "    else:\n",
    "        print(f\"❌ Skipping {parent_folder}/{checkpoint_name}\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model for clintox chem 77M MTR:u5fiiru1/checkpoint-228\n",
    "\n",
    "📌 Test Results: {'eval_mcc_metric': 0.7416198487095662, 'eval_loss': 0.5224099159240723, 'eval_Accuracy': 0.9370629370629371, 'eval_AUC-ROC': 0.9875739644970415, 'eval_Precision': 0.5909090909090909, 'eval_Recall': 1.0, 'eval_F1-score': 0.7428571428571429, 'eval_runtime': 0.2582, 'eval_samples_per_second': 553.851, 'eval_steps_per_second': 11.619}\n",
    "\n",
    "### Best model for clintox chem 10M MLM : 3j0ob8j9/checkpoint-266\n",
    "\n",
    "📌 Test Results: {'eval_mcc_metric': 0.8693611470909237, 'eval_loss': 0.18695949018001556, 'eval_Accuracy': 0.9790209790209791, 'eval_AUC-ROC': 0.9881656804733728, 'eval_Precision': 0.9166666666666666, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.88, 'eval_runtime': 0.7212, 'eval_samples_per_second': 198.273, 'eval_steps_per_second': 4.16}\n",
    "\n",
    "### Best model for clintox chem 10M MTR: r98v7m7c/checkpoint-152\n",
    "\n",
    "📌 Test Results: {'eval_mcc_metric': 0.7041543391425868, 'eval_loss': 0.5360366702079773, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.9230769230769231, 'eval_AUC-ROC': 0.9852071005917159, 'eval_Precision': 0.5416666666666666, 'eval_Recall': 1.0, 'eval_F1-score': 0.7027027027027027, 'eval_runtime': 0.1188, 'eval_samples_per_second': 1204.016, 'eval_steps_per_second': 42.098}\n",
    "\n",
    "### Best model for clin chem 5M MTR: mcnkpsf4/checkpoint-646\n",
    "📌 Test Results: {'eval_mcc_metric': 0.8693611470909237, 'eval_loss': 0.17545665800571442, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.9790209790209791, 'eval_AUC-ROC': 0.991715976331361, 'eval_Precision': 0.9166666666666666, 'eval_Recall': 0.8461538461538461, 'eval_F1-score': 0.88, 'eval_runtime': 0.1169, 'eval_samples_per_second': 1223.64, 'eval_steps_per_second': 42.785}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Merge the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'DeepChem/ChemBERTa-5M-MTR',\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel  \n",
    "\n",
    "adapter_model = PeftModel.from_pretrained(base_model, \"/home/raghvendra2/Molformer_Finetuning/clintox_models_5MTR/mcnkpsf4/checkpoint-646\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_clintox_chemberta_5m_MTR= adapter_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model to Chemberta finetuned model lora 100M MTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/raghvendra2/Molformer_Finetuning/Clintox_Final_chemberta_5m_mtr_model\"\n",
    "\n",
    "final_model_clintox_chemberta_5m_MTR.save_pretrained(save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Molformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
