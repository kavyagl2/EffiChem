{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1beda4e5",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning using WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ee29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import wandb\n",
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from evaluate import load\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cbd481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load():\n",
    "    train_bbbp=pd.read_csv('bbbp_dataset/train.csv')\n",
    "    val_bbbp=pd.read_csv('bbbp_dataset/valid.csv')\n",
    "\n",
    "    return train_bbbp, val_bbbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f2770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data_process,tokenizer):\n",
    "\n",
    "    smiles_list = data_process['smiles'].tolist()\n",
    "    tokenized=tokenizer(smiles_list)\n",
    "    \n",
    "    \n",
    "    dataset = Dataset.from_dict(tokenized)\n",
    "    \n",
    "\n",
    "    labels = data_process['p_np'].tolist() \n",
    "    \n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "    \n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "def lora_config(r,lora_alpha,dropout):\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",  # Sequence classification task\n",
    "        r=r,  \n",
    "        lora_alpha=lora_alpha,  \n",
    "        target_modules='all-linear',\n",
    "        lora_dropout=dropout \n",
    "    )\n",
    "\n",
    "    return lora_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59ce24",
   "metadata": {},
   "source": [
    "### Weighted Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60610c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_weights_calculation(train_dataset):\n",
    "\n",
    "        # Calculate class weights based on the distribution of labels\n",
    "        class_weights = [1 - (train_dataset['labels'].count(0) / len(train_dataset['labels'])),\n",
    "                        1 - (train_dataset['labels'].count(1) / len(train_dataset['labels']))]\n",
    "        return torch.from_numpy(np.array(class_weights)).float()\n",
    "\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "   \n",
    "    def compute_loss(self, model, inputs,train_dataset, return_outputs=False, num_items_in_batch=None):\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Extract labels\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        class_weights= class_weights_calculation(train_dataset)\n",
    "        # compute custom loss (suppose one has 2 labels with different weights)\n",
    "        loss_func = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_func(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85abb315",
   "metadata": {},
   "source": [
    "### Focal Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f869a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#focal loss computation\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def focal_loss(inputs, targets, alpha=1, gamma=2):\n",
    "    log_prob = F.log_softmax(inputs, dim=-1)\n",
    "    prob = torch.exp(log_prob)  # Convert log probabilities back to normal probabilities\n",
    "\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[-1])\n",
    "    pt = torch.sum(prob * targets_one_hot, dim=-1)  # Get probability of the true class\n",
    "\n",
    "    focal_loss = -alpha * (1 - pt) ** gamma * torch.sum(log_prob * targets_one_hot, dim=-1)\n",
    "    \n",
    "    return focal_loss.mean() \n",
    "\n",
    "\n",
    "class FocalLossTrainer(Trainer):\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = focal_loss(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4d1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef\n",
    "\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "mcc_metric= load(\"matthews_correlation\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    probabilities = softmax(logits, axis=1)[:, 1]  # Get probabilities for class 1\n",
    "    predictions = np.argmax(logits, axis=1)  # Choose the most likely class\n",
    "    \n",
    "\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        \"eval_mcc_metric\": mcc,\n",
    "        \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"AUC-ROC\": roc_auc_score(labels, probabilities),  # AUC-ROC requires probabilities\n",
    "        \"Precision\": precision_score(labels, predictions),\n",
    "        \"Recall\": recall_score(labels, predictions),\n",
    "        \"F1-score\": f1_score(labels, predictions)\n",
    "    } \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b785956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sweep_config = {\n",
    "\"name\": \"Flavor Hyperparameter Tuning\",\n",
    "\"method\": \"bayes\",\n",
    "\"metric\": {\n",
    "    \"goal\": \"maximize\", \n",
    "    \"name\": \"eval/mcc_metric\"},\n",
    "\"parameters\": {\"lr\": {\n",
    "        \"distribution\": \"uniform\",\n",
    "        \"min\": 1e-5,  \n",
    "        \"max\": 2e-3},\n",
    "    \"r\": {\"values\": [4,8,16,32,64, 128]},\n",
    "    \"lora_alpha\": {\"values\": [4,8,16,32,64,128]},\n",
    "    \"dropout\": {\"values\": [0.0,0.1,0.2] },\n",
    "    \n",
    "    \"optimizer\": {\"value\": [\"adamw\"]}}\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"huggingface\")\n",
    "\n",
    "model_list= [\"DeepChem/ChemBERTa-77M-MLM\",\n",
    "             \"DeepChem/ChemBERTa-10M-MLM\",\n",
    "             \"DeepChem/ChemBERTa-10M-MTR\",\n",
    "             \"DeepChem/ChemBERTa-5M-MTR\",\n",
    "             \"DeepChem/ChemBERTa-77M-MTR\",\n",
    "             \"ibm/MoLFormer-XL-both-10pct\"]\n",
    "for model_name in model_list:\n",
    "    print(f\"Running sweep for model: {model_name}\")\n",
    "    \n",
    "    def safe_model_name(name1):\n",
    "        return re.sub(r\"[^a-zA-Z0-9]\", \"__\", name1)\n",
    "\n",
    "\n",
    "    def run_training():\n",
    "        print(f\"Running training for model: {model_name}\")\n",
    "        # Initialize W&B with sweep\n",
    "        run = wandb.init(project=\"BBBP focal loss Hyperparameter Tuning\")\n",
    "        config = run.config\n",
    "\n",
    "        model_id_clean = safe_model_name(model_name)\n",
    "        print(f\"Model ID cleaned: {model_id_clean}\")\n",
    "        run_id = wandb.run.id\n",
    "\n",
    "        # Define unique output folders\n",
    "        save_dir = f\".../{model_id_clean}/{run_id}\"\n",
    "        logging_dir = f\".../{model_id_clean}/{run_id}\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Load tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=2,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # Load and preprocess data\n",
    "\n",
    "        train_data, val_data = data_load()\n",
    "        training_data = data_prep(train_data, tokenizer)\n",
    "        validation_data = data_prep(val_data, tokenizer)\n",
    "\n",
    "        # Apply LoRA\n",
    "        peft_config = lora_config(config.r, config.lora_alpha, config.dropout)\n",
    "        lora_model = get_peft_model(model, peft_config)\n",
    "        lora_model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "        # Define training args\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=save_dir,\n",
    "            eval_strategy=\"epoch\",\n",
    "            learning_rate=config.lr,\n",
    "            per_device_train_batch_size=32,\n",
    "            per_device_eval_batch_size=32,\n",
    "            num_train_epochs=10,\n",
    "            weight_decay=0.01,\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_dir=logging_dir,\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=500,\n",
    "            report_to=\"wandb\",\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_mcc_metric\"\n",
    "        )\n",
    "\n",
    "        accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            logits, labels = eval_pred\n",
    "            probabilities = softmax(logits, axis=1)[:, 1]\n",
    "            predictions = np.argmax(logits, axis=1)\n",
    "            mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "            return {\n",
    "                \"eval_mcc_metric\": mcc,\n",
    "                \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "                \"AUC-ROC\": roc_auc_score(labels, probabilities),\n",
    "                \"Precision\": precision_score(labels, predictions),\n",
    "                \"Recall\": recall_score(labels, predictions),\n",
    "                \"F1-score\": f1_score(labels, predictions)\n",
    "            }\n",
    "\n",
    "        # Train with weigted loss trainer\n",
    "        trainer = WeightedLossTrainer(\n",
    "            model=lora_model,\n",
    "            args=training_args,\n",
    "            train_dataset=training_data,\n",
    "            eval_dataset=validation_data,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "        )\n",
    "        \n",
    "        '''\n",
    "        # train with focal loss trainer\n",
    "        trainer = FocalLossTrainer(\n",
    "            model=lora_model,\n",
    "            args=training_args,\n",
    "            train_dataset=training_data,\n",
    "            eval_dataset=validation_data,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "        )'''\n",
    "\n",
    "        trainer.train()\n",
    "        trainer.save_model(save_dir)\n",
    "        print(f\"Model saved to {save_dir}\")\n",
    "        print(f\"Training completed for model: {model_name}\")\n",
    "        \n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "\n",
    "    wandb.agent(sweep_id, function=run_training, count=5)\n",
    "\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(f\"huggingface/{sweep_id}\")\n",
    "    print(sweep.runs[0].summary_metrics)\n",
    "\n",
    "    runs_with_rmse = [run for run in sweep.runs if 'eval/mcc_metric' in run.summary_metrics]\n",
    "    if runs_with_rmse:\n",
    "        # Sort by rmse in descending order (maximize)\n",
    "        best_run = sorted(runs_with_rmse, key=lambda run: run.summary_metrics['eval/mcc_metric'])[0]\n",
    "    else:\n",
    "        raise ValueError(\"No runs found with 'eval/mcc_metric' metric.\")\n",
    "\n",
    "    best_hyperparameters = best_run.config\n",
    "    print(f\"Best hyperparameters: {best_hyperparameters}\")\n",
    "    print(\"completed sweep for model: \",model_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8705b2",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a625624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = softmax(logits, axis=1)[:, 1]  # Get probabilities for class 1\n",
    "        predictions = np.argmax(logits, axis=1)  # Choose the most likely class\n",
    "        mcc = matthews_corrcoef(labels, predictions)\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities),  # AUC-ROC requires probabilities\n",
    "            \"Precision\": precision_score(labels, predictions),\n",
    "            \"Recall\": recall_score(labels, predictions),\n",
    "            \"F1-score\": f1_score(labels, predictions)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aefc43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Map your folder names to the base HuggingFace model names\n",
    "MODEL_NAME_MAP = {\n",
    "    \"DeepChem__ChemBERTa__5M__MTR\": \"DeepChem/ChemBERTa-5M-MTR\",\n",
    "    \"DeepChem__ChemBERTa__10M__MTR\": \"DeepChem/ChemBERTa-10M-MTR\",\n",
    "    \"DeepChem__ChemBERTa__77M__MLM\": \"DeepChem/ChemBERTa-77M-MLM\",\n",
    "    \"DeepChem__ChemBERTa__10M__MLM\": \"DeepChem/ChemBERTa-10M-MLM\",\n",
    "    \"DeepChem__ChemBERTa__77M__MTR\": \"DeepChem/ChemBERTa-77M-MTR\",\n",
    "    \"ibm__MoLFormer__XL__both__10pct\": \"ibm/MoLFormer-XL-both-10pct\",\n",
    "       \n",
    "}\n",
    "\n",
    "test_data=pd.read_csv('bbbp_dataset/test.csv')\n",
    "\n",
    "models_root_dir = \".../models_bbbp_chem_WL\" #add model saved path\n",
    "\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"./test_results_bbbp\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    report_to=\"none\",  \n",
    "    disable_tqdm=True, \n",
    "  \n",
    "\n",
    ")\n",
    "\n",
    "def find_all_peft_checkpoints(root_dir):\n",
    "    checkpoints = []\n",
    "    for model_folder in os.listdir(root_dir):\n",
    "        model_folder_path = os.path.join(root_dir, model_folder)\n",
    "        if not os.path.isdir(model_folder_path):\n",
    "            continue\n",
    "        for run_id in os.listdir(model_folder_path):\n",
    "            run_path = os.path.join(model_folder_path, run_id)\n",
    "            if not os.path.isdir(run_path):\n",
    "                continue\n",
    "            for subdir in os.listdir(run_path):\n",
    "                checkpoint_path = os.path.join(run_path, subdir)\n",
    "                if subdir.startswith(\"checkpoint-\") and os.path.exists(os.path.join(checkpoint_path, \"adapter_config.json\")):\n",
    "                    checkpoints.append((model_folder, run_id, checkpoint_path))\n",
    "    return checkpoints\n",
    "\n",
    "valid_checkpoints = find_all_peft_checkpoints(models_root_dir)\n",
    "print(f\"Found {len(valid_checkpoints)} valid checkpoints.\")\n",
    "\n",
    "for model_folder, run_id, checkpoint_path in valid_checkpoints:\n",
    "    print(\"Model folder: \",model_folder)\n",
    "\n",
    "    hf_model_name = MODEL_NAME_MAP[model_folder]\n",
    "    print(f\"Using base model: {hf_model_name}\")\n",
    "    # Load tokenizer and base model for the model type\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hf_model_name, trust_remote_code=True)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        hf_model_name,\n",
    "        num_labels=2,\n",
    "        problem_type=\"single_label_classification\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    from datasets import Dataset\n",
    "\n",
    "    smiles_test = test_data['smiles'].tolist()\n",
    "\n",
    "    test_tokenized =tokenizer(smiles_test)\n",
    "\n",
    "    test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "    test_labels = test_data['p_np'].tolist() \n",
    "\n",
    "\n",
    "    test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "\n",
    "    # Load the adapter checkpoint\n",
    "    adapter_model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    adapter_model.eval()\n",
    "\n",
    "    # Eval\n",
    "    from transformers import Trainer\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=adapter_model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    print(f\"\\n Evaluating {model_folder}/{run_id}/{os.path.basename(checkpoint_path)}\")\n",
    "    \n",
    "    test_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f287b",
   "metadata": {},
   "source": [
    "## Load and Merge Base Model with LoRA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-77M-MLM\", #add the base model name\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True,\n",
    "    \n",
    ")\n",
    "from peft import PeftModel  \n",
    "\n",
    "adapter_model = PeftModel.from_pretrained(base_model, \".../models_bbbp_chem_WL/DeepChem__ChemBERTa__77M__MLM/x38bwbvz/checkpoint-416\")\n",
    "\n",
    "final_model_clintox_molformer= adapter_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \".../Final_merged_model/weighted_loss_bbbp/final_model_chem_77M-MLM-WL\"\n",
    "\n",
    "final_model_clintox_molformer.save_pretrained(save_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
