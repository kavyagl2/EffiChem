{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ded246",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning using WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b97a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Importing Libraries\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import PeftModel  \n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7edc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load():\n",
    "    train_flav=pd.read_csv('.../flavor_datasets/fart_train.csv')\n",
    "    val_flav=pd.read_csv('.../flavor_datasets/fart_val.csv')\n",
    "    train_flav.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "    val_flav.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "\n",
    "    return train_flav, val_flav  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data_process):\n",
    "\n",
    "    dataset = Dataset.from_pandas(data_process)\n",
    "       \n",
    "\n",
    "    return dataset\n",
    "\n",
    "def tokenize_function(examples,tokenizer):\n",
    "\n",
    "    return tokenizer(examples[\"Canonicalized SMILES\"], padding=\"max_length\", truncation=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ae29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(dataset):\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    encoded_labels = label_encoder.fit_transform(dataset['Canonicalized Taste'])\n",
    "\n",
    "    dataset = dataset.add_column('label', encoded_labels)\n",
    "    \n",
    "    columns_to_remove = [\"Canonicalized SMILES\", \"Standardized SMILES\", \n",
    "                     \"Canonicalized Taste\", \"Original Labels\", \"Source\", \"is_multiclass\"]\n",
    "\n",
    "\n",
    "    dataset = dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed031ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def lora_config(r, lora_alpha, dropout):\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",  # Sequence classification task\n",
    "        r=r,  # Rank of LoRA matrices\n",
    "        lora_alpha=lora_alpha,  # Scaling factor double of rank( from the rule of thumb)\n",
    "        target_modules='all-linear',\n",
    "        lora_dropout=dropout  # Dropout rate\n",
    "        #init_lora_weights=\"gaussian\"\n",
    "    )\n",
    "\n",
    "    return lora_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411eee69",
   "metadata": {},
   "source": [
    "## Focal Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b43ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def focal_loss(inputs, targets, alpha=1, gamma=2):\n",
    "    log_prob = F.log_softmax(inputs, dim=-1)\n",
    "    prob = torch.exp(log_prob)  # Convert log probabilities back to normal probabilities\n",
    "\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[-1])\n",
    "    pt = torch.sum(prob * targets_one_hot, dim=-1)  # Get probability of the true class\n",
    "\n",
    "    focal_loss = -alpha * (1 - pt) ** gamma * torch.sum(log_prob * targets_one_hot, dim=-1)\n",
    "    \n",
    "    return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8235cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class focalloss(Trainer):\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = focal_loss(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2562fd",
   "metadata": {},
   "source": [
    "## Weighted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Trainer\n",
    "from collections import Counter\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Extract labels from train_dataset\n",
    "        labels = self.train_dataset['labels']  \n",
    "\n",
    "        # Count label frequencies\n",
    "        label_counts = Counter(labels)\n",
    "        total_count = len(labels)\n",
    "\n",
    "        # Compute inverse frequency weights\n",
    "        num_classes = self.model.config.num_labels\n",
    "        weights = [1 - (label_counts[i] / total_count) if i in label_counts else 1.0 for i in range(num_classes)]\n",
    "\n",
    "        self.class_weights = torch.tensor(weights).float().to(\"cuda\")\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False,num_items_in_batch=None, **kwargs):\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # Use class weights in CrossEntropyLoss\n",
    "        loss_func = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_func(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f668bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sweep_config = {\n",
    "\"name\": \"Flavor Hyperparameter Tuning\",\n",
    "\"method\": \"bayes\",\n",
    "\"metric\": {\n",
    "    \"goal\": \"maximize\", \n",
    "    \"name\": \"eval/mcc_metric\"},\n",
    "\"parameters\": {\"lr\": {\n",
    "        \"distribution\": \"uniform\",\n",
    "        \"min\": 1e-5,  \n",
    "        \"max\": 2e-3},\n",
    "    \"r\": {\"values\": [4,8,16,32,64, 128]},\n",
    "    \"lora_alpha\": {\"values\": [4,8,16,32,64,128]},\n",
    "    \"dropout\": {\"values\": [0.0,0.1,0.2] },\n",
    "    \n",
    "    \"optimizer\": {\"value\": [\"adamw\"]}}\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"huggingface\")\n",
    "\n",
    "model_list= [\"DeepChem/ChemBERTa-77M-MLM\",\n",
    "             \"DeepChem/ChemBERTa-10M-MLM\",\n",
    "             \"DeepChem/ChemBERTa-10M-MTR\",\n",
    "             \"DeepChem/ChemBERTa-5M-MTR\",\n",
    "             \"DeepChem/ChemBERTa-77M-MTR\",\n",
    "             \"ibm/MoLFormer-XL-both-10pct\"]\n",
    "\n",
    "for model_name in model_list:\n",
    "    print(f\"Running sweep for model: {model_name}\")\n",
    "    \n",
    "    def safe_model_name(name1):\n",
    "        return re.sub(r\"[^a-zA-Z0-9]\", \"__\", name1)\n",
    "\n",
    "\n",
    "    \n",
    "    def run_training():\n",
    "\n",
    "        run = wandb.init(project=\"flavor analysis chemberta Hyperparameter Tuning\")\n",
    "        config = run.config\n",
    "\n",
    "\n",
    "        model_id_clean = safe_model_name(model_name)\n",
    "        print(f\"Model ID cleaned: {model_id_clean}\")\n",
    "        run_id = wandb.run.id\n",
    "\n",
    "        # Define unique output folders\n",
    "        save_dir = f\".../{model_id_clean}/{run_id}\"\n",
    "        logging_dir = f\".../{model_id_clean}/{run_id}\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=5,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "\n",
    "        train_data, val_data=data_load()\n",
    "        training_data=data_prep(train_data)\n",
    "        validation_data=data_prep(val_data)    \n",
    "        training_data=training_data.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "        validation_data=validation_data.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "\n",
    "\n",
    "        training_data=label_encoding(training_data)\n",
    "        validation_data=label_encoding(validation_data)\n",
    "        \n",
    "\n",
    "        peft_config = lora_config(config.r, config.lora_alpha, config.dropout)\n",
    "        lora_model = get_peft_model(model, peft_config)\n",
    "        lora_model.print_trainable_parameters()\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        eval_strategy=\"steps\",\n",
    "        learning_rate=config.lr,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"steps\",\n",
    "        logging_dir=\"./logs_flavor_chem_wandb\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        report_to=\"wandb\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_mcc_metric\",\n",
    "        greater_is_better=True,\n",
    "        remove_unused_columns=False,\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "        metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "\n",
    "            logits, labels = eval_pred\n",
    "\n",
    "            predictions = np.argmax(logits, axis=-1)\n",
    "            probabilities= softmax(logits, axis=1)\n",
    "            mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "                \n",
    "            return {\n",
    "                    \"eval_mcc_metric\": mcc,\n",
    "                    \"Accuracy\": metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "                    \"AUC-ROC\": roc_auc_score(labels, probabilities,multi_class=\"ovr\"),  # AUC-ROC requires probabilities\n",
    "                    \"Precision\": precision_score(labels, predictions,average=\"macro\"),\n",
    "                    \"Recall\": recall_score(labels, predictions,average=\"macro\"),\n",
    "                    \"F1-score\": f1_score(labels, predictions,average=\"macro\")\n",
    "                }\n",
    "\n",
    "\n",
    "        trainer_flavor = WeightedLossTrainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=training_data,\n",
    "        eval_dataset= validation_data,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        trainer_flavor = focalloss(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=training_data,\n",
    "        eval_dataset= validation_data,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        trainer_flavor.train()\n",
    "        trainer_flavor.save_model(save_dir)\n",
    "            \n",
    "        print(f\"Model saved to {save_dir}\")\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    wandb.agent(sweep_id, function=run_training, count=5)\n",
    "\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(f\"huggingface/{sweep_id}\")\n",
    "    print(sweep.runs[0].summary_metrics)\n",
    "\n",
    "    runs_with_rmse = [run for run in sweep.runs if 'eval/mcc_metric' in run.summary_metrics]\n",
    "    if runs_with_rmse:\n",
    "        # Sort by rmse in descending order (maximize)\n",
    "        best_run = sorted(runs_with_rmse, key=lambda run: run.summary_metrics['eval/mcc_metric'])[0]\n",
    "    else:\n",
    "        raise ValueError(\"No runs found with 'eval/mcc_metric' metric.\")\n",
    "\n",
    "    best_hyperparameters = best_run.config\n",
    "    print(f\"Best hyperparameters: {best_hyperparameters}\")\n",
    "    print(\"completed sweep for model: \",model_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36faa73e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee564037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute metrics\n",
    "#metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    probabilities= softmax(logits, axis=1)\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "        \n",
    "    return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            #\"Accuracy\": metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities,multi_class=\"ovr\"),  # AUC-ROC requires probabilities\n",
    "            \"Precision\": precision_score(labels, predictions,average=\"macro\"),\n",
    "            \"Recall\": recall_score(labels, predictions,average=\"macro\"),\n",
    "            \"F1-score\": f1_score(labels, predictions,average=\"macro\")\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f737c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Map your folder names to the base HuggingFace model names\n",
    "MODEL_NAME_MAP = {\n",
    "    \"DeepChemWL_Flavor_ChemBERTaWL_Flavor_5MWL_Flavor_MTR\": \"DeepChem/ChemBERTa-5M-MTR\",\n",
    "    \"DeepChemWL_Flavor_ChemBERTaWL_Flavor_10MWL_Flavor_MTR\": \"DeepChem/ChemBERTa-10M-MTR\",\n",
    "    \"DeepChemWL_Flavor_ChemBERTaWL_Flavor_77MWL_Flavor_MLM\": \"DeepChem/ChemBERTa-77M-MLM\",\n",
    "    \"DeepChemWL_Flavor_ChemBERTaWL_Flavor_10MWL_Flavor_MLM\": \"DeepChem/ChemBERTa-10M-MLM\",\n",
    "    \"DeepChemWL_Flavor_ChemBERTaWL_Flavor_77MWL_Flavor_MTR\": \"DeepChem/ChemBERTa-77M-MTR\",\n",
    "    \"ibmWL_Flavor_MoLFormerWL_Flavor_XLWL_Flavor_bothWL_Flavor_10pct\":\"ibm/MoLFormer-XL-both-10pct\"\n",
    "    }\n",
    "\n",
    "\n",
    "models_root_dir = \".../models_Flavor_WL\"\n",
    "\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"./test_results_flavor\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    report_to=\"none\",  # Disable logging to W&B for test\n",
    "    disable_tqdm=True \n",
    "\n",
    ")\n",
    "\n",
    "def find_all_peft_checkpoints(root_dir):\n",
    "    checkpoints = []\n",
    "    for model_folder in os.listdir(root_dir):\n",
    "        model_folder_path = os.path.join(root_dir, model_folder)\n",
    "        if not os.path.isdir(model_folder_path):\n",
    "            continue\n",
    "        for run_id in os.listdir(model_folder_path):\n",
    "            run_path = os.path.join(model_folder_path, run_id)\n",
    "            if not os.path.isdir(run_path):\n",
    "                continue\n",
    "            for subdir in os.listdir(run_path):\n",
    "                checkpoint_path = os.path.join(run_path, subdir)\n",
    "                if subdir.startswith(\"checkpoint-\") and os.path.exists(os.path.join(checkpoint_path, \"adapter_config.json\")):\n",
    "                    checkpoints.append((model_folder, run_id, checkpoint_path))\n",
    "    return checkpoints\n",
    "\n",
    "valid_checkpoints = find_all_peft_checkpoints(models_root_dir)\n",
    "print(f\"Found {len(valid_checkpoints)} valid checkpoints.\")\n",
    "\n",
    "for model_folder, run_id, checkpoint_path in valid_checkpoints:\n",
    "    print(\"Model folder: \",model_folder)\n",
    "\n",
    "    hf_model_name = MODEL_NAME_MAP[model_folder]\n",
    "    print(f\"Using base model: {hf_model_name}\")\n",
    "\n",
    "    \n",
    "    # Load tokenizer and base model for the model type\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(hf_model_name, trust_remote_code=True)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        hf_model_name,\n",
    "        num_labels=5,\n",
    "        problem_type=\"single_label_classification\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    from datasets import Dataset\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    test_dataset= pd.read_csv('.../fart_test.csv')\n",
    "    label_encoder = LabelEncoder() \n",
    "    test_dataset = Dataset.from_pandas(test_dataset)\n",
    "    def tokenize_function(examples):\n",
    "\n",
    "        return tokenizer(examples[\"Canonicalized SMILES\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    \n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "    encoded_labels = label_encoder.fit_transform(test_dataset['Canonicalized Taste'])\n",
    "    \n",
    "\n",
    "    test_dataset = test_dataset.add_column('label', encoded_labels) \n",
    "    columns_to_remove = [\"Canonicalized SMILES\", \"Standardized SMILES\", \n",
    "                        \"Canonicalized Taste\", \"Original Labels\", \"Source\", \"is_multiclass\"]\n",
    "    test_dataset = test_dataset.remove_columns(columns_to_remove) \n",
    "\n",
    "    \n",
    "\n",
    "    # Load the adapter checkpoint\n",
    "    adapter_model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    adapter_model.eval()\n",
    "\n",
    "    # Eval\n",
    "    from transformers import Trainer\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=adapter_model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    print(f\"\\n🔍 Evaluating {model_folder}/{run_id}/{os.path.basename(checkpoint_path)}\")\n",
    "    \n",
    "    test_results = trainer.evaluate()\n",
    "    print(f\"Test results: {test_results}\")\n",
    "   \n",
    "    \n",
    "   \n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313e8d00",
   "metadata": {},
   "source": [
    "## Load and Merge Base Model with LoRA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632155dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-77M-MLM\", #change model name as per your requirement\n",
    "    num_labels=5,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "adapter_model = PeftModel.from_pretrained(base_model, \".../DeepChem__ChemBERTa__77M__MLM/x38bwbvz/checkpoint-416\")\n",
    "\n",
    "final_model_clintox_molformer= adapter_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bcca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \".../weighted_loss_clin/final_model_chem_77M-MLM-WL\"\n",
    "\n",
    "final_model_clintox_molformer.save_pretrained(save_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
