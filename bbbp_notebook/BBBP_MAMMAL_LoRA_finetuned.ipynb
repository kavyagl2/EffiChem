{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701692e8",
   "metadata": {},
   "source": [
    "# Finetuning MAMMAL Model for BBBP Prediction Task with LoRA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4adbf430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "import evaluate\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9fd714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mammal.examples.molnet.molnet_infer import load_model, task_infer\n",
    "\n",
    "\n",
    "task_dict = load_model(task_name=\"BBBP\", device=\"cpu\")\n",
    "model = task_dict[\"model\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2048ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a35c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40795fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2037685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abccc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading datasets\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_bbbp=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Molformer_Finetuned/bbbp/train.csv')\n",
    "val_bbbp=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Molformer_Finetuned/bbbp/valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0900c9ef",
   "metadata": {},
   "source": [
    "Preparing Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bafcccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list= train_bbbp['smiles'].tolist()\n",
    "smiles_val=val_bbbp['smiles'].tolist()\n",
    "train_tokenized=tokenizer(smiles_list)\n",
    "val_tokenized=tokenizer(smiles_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b198c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "val_dataset = Dataset.from_dict(val_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68dbcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_bbbp['p_np'].tolist() # Assuming tasks start from column 1\n",
    "val_labels = val_bbbp['p_np'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e817f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "val_dataset = val_dataset.add_column(\"labels\", val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22c9a1",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning using WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05b892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21bedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from evaluate import load\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cbc39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load():\n",
    "    train_bbbp=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/bbbp/train.csv')\n",
    "    val_bbbp=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/bbbp/valid.csv')\n",
    "\n",
    "    return train_bbbp, val_bbbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data_process,tokenizer):\n",
    "\n",
    "    smiles_list = data_process['smiles'].tolist()\n",
    "    tokenized=tokenizer(smiles_list)\n",
    "    \n",
    "    \n",
    "    dataset = Dataset.from_dict(tokenized)\n",
    "    \n",
    "\n",
    "    labels = data_process['p_np'].tolist() # Assuming tasks start from column 1\n",
    "    \n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "    \n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf4185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "def lora_config(r,lora_alpha,dropout):\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",  # Sequence classification task\n",
    "        r=r,  # Rank of LoRA matrices\n",
    "        lora_alpha=lora_alpha,  # Scaling factor double of rank( from the rule of thumb)\n",
    "        target_modules='all-linear',\n",
    "        lora_dropout=dropout  # Dropout rate\n",
    "        #init_lora_weights=\"gaussian\"\n",
    "    )\n",
    "\n",
    "    return lora_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080f261",
   "metadata": {},
   "source": [
    "### Weighted Loss Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5147ecd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;32m----> 3\u001b[0m class_weights\u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m(\u001b[43mtrain_dataset\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])),\n",
      "\u001b[1;32m      4\u001b[0m                            \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m(train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]))]\n",
      "\u001b[1;32m      6\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(class_weights))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mWeightedLossTrainer\u001b[39;00m(Trainer):\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class_weights= [1-(train_dataset['labels'].count(0)/len(train_dataset['labels'])),\n",
    "                           1-(train_dataset['labels'].count(1)/len(train_dataset['labels']))]\n",
    "\n",
    "class_weights = torch.from_numpy(np.array(class_weights)).float().to(\"cuda\")\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Extract labels\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # compute custom loss (suppose one has 2 labels with different weights)\n",
    "        loss_func = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_func(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c952da0c",
   "metadata": {},
   "source": [
    "### Focal Loss Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#focal loss computation\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def focal_loss_multiclass(inputs, targets, alpha=1, gamma=2):\n",
    "    log_prob = F.log_softmax(inputs, dim=-1)\n",
    "    prob = torch.exp(log_prob)  # Convert log probabilities back to normal probabilities\n",
    "\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[-1])\n",
    "    pt = torch.sum(prob * targets_one_hot, dim=-1)  # Get probability of the true class\n",
    "\n",
    "    focal_loss = -alpha * (1 - pt) ** gamma * torch.sum(log_prob * targets_one_hot, dim=-1)\n",
    "    \n",
    "    return focal_loss.mean() \n",
    "\n",
    "\n",
    "class FocalLossTrainer(Trainer):\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = focal_loss_multiclass(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f563226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef\n",
    "\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "mcc_metric= load(\"matthews_correlation\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    probabilities = softmax(logits, axis=1)[:, 1]  # Get probabilities for class 1\n",
    "    predictions = np.argmax(logits, axis=1)  # Choose the most likely class\n",
    "    \n",
    "\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        \"eval_mcc_metric\": mcc,\n",
    "        \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"AUC-ROC\": roc_auc_score(labels, probabilities),  # AUC-ROC requires probabilities\n",
    "        \"Precision\": precision_score(labels, predictions),\n",
    "        \"Recall\": recall_score(labels, predictions),\n",
    "        \"F1-score\": f1_score(labels, predictions)\n",
    "    } \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d77bc",
   "metadata": {},
   "source": [
    "### Weighted Loss Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d90b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/BiomedSciAI/biomed-multi-alignment.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac9db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuse.data.tokenizers.modular_tokenizer.op import ModularTokenizerOp\n",
    "from mammal.model import Mammal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a64a0ef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in ibm/biomed.omics.bl.sm.ma-ted-458m. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_clin \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mibm/biomed.omics.bl.sm.ma-ted-458m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msingle_label_classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:531\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 531\u001b[0m config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs_orig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1133\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(pretrained_model_name_or_path):\n\u001b[1;32m   1131\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m CONFIG_MAPPING[pattern]\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m-> 1133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized model in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould have a `model_type` key in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, or contain one of the following strings \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min its name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(CONFIG_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1137\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized model in ibm/biomed.omics.bl.sm.ma-ted-458m. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth"
     ]
    }
   ],
   "source": [
    "model_clin = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"ibm/biomed.omics.bl.sm.ma-ted-458m\",\n",
    "        num_labels=2,\n",
    "        problem_type=\"single_label_classification\",    \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize W&B with sweep\n",
    "def run_training():\n",
    "    run = wandb.init(project=\"BBBP MAMMAL Hyperparameter Tuning\")\n",
    "    config = run.config   \n",
    "\n",
    "    # Define unique save path for each W&B run\n",
    "    save_dir = f\"./models_MAMMAL_BBBP/{wandb.run.id}\"  # Unique directory for each run\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    model_clin = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"ibm/biomed.omics.bl.sm.ma-ted-458m\",\n",
    "        num_labels=2,\n",
    "        problem_type=\"single_label_classification\",    \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = ModularTokenizerOp.from_pretrained(\"ibm/biomed.omics.bl.sm.ma-ted-458m\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Load data\n",
    "    train_data, val_data = data_load()\n",
    "    training_data = data_prep(train_data, tokenizer)\n",
    "    validation_data = data_prep(val_data, tokenizer)\n",
    "\n",
    "\n",
    "    # Apply LoRA\n",
    "    peft_config = lora_config(config.r, config.lora_alpha, config.dropout)\n",
    "    lora_model = get_peft_model(model_clin, peft_config)\n",
    "    \n",
    "    lora_model.print_trainable_parameters()\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=config.lr,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=20,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",  # Save model at each epoch\n",
    "        logging_dir=f\"./logs_BBBP/{wandb.run.id}\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        report_to=\"wandb\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_mcc_metric\"\n",
    "    )\n",
    "\n",
    "    accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = softmax(logits, axis=1)[:, 1]  \n",
    "        predictions = np.argmax(logits, axis=1)  \n",
    "        mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "        return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities),\n",
    "            \"Precision\": precision_score(labels, predictions),\n",
    "            \"Recall\": recall_score(labels, predictions),\n",
    "            \"F1-score\": f1_score(labels, predictions)\n",
    "        }\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=training_data,\n",
    "        eval_dataset=validation_data,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save model and tokenizer for this run\n",
    "    trainer.save_model(save_dir)\n",
    "    \n",
    "    print(f\"Model saved to {save_dir}\")\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c60f204",
   "metadata": {},
   "source": [
    "### Focal loss trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c400f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize W&B with sweep\n",
    "def run_training():\n",
    "    run = wandb.init(project=\"BBBP focal loss Hyperparameter Tuning\")\n",
    "    config = run.config   \n",
    "\n",
    "    # Define unique save path for each W&B run\n",
    "    save_dir = f\"./models_BBBP_chemberta_focal_loss/{wandb.run.id}\"  # Unique directory for each run\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-77M-MTR\",                                 # Define any other model here\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Load data\n",
    "    train_data, val_data = data_load()\n",
    "    training_data = data_prep(train_data, tokenizer)\n",
    "    validation_data = data_prep(val_data, tokenizer)\n",
    "\n",
    "\n",
    "    # Apply LoRA\n",
    "    peft_config = lora_config(config.r, config.lora_alpha, config.dropout)\n",
    "    lora_model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    lora_model.print_trainable_parameters()\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=config.lr,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=20,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",  # Save model at each epoch\n",
    "        logging_dir=f\"./logs_BBBP/{wandb.run.id}\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        report_to=\"wandb\",\n",
    "        save_total_limit=10,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_mcc_metric\"\n",
    "    )\n",
    "\n",
    "    accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = softmax(logits, axis=1)[:, 1]  \n",
    "        predictions = np.argmax(logits, axis=1)  \n",
    "        mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "        return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities),\n",
    "            \"Precision\": precision_score(labels, predictions),\n",
    "            \"Recall\": recall_score(labels, predictions),\n",
    "            \"F1-score\": f1_score(labels, predictions)\n",
    "        }\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = FocalLossTrainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=training_data,\n",
    "        eval_dataset=validation_data,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save model and tokenizer for this run\n",
    "    trainer.save_model(save_dir)\n",
    "    \n",
    "    print(f\"Model saved to {save_dir}\")\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a016913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: s602kzmr\n",
      "Sweep URL: https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y1eq8vgp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.3111835528238495e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'BBBP focal loss Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/wandb/run-20250409_091043-y1eq8vgp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/y1eq8vgp' target=\"_blank\">revived-sweep-1</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/y1eq8vgp' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/y1eq8vgp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,127,234 || all params: 5,703,156 || trainable%: 37.2992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1781731/2934716305.py:68: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1040' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1040/1040 00:57, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.158149</td>\n",
       "      <td>0.136888</td>\n",
       "      <td>0.687805</td>\n",
       "      <td>0.774943</td>\n",
       "      <td>0.734104</td>\n",
       "      <td>0.875862</td>\n",
       "      <td>0.798742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>0.140867</td>\n",
       "      <td>0.332410</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.849195</td>\n",
       "      <td>0.751323</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.850299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.157900</td>\n",
       "      <td>0.128012</td>\n",
       "      <td>0.332410</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.871724</td>\n",
       "      <td>0.751323</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.850299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.118547</td>\n",
       "      <td>0.368660</td>\n",
       "      <td>0.765854</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.759358</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.855422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.111764</td>\n",
       "      <td>0.419110</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.895632</td>\n",
       "      <td>0.771739</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.863222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.106825</td>\n",
       "      <td>0.435092</td>\n",
       "      <td>0.785366</td>\n",
       "      <td>0.903908</td>\n",
       "      <td>0.775956</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.865854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>0.466037</td>\n",
       "      <td>0.795122</td>\n",
       "      <td>0.908621</td>\n",
       "      <td>0.784530</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.871166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.100909</td>\n",
       "      <td>0.481062</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.912069</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.873846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.098892</td>\n",
       "      <td>0.510343</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.915517</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.879257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>0.097504</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.917816</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.082600</td>\n",
       "      <td>0.096671</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.918966</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.095897</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.920230</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>0.095580</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.920230</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.095185</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.920230</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.094856</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.920115</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>0.094813</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.919655</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>0.094639</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.919770</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.094642</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.919310</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.094582</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.919770</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.071400</td>\n",
       "      <td>0.094561</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.919770</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_BBBP_chemberta_focal_loss/y1eq8vgp\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▆▆▇▇▇█████████████</td></tr><tr><td>eval/Accuracy</td><td>▁▅▅▅▆▆▇▇████████████</td></tr><tr><td>eval/F1-score</td><td>▁▅▅▆▆▇▇▇████████████</td></tr><tr><td>eval/Precision</td><td>▁▃▃▄▅▅▆▇████████████</td></tr><tr><td>eval/Recall</td><td>▁███████████████████</td></tr><tr><td>eval/loss</td><td>█▆▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▅▅▅▆▆▇▇████████████</td></tr><tr><td>eval/runtime</td><td>▅▃▁▂▂▄▂█▃▃▄▃▆▅▅▆▃▅▅▅</td></tr><tr><td>eval/samples_per_second</td><td>▄▆█▇▇▅▇▁▆▆▅▆▃▄▄▃▆▄▄▄</td></tr><tr><td>eval/steps_per_second</td><td>▄▆█▇▇▅▇▁▆▆▅▆▃▄▄▃▆▄▄▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>██▃▂▂▃▂▄▁▃</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▅▄▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.91977</td></tr><tr><td>eval/Accuracy</td><td>0.81463</td></tr><tr><td>eval/F1-score</td><td>0.88199</td></tr><tr><td>eval/Precision</td><td>0.80226</td></tr><tr><td>eval/Recall</td><td>0.97931</td></tr><tr><td>eval/loss</td><td>0.09456</td></tr><tr><td>eval/mcc_metric</td><td>0.52464</td></tr><tr><td>eval/runtime</td><td>0.1359</td></tr><tr><td>eval/samples_per_second</td><td>1508.322</td></tr><tr><td>eval/steps_per_second</td><td>51.504</td></tr><tr><td>total_flos</td><td>109776564695448.0</td></tr><tr><td>train/epoch</td><td>20</td></tr><tr><td>train/global_step</td><td>1040</td></tr><tr><td>train/grad_norm</td><td>0.15653</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0714</td></tr><tr><td>train_loss</td><td>0.09093</td></tr><tr><td>train_runtime</td><td>57.6126</td></tr><tr><td>train_samples_per_second</td><td>568.973</td></tr><tr><td>train_steps_per_second</td><td>18.052</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">revived-sweep-1</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/y1eq8vgp' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/y1eq8vgp</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250409_091043-y1eq8vgp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 848zobqw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.4126982649556346e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'BBBP focal loss Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/wandb/run-20250409_091151-848zobqw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/848zobqw' target=\"_blank\">summer-sweep-2</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/848zobqw' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/848zobqw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 272,274 || all params: 3,709,716 || trainable%: 7.3395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1781731/2934716305.py:68: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1040' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1040/1040 00:53, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.152883</td>\n",
       "      <td>0.140559</td>\n",
       "      <td>0.712195</td>\n",
       "      <td>0.797931</td>\n",
       "      <td>0.723958</td>\n",
       "      <td>0.958621</td>\n",
       "      <td>0.824926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.152200</td>\n",
       "      <td>0.130585</td>\n",
       "      <td>0.275018</td>\n",
       "      <td>0.741463</td>\n",
       "      <td>0.860115</td>\n",
       "      <td>0.737113</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.843658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.152200</td>\n",
       "      <td>0.114726</td>\n",
       "      <td>0.378375</td>\n",
       "      <td>0.765854</td>\n",
       "      <td>0.884253</td>\n",
       "      <td>0.753927</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.106197</td>\n",
       "      <td>0.368660</td>\n",
       "      <td>0.765854</td>\n",
       "      <td>0.896667</td>\n",
       "      <td>0.759358</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.855422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.101417</td>\n",
       "      <td>0.435092</td>\n",
       "      <td>0.785366</td>\n",
       "      <td>0.905632</td>\n",
       "      <td>0.775956</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.865854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>0.098108</td>\n",
       "      <td>0.495823</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.911839</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.876543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>0.096143</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.914138</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.095177</td>\n",
       "      <td>0.507977</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.916092</td>\n",
       "      <td>0.801136</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.878505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.094416</td>\n",
       "      <td>0.522319</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.917586</td>\n",
       "      <td>0.805714</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.881250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.093910</td>\n",
       "      <td>0.536466</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.918736</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.884013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.094631</td>\n",
       "      <td>0.550435</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.918851</td>\n",
       "      <td>0.815029</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.886792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.094858</td>\n",
       "      <td>0.564238</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.819767</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.889590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.095334</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.920115</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.892405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.061900</td>\n",
       "      <td>0.095869</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.919885</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.892405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.061900</td>\n",
       "      <td>0.096323</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.892405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>0.097406</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.919770</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.892405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.058500</td>\n",
       "      <td>0.097176</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.892405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.097547</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.919885</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.892405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.097647</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.919885</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.892405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.097624</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.919885</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.892405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_BBBP_chemberta_focal_loss/848zobqw\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▆▇▇███████████████</td></tr><tr><td>eval/Accuracy</td><td>▁▃▄▄▅▆▇▇▇▇▇█████████</td></tr><tr><td>eval/F1-score</td><td>▁▃▄▄▅▆▇▇▇▇▇█████████</td></tr><tr><td>eval/Precision</td><td>▁▂▃▃▅▆▆▆▇▇▇█████████</td></tr><tr><td>eval/Recall</td><td>▁▇█▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄</td></tr><tr><td>eval/loss</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▃▅▅▆▇▇▇▇▇██████████</td></tr><tr><td>eval/runtime</td><td>▄▁▂▁▁▂▂▂▁█▃▂▂▂▄▃▂▂▄▄</td></tr><tr><td>eval/samples_per_second</td><td>▅█▇██▇▇▇█▁▆▇▇▇▅▆▇▇▅▅</td></tr><tr><td>eval/steps_per_second</td><td>▅█▇██▇▇▇█▁▆▇▇▇▅▆▇▇▅▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▄▂▁▂▂▂▅▆▁█</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.91989</td></tr><tr><td>eval/Accuracy</td><td>0.83415</td></tr><tr><td>eval/F1-score</td><td>0.89241</td></tr><tr><td>eval/Precision</td><td>0.82456</td></tr><tr><td>eval/Recall</td><td>0.97241</td></tr><tr><td>eval/loss</td><td>0.09762</td></tr><tr><td>eval/mcc_metric</td><td>0.57789</td></tr><tr><td>eval/runtime</td><td>0.1352</td></tr><tr><td>eval/samples_per_second</td><td>1516.665</td></tr><tr><td>eval/steps_per_second</td><td>51.789</td></tr><tr><td>total_flos</td><td>68288585081688.0</td></tr><tr><td>train/epoch</td><td>20</td></tr><tr><td>train/global_step</td><td>1040</td></tr><tr><td>train/grad_norm</td><td>0.44962</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0593</td></tr><tr><td>train_loss</td><td>0.07704</td></tr><tr><td>train_runtime</td><td>54.0327</td></tr><tr><td>train_samples_per_second</td><td>606.669</td></tr><tr><td>train_steps_per_second</td><td>19.248</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-sweep-2</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/848zobqw' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/848zobqw</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250409_091151-848zobqw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e9i75nlt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.5683775139356563e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'BBBP focal loss Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/wandb/run-20250409_091303-e9i75nlt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/e9i75nlt' target=\"_blank\">eager-sweep-3</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/e9i75nlt' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/e9i75nlt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 395,938 || all params: 3,842,612 || trainable%: 10.3039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1781731/2934716305.py:68: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1040' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1040/1040 00:54, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.152845</td>\n",
       "      <td>0.232612</td>\n",
       "      <td>0.726829</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.745856</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.828221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.152200</td>\n",
       "      <td>0.132010</td>\n",
       "      <td>0.293370</td>\n",
       "      <td>0.746341</td>\n",
       "      <td>0.867126</td>\n",
       "      <td>0.743455</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.845238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.152200</td>\n",
       "      <td>0.117074</td>\n",
       "      <td>0.385941</td>\n",
       "      <td>0.770732</td>\n",
       "      <td>0.888046</td>\n",
       "      <td>0.763441</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.858006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>0.107829</td>\n",
       "      <td>0.419110</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.901494</td>\n",
       "      <td>0.771739</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.863222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.108400</td>\n",
       "      <td>0.102538</td>\n",
       "      <td>0.450723</td>\n",
       "      <td>0.790244</td>\n",
       "      <td>0.910230</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.868502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.099228</td>\n",
       "      <td>0.510343</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.915402</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.879257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.097153</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.916897</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.095830</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.919770</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.094464</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.920690</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.093912</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.921839</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.093931</td>\n",
       "      <td>0.538740</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.922299</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.884735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.093876</td>\n",
       "      <td>0.552651</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.922874</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.094215</td>\n",
       "      <td>0.552651</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.922069</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.065100</td>\n",
       "      <td>0.094352</td>\n",
       "      <td>0.552651</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.922069</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.065100</td>\n",
       "      <td>0.094452</td>\n",
       "      <td>0.552651</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.922069</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.095040</td>\n",
       "      <td>0.552651</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.921609</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.095044</td>\n",
       "      <td>0.552651</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.921724</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.060600</td>\n",
       "      <td>0.095271</td>\n",
       "      <td>0.552651</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.921724</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.060600</td>\n",
       "      <td>0.095262</td>\n",
       "      <td>0.552651</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.921839</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.095274</td>\n",
       "      <td>0.552651</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.921724</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_BBBP_chemberta_focal_loss/e9i75nlt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▆▇▇███████████████</td></tr><tr><td>eval/Accuracy</td><td>▁▂▄▅▆▇▇▇▇▇██████████</td></tr><tr><td>eval/F1-score</td><td>▁▃▅▅▆▇▇▇▇▇██████████</td></tr><tr><td>eval/Precision</td><td>▁▁▃▄▅▇▇▇▇▇██████████</td></tr><tr><td>eval/Recall</td><td>▁███████████████████</td></tr><tr><td>eval/loss</td><td>█▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▂▄▅▆▇▇▇▇▇██████████</td></tr><tr><td>eval/runtime</td><td>▃▁▂▅▃█▃▃▅▃▄▃▃▆▅▆▆▄▆▄</td></tr><tr><td>eval/samples_per_second</td><td>▆█▇▄▆▁▆▆▄▆▅▆▆▃▄▃▃▅▃▅</td></tr><tr><td>eval/steps_per_second</td><td>▆█▇▄▆▁▆▆▄▆▅▆▆▃▄▃▃▅▃▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▇▅▂▃▃▃▂█▁▅</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▅▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.92172</td></tr><tr><td>eval/Accuracy</td><td>0.82439</td></tr><tr><td>eval/F1-score</td><td>0.8875</td></tr><tr><td>eval/Precision</td><td>0.81143</td></tr><tr><td>eval/Recall</td><td>0.97931</td></tr><tr><td>eval/loss</td><td>0.09527</td></tr><tr><td>eval/mcc_metric</td><td>0.55265</td></tr><tr><td>eval/runtime</td><td>0.1332</td></tr><tr><td>eval/samples_per_second</td><td>1538.949</td></tr><tr><td>eval/steps_per_second</td><td>52.549</td></tr><tr><td>total_flos</td><td>71054450389272.0</td></tr><tr><td>train/epoch</td><td>20</td></tr><tr><td>train/global_step</td><td>1040</td></tr><tr><td>train/grad_norm</td><td>0.22217</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0625</td></tr><tr><td>train_loss</td><td>0.08017</td></tr><tr><td>train_runtime</td><td>54.4337</td></tr><tr><td>train_samples_per_second</td><td>602.201</td></tr><tr><td>train_steps_per_second</td><td>19.106</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-sweep-3</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/e9i75nlt' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/e9i75nlt</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250409_091303-e9i75nlt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j620qhlv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.452965561301102e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'BBBP focal loss Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/wandb/run-20250409_091415-j620qhlv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/j620qhlv' target=\"_blank\">solar-sweep-4</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/j620qhlv' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/j620qhlv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 395,938 || all params: 3,842,612 || trainable%: 10.3039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1781731/2934716305.py:68: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1040' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1040/1040 00:53, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.152278</td>\n",
       "      <td>0.165863</td>\n",
       "      <td>0.717073</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.727749</td>\n",
       "      <td>0.958621</td>\n",
       "      <td>0.827381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.151300</td>\n",
       "      <td>0.129289</td>\n",
       "      <td>0.250625</td>\n",
       "      <td>0.736585</td>\n",
       "      <td>0.874828</td>\n",
       "      <td>0.735751</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.840237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.151300</td>\n",
       "      <td>0.113121</td>\n",
       "      <td>0.332410</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.900115</td>\n",
       "      <td>0.751323</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.850299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.102700</td>\n",
       "      <td>0.104636</td>\n",
       "      <td>0.419110</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.913103</td>\n",
       "      <td>0.771739</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.863222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.102700</td>\n",
       "      <td>0.099745</td>\n",
       "      <td>0.510343</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.916782</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.879257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.082400</td>\n",
       "      <td>0.096492</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.920230</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.082400</td>\n",
       "      <td>0.094887</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.920460</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.093903</td>\n",
       "      <td>0.538740</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.921724</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.884735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.069500</td>\n",
       "      <td>0.093512</td>\n",
       "      <td>0.552651</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.922529</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.065100</td>\n",
       "      <td>0.093567</td>\n",
       "      <td>0.536466</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.922759</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.884013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.065100</td>\n",
       "      <td>0.094346</td>\n",
       "      <td>0.550435</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.921494</td>\n",
       "      <td>0.815029</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.886792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.061700</td>\n",
       "      <td>0.094803</td>\n",
       "      <td>0.564238</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.920805</td>\n",
       "      <td>0.819767</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.889590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.061700</td>\n",
       "      <td>0.095406</td>\n",
       "      <td>0.564238</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.920690</td>\n",
       "      <td>0.819767</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.889590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.096035</td>\n",
       "      <td>0.564238</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.920690</td>\n",
       "      <td>0.819767</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.889590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.096633</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.920690</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.892405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.097939</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.892405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.097756</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.920115</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.892405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.098100</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.920230</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.892405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.098173</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.920345</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.892405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.098160</td>\n",
       "      <td>0.577889</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.920345</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.892405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_BBBP_chemberta_focal_loss/j620qhlv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▇▇████████████████</td></tr><tr><td>eval/Accuracy</td><td>▁▂▃▅▇▇▇▇▇▇▇█████████</td></tr><tr><td>eval/F1-score</td><td>▁▂▃▅▇▇▇▇▇▇▇█████████</td></tr><tr><td>eval/Precision</td><td>▁▂▃▄▆▆▆▇▇▇▇█████████</td></tr><tr><td>eval/Recall</td><td>▁████████▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>eval/loss</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂</td></tr><tr><td>eval/mcc_metric</td><td>▁▂▄▅▇▇▇▇█▇██████████</td></tr><tr><td>eval/runtime</td><td>▄▂▁▁▁▃▂█▂▂▂▂▅▃▂▂▃▄▃▂</td></tr><tr><td>eval/samples_per_second</td><td>▄▇███▆▇▁▇▇▇▇▃▆▇▇▆▅▆▇</td></tr><tr><td>eval/steps_per_second</td><td>▄▇███▆▇▁▇▇▇▇▃▆▇▇▆▅▆▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▇▂▂▅▃▂▄█▁█</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.92034</td></tr><tr><td>eval/Accuracy</td><td>0.83415</td></tr><tr><td>eval/F1-score</td><td>0.89241</td></tr><tr><td>eval/Precision</td><td>0.82456</td></tr><tr><td>eval/Recall</td><td>0.97241</td></tr><tr><td>eval/loss</td><td>0.09816</td></tr><tr><td>eval/mcc_metric</td><td>0.57789</td></tr><tr><td>eval/runtime</td><td>0.1336</td></tr><tr><td>eval/samples_per_second</td><td>1534.352</td></tr><tr><td>eval/steps_per_second</td><td>52.392</td></tr><tr><td>total_flos</td><td>71054450389272.0</td></tr><tr><td>train/epoch</td><td>20</td></tr><tr><td>train/global_step</td><td>1040</td></tr><tr><td>train/grad_norm</td><td>0.34629</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0591</td></tr><tr><td>train_loss</td><td>0.07615</td></tr><tr><td>train_runtime</td><td>53.1524</td></tr><tr><td>train_samples_per_second</td><td>616.718</td></tr><tr><td>train_steps_per_second</td><td>19.566</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-sweep-4</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/j620qhlv' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/j620qhlv</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250409_091415-j620qhlv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 082b48rt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.536971125513578e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'BBBP focal loss Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/wandb/run-20250409_091518-082b48rt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/082b48rt' target=\"_blank\">cool-sweep-5</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/082b48rt' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/082b48rt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 395,938 || all params: 3,842,612 || trainable%: 10.3039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1781731/2934716305.py:68: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1040' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1040/1040 00:55, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.145412</td>\n",
       "      <td>0.202546</td>\n",
       "      <td>0.726829</td>\n",
       "      <td>0.839310</td>\n",
       "      <td>0.725888</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.836257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>0.117272</td>\n",
       "      <td>0.176133</td>\n",
       "      <td>0.721951</td>\n",
       "      <td>0.897356</td>\n",
       "      <td>0.724490</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.832845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.143100</td>\n",
       "      <td>0.104628</td>\n",
       "      <td>0.385941</td>\n",
       "      <td>0.770732</td>\n",
       "      <td>0.918621</td>\n",
       "      <td>0.763441</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.858006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.087700</td>\n",
       "      <td>0.096587</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.924138</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.087700</td>\n",
       "      <td>0.092497</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.923563</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.092323</td>\n",
       "      <td>0.564238</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.921379</td>\n",
       "      <td>0.819767</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.889590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.094237</td>\n",
       "      <td>0.591400</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.920345</td>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.895238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.094812</td>\n",
       "      <td>0.591400</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.920115</td>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.895238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.096470</td>\n",
       "      <td>0.591400</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.921034</td>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.895238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>0.097484</td>\n",
       "      <td>0.618043</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.921839</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.900958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>0.098689</td>\n",
       "      <td>0.631194</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.921954</td>\n",
       "      <td>0.844311</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.903846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.098648</td>\n",
       "      <td>0.631194</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.922529</td>\n",
       "      <td>0.844311</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.903846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.098721</td>\n",
       "      <td>0.631194</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.922414</td>\n",
       "      <td>0.844311</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.903846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.099300</td>\n",
       "      <td>0.616929</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.923218</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.900322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.099783</td>\n",
       "      <td>0.603093</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.923908</td>\n",
       "      <td>0.842424</td>\n",
       "      <td>0.958621</td>\n",
       "      <td>0.896774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.101660</td>\n",
       "      <td>0.603093</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.923793</td>\n",
       "      <td>0.842424</td>\n",
       "      <td>0.958621</td>\n",
       "      <td>0.896774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.100867</td>\n",
       "      <td>0.603093</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.924368</td>\n",
       "      <td>0.842424</td>\n",
       "      <td>0.958621</td>\n",
       "      <td>0.896774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.101071</td>\n",
       "      <td>0.603093</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.924598</td>\n",
       "      <td>0.842424</td>\n",
       "      <td>0.958621</td>\n",
       "      <td>0.896774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.101058</td>\n",
       "      <td>0.603093</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.924828</td>\n",
       "      <td>0.842424</td>\n",
       "      <td>0.958621</td>\n",
       "      <td>0.896774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.101028</td>\n",
       "      <td>0.603093</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.924828</td>\n",
       "      <td>0.842424</td>\n",
       "      <td>0.958621</td>\n",
       "      <td>0.896774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_BBBP_chemberta_focal_loss/082b48rt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▆▇█████████████████</td></tr><tr><td>eval/Accuracy</td><td>▁▁▄▆▆▇▇▇▇█████▇▇▇▇▇▇</td></tr><tr><td>eval/F1-score</td><td>▁▁▃▆▆▇▇▇▇█████▇▇▇▇▇▇</td></tr><tr><td>eval/Precision</td><td>▁▁▃▆▆▇▇▇▇███████████</td></tr><tr><td>eval/Recall</td><td>█▆▆▆▆▅▅▅▅▅▅▅▅▃▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>█▄▃▂▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>eval/mcc_metric</td><td>▁▁▄▆▆▇▇▇▇███████████</td></tr><tr><td>eval/runtime</td><td>▂▁▂▃▂▂▂▃█▂▂▃▂▃▂▃▂▄▄▃</td></tr><tr><td>eval/samples_per_second</td><td>▇█▇▆▇▇▇▆▁▇▇▆▆▆▆▆▇▅▅▆</td></tr><tr><td>eval/steps_per_second</td><td>▇█▇▆▇▇▇▆▁▇▇▆▆▆▆▆▇▅▅▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▄▁▂▆▃▄▆▇▂█</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.92483</td></tr><tr><td>eval/Accuracy</td><td>0.8439</td></tr><tr><td>eval/F1-score</td><td>0.89677</td></tr><tr><td>eval/Precision</td><td>0.84242</td></tr><tr><td>eval/Recall</td><td>0.95862</td></tr><tr><td>eval/loss</td><td>0.10103</td></tr><tr><td>eval/mcc_metric</td><td>0.60309</td></tr><tr><td>eval/runtime</td><td>0.1367</td></tr><tr><td>eval/samples_per_second</td><td>1500.169</td></tr><tr><td>eval/steps_per_second</td><td>51.225</td></tr><tr><td>total_flos</td><td>71054450389272.0</td></tr><tr><td>train/epoch</td><td>20</td></tr><tr><td>train/global_step</td><td>1040</td></tr><tr><td>train/grad_norm</td><td>0.65713</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0554</td></tr><tr><td>train_loss</td><td>0.06905</td></tr><tr><td>train_runtime</td><td>55.3625</td></tr><tr><td>train_samples_per_second</td><td>592.098</td></tr><tr><td>train_steps_per_second</td><td>18.785</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cool-sweep-5</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/082b48rt' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/082b48rt</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250409_091518-082b48rt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p4hh23r5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.8067884774620412e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'BBBP focal loss Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/wandb/run-20250409_091626-p4hh23r5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/p4hh23r5' target=\"_blank\">denim-sweep-6</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/s602kzmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/p4hh23r5' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/p4hh23r5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 395,938 || all params: 3,842,612 || trainable%: 10.3039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1781731/2934716305.py:68: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='503' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 503/1040 00:26 < 00:28, 19.01 it/s, Epoch 9.65/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.139675</td>\n",
       "      <td>0.174226</td>\n",
       "      <td>0.721951</td>\n",
       "      <td>0.858161</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.833819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.111588</td>\n",
       "      <td>0.227441</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.907126</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.837758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.100113</td>\n",
       "      <td>0.510343</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.922874</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.879257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.081200</td>\n",
       "      <td>0.092847</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.924828</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.081200</td>\n",
       "      <td>0.091928</td>\n",
       "      <td>0.564238</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.923218</td>\n",
       "      <td>0.819767</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.889590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.094641</td>\n",
       "      <td>0.591400</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.918966</td>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.895238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.097466</td>\n",
       "      <td>0.591400</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.920115</td>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.895238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.097571</td>\n",
       "      <td>0.618043</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.920690</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.900958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.099037</td>\n",
       "      <td>0.631194</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.921724</td>\n",
       "      <td>0.844311</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.903846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    sweep_config = {\n",
    "    \"name\": \"BBBP Chemberta Hyperparameter Tuning\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"goal\": \"maximize\", \n",
    "        \"name\": \"eval/mcc_metric\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 1e-5,  \n",
    "            \"max\": 2e-5\n",
    "        },\n",
    "        \"r\": {\n",
    "            \"values\": [8,16,32,64, 128]\n",
    "        },\n",
    "        \"lora_alpha\": {\n",
    "            \"values\": [8,16,32,64,128]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"values\": [0.0,0.1,0.2]\n",
    "        },\n",
    "        \n",
    "        \"optimizer\": {\n",
    "            \"value\": [\"adamw\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"huggingface\")\n",
    "    wandb.agent(sweep_id, function=run_training, count=10)\n",
    "\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(f\"huggingface/{sweep_id}\")\n",
    "    print(sweep.runs[0].summary_metrics)\n",
    "\n",
    "    runs_with_rmse = [run for run in sweep.runs if 'eval/mcc_metric' in run.summary_metrics]\n",
    "    if runs_with_rmse:\n",
    "        # Sort by rmse in descending order (maximize)\n",
    "        best_run = sorted(runs_with_rmse, key=lambda run: run.summary_metrics['eval/mcc_metric'])[0]\n",
    "    else:\n",
    "        raise ValueError(\"No runs found with 'eval/mcc_metric' metric.\")\n",
    "\n",
    "    best_hyperparameters = best_run.config\n",
    "    print(f\"Best hyperparameters: {best_hyperparameters}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582853ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Molformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
