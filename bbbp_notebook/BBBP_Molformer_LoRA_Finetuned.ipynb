{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b466aaec",
   "metadata": {},
   "source": [
    "## BBBP Prediction Task Using LoRA Finetuned Molformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6dd442",
   "metadata": {},
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a83b4d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_bbbp=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Molformer_Finetuned/bbbp/train.csv')\n",
    "val_bbbp=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Molformer_Finetuned/bbbp/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1fd3993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>name</th>\n",
       "      <th>p_np</th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Propanolol</td>\n",
       "      <td>1</td>\n",
       "      <td>[Cl].CC(C)NCC(O)COc1cccc2ccccc12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Terbutylchlorambucil</td>\n",
       "      <td>1</td>\n",
       "      <td>C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>40730</td>\n",
       "      <td>1</td>\n",
       "      <td>c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>cefoperazone</td>\n",
       "      <td>1</td>\n",
       "      <td>CCN1CCN(C(=O)N[C@@H](C(=O)N[C@H]2[C@H]3SCC(=C(...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num                  name  p_np  \\\n",
       "0    1            Propanolol     1   \n",
       "1    2  Terbutylchlorambucil     1   \n",
       "2    3                 40730     1   \n",
       "3    4                    24     1   \n",
       "4    6          cefoperazone     1   \n",
       "\n",
       "                                              smiles  \n",
       "0                   [Cl].CC(C)NCC(O)COc1cccc2ccccc12  \n",
       "1           C(=O)(OC(C)(C)C)CCCc1ccc(cc1)N(CCCl)CCCl  \n",
       "2  c12c3c(N4CCN(C)CC4)c(F)cc1c(c(C(O)=O)cn2C(C)CO...  \n",
       "3                   C1CCN(CC1)Cc1cccc(c1)OCCCNC(=O)C  \n",
       "4  CCN1CCN(C(=O)N[C@@H](C(=O)N[C@H]2[C@H]3SCC(=C(...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bbbp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ddd123b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per label:\n",
      " num       0\n",
      "name      0\n",
      "p_np      0\n",
      "smiles    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = train_bbbp.isnull().sum()\n",
    "\n",
    "print(\"Missing values per label:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a105a53a",
   "metadata": {},
   "source": [
    "No missing values present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f88b9d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p_np\n",
       "1    0.787675\n",
       "0    0.212325\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bbbp['p_np'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4919f633",
   "metadata": {},
   "source": [
    "We have class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef47dad6",
   "metadata": {},
   "source": [
    "Load tokenizer and Classsification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd62fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"ibm/MoLFormer-XL-both-10pct\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load the model with a classification head\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"ibm/MoLFormer-XL-both-10pct\",\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35877536",
   "metadata": {},
   "source": [
    "Preparing Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "606db441",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list= train_bbbp['smiles'].tolist()\n",
    "smiles_val=val_bbbp['smiles'].tolist()\n",
    "train_tokenized=tokenizer(smiles_list)\n",
    "val_tokenized=tokenizer(smiles_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cead36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "val_dataset = Dataset.from_dict(val_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "737f1276",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_bbbp['p_np'].tolist() # Assuming tasks start from column 1\n",
    "val_labels = val_bbbp['p_np'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0deae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "val_dataset = val_dataset.add_column(\"labels\", val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4727a",
   "metadata": {},
   "source": [
    "LoRA Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec5d879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from evaluate import load\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bce51d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load():\n",
    "    train_bbbp=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Molformer_Finetuned/bbbp/train.csv')\n",
    "    val_bbbp=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Molformer_Finetuned/bbbp/valid.csv')\n",
    "\n",
    "    return train_bbbp, val_bbbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68183f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data_process,tokenizer):\n",
    "\n",
    "    smiles_list = data_process['smiles'].tolist()\n",
    "    tokenized=tokenizer(smiles_list)\n",
    "    \n",
    "    \n",
    "    dataset = Dataset.from_dict(tokenized)\n",
    "    \n",
    "\n",
    "    labels = data_process['p_np'].tolist() # Assuming tasks start from column 1\n",
    "    \n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "    \n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7061e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "def lora_config(r,lora_alpha,dropout):\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",  # Sequence classification task\n",
    "        r=r,  # Rank of LoRA matrices\n",
    "        lora_alpha=lora_alpha,  # Scaling factor double of rank( from the rule of thumb)\n",
    "        target_modules='all-linear',\n",
    "        lora_dropout=dropout  # Dropout rate\n",
    "        #init_lora_weights=\"gaussian\"\n",
    "    )\n",
    "\n",
    "    return lora_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5557897a",
   "metadata": {},
   "source": [
    "### Weighted Loss Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a5fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class_weights= [1-(train_dataset['labels'].count(0)/len(train_dataset['labels'])),\n",
    "                           1-(train_dataset['labels'].count(1)/len(train_dataset['labels']))]\n",
    "\n",
    "class_weights = torch.from_numpy(np.array(class_weights)).float().to(\"cuda\")\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Extract labels\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # compute custom loss (suppose one has 2 labels with different weights)\n",
    "        loss_func = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_func(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d76305",
   "metadata": {},
   "source": [
    "### Focal Loss Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34454273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#focal loss computation\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def focal_loss_multiclass(inputs, targets, alpha=1, gamma=2):\n",
    "    log_prob = F.log_softmax(inputs, dim=-1)\n",
    "    prob = torch.exp(log_prob)  # Convert log probabilities back to normal probabilities\n",
    "\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[-1])\n",
    "    pt = torch.sum(prob * targets_one_hot, dim=-1)  # Get probability of the true class\n",
    "\n",
    "    focal_loss = -alpha * (1 - pt) ** gamma * torch.sum(log_prob * targets_one_hot, dim=-1)\n",
    "    \n",
    "    return focal_loss.mean() \n",
    "\n",
    "\n",
    "class FocalLossTrainer(Trainer):\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = focal_loss_multiclass(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "229f7c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef\n",
    "\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "mcc_metric= load(\"matthews_correlation\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    probabilities = softmax(logits, axis=1)[:, 1]  # Get probabilities for class 1\n",
    "    predictions = np.argmax(logits, axis=1)  # Choose the most likely class\n",
    "    \n",
    "\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        \"eval_mcc_metric\": mcc,\n",
    "        \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"AUC-ROC\": roc_auc_score(labels, probabilities),  # AUC-ROC requires probabilities\n",
    "        \"Precision\": precision_score(labels, predictions),\n",
    "        \"Recall\": recall_score(labels, predictions),\n",
    "        \"F1-score\": f1_score(labels, predictions)\n",
    "    } \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc85a7",
   "metadata": {},
   "source": [
    "### Weighted Loss Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cff33dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize W&B with sweep\n",
    "def run_training():\n",
    "    run = wandb.init(project=\"BBBP Hyperparameter Tuning\")\n",
    "    config = run.config   \n",
    "\n",
    "    # Define unique save path for each W&B run\n",
    "    save_dir = f\"./models_BBBP/{wandb.run.id}\"  # Unique directory for each run\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"ibm/MoLFormer-XL-both-10pct\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Load data\n",
    "    train_data, val_data = data_load()\n",
    "    training_data = data_prep(train_data, tokenizer)\n",
    "    validation_data = data_prep(val_data, tokenizer)\n",
    "\n",
    "    # Load base model\n",
    "    model_clin = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"ibm/MoLFormer-XL-both-10pct\",\n",
    "        num_labels=2,\n",
    "        problem_type=\"single_label_classification\",    \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Apply LoRA\n",
    "    peft_config = lora_config(config.r, config.lora_alpha, config.dropout)\n",
    "    lora_model = get_peft_model(model_clin, peft_config)\n",
    "    \n",
    "    lora_model.print_trainable_parameters()\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=config.lr,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=20,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",  # Save model at each epoch\n",
    "        logging_dir=f\"./logs_BBBP/{wandb.run.id}\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        report_to=\"wandb\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_mcc_metric\"\n",
    "    )\n",
    "\n",
    "    accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = softmax(logits, axis=1)[:, 1]  \n",
    "        predictions = np.argmax(logits, axis=1)  \n",
    "        mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "        return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities),\n",
    "            \"Precision\": precision_score(labels, predictions),\n",
    "            \"Recall\": recall_score(labels, predictions),\n",
    "            \"F1-score\": f1_score(labels, predictions)\n",
    "        }\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=training_data,\n",
    "        eval_dataset=validation_data,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save model and tokenizer for this run\n",
    "    trainer.save_model(save_dir)\n",
    "    \n",
    "    print(f\"Model saved to {save_dir}\")\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78dd991",
   "metadata": {},
   "source": [
    "### Focal loss trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d30bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize W&B with sweep\n",
    "def run_training():\n",
    "    run = wandb.init(project=\"BBBP focal loss Hyperparameter Tuning\")\n",
    "    config = run.config   \n",
    "\n",
    "    # Define unique save path for each W&B run\n",
    "    save_dir = f\"./models_BBBP_focal_loss/{wandb.run.id}\"  # Unique directory for each run\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"ibm/MoLFormer-XL-both-10pct\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Load data\n",
    "    train_data, val_data = data_load()\n",
    "    training_data = data_prep(train_data, tokenizer)\n",
    "    validation_data = data_prep(val_data, tokenizer)\n",
    "\n",
    "    # Load base model\n",
    "    model_clin = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"ibm/MoLFormer-XL-both-10pct\",\n",
    "        num_labels=2,\n",
    "        problem_type=\"single_label_classification\",    \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Apply LoRA\n",
    "    peft_config = lora_config(config.r, config.lora_alpha, config.dropout)\n",
    "    lora_model = get_peft_model(model_clin, peft_config)\n",
    "    \n",
    "    lora_model.print_trainable_parameters()\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=config.lr,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=20,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",  # Save model at each epoch\n",
    "        logging_dir=f\"./logs_BBBP/{wandb.run.id}\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        report_to=\"wandb\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_mcc_metric\"\n",
    "    )\n",
    "\n",
    "    accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = softmax(logits, axis=1)[:, 1]  \n",
    "        predictions = np.argmax(logits, axis=1)  \n",
    "        mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "        return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities),\n",
    "            \"Precision\": precision_score(labels, predictions),\n",
    "            \"Recall\": recall_score(labels, predictions),\n",
    "            \"F1-score\": f1_score(labels, predictions)\n",
    "        }\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = FocalLossTrainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=training_data,\n",
    "        eval_dataset=validation_data,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save model and tokenizer for this run\n",
    "    trainer.save_model(save_dir)\n",
    "    \n",
    "    print(f\"Model saved to {save_dir}\")\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54586d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,640,458 || all params: 47,213,588 || trainable%: 3.4745\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "\"ibm/MoLFormer-XL-both-10pct\",\n",
    "trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Load base model\n",
    "model_clin = AutoModelForSequenceClassification.from_pretrained(\n",
    "\"ibm/MoLFormer-XL-both-10pct\",\n",
    "num_labels=2,\n",
    "problem_type=\"single_label_classification\",    \n",
    "trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "peft_config = lora_config(4, 128, 0.2)\n",
    "lora_model = get_peft_model(model_clin, peft_config)\n",
    "\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a82522",
   "metadata": {},
   "source": [
    "Evaluate on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2ff4907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MolformerForSequenceClassification were not initialized from the model checkpoint at ibm/MoLFormer-XL-both-10pct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.dense2.bias', 'classifier.dense2.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"ibm/MoLFormer-XL-both-10pct\",\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True,\n",
    "    deterministic_eval=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"ibm/MoLFormer-XL-both-10pct\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12b3e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Molformer_Finetuned/bbbp/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7760b2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "smiles_test = test_data['smiles'].tolist()\n",
    "\n",
    "test_tokenized =tokenizer(smiles_test)\n",
    "\n",
    "test_dataset = Dataset.from_dict(test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf4573af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_data['p_np'].tolist() \n",
    "\n",
    "\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8180893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = softmax(logits, axis=1)[:, 1]  # Get probabilities for class 1\n",
    "        predictions = np.argmax(logits, axis=1)  # Choose the most likely class\n",
    "        mcc = matthews_corrcoef(labels, predictions)\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities),  # AUC-ROC requires probabilities\n",
    "            \"Precision\": precision_score(labels, predictions),\n",
    "            \"Recall\": recall_score(labels, predictions),\n",
    "            \"F1-score\": f1_score(labels, predictions)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22530933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"./test_results_clintox_wandb\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    report_to=\"none\",  # Disable logging to W&B for test\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "\"ibm/MoLFormer-XL-both-10pct\",\n",
    "trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46b102dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./models_BBBP_focal_loss/qh5ahemf', './models_BBBP_focal_loss/6lsvst0q', './models_BBBP_focal_loss/7gw4703h', './models_BBBP_focal_loss/zvxvvpai', './models_BBBP_focal_loss/u6qyowv9', './models_BBBP_focal_loss/mbek35f7', './models_BBBP_focal_loss/bxga4pr1', './models_BBBP_focal_loss/mnh92oo9', './models_BBBP_focal_loss/zoq9c9ir', './models_BBBP_focal_loss/appu9sb4']\n",
      "Evaluating model: ./models_BBBP_focal_loss/qh5ahemf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2091449/2440043454.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP focal loss ./models_BBBP_focal_loss/qh5ahemf: {'eval_mcc_metric': 0.8214238960405138, 'eval_loss': 0.12069400399923325, 'eval_model_preparation_time': 0.0131, 'eval_Accuracy': 0.9175257731958762, 'eval_AUC-ROC': 0.964273445551357, 'eval_Precision': 0.928, 'eval_Recall': 0.943089430894309, 'eval_F1-score': 0.9354838709677419, 'eval_runtime': 0.4934, 'eval_samples_per_second': 393.157, 'eval_steps_per_second': 26.346}\n",
      "Evaluating model: ./models_BBBP_focal_loss/6lsvst0q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2091449/2440043454.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP focal loss ./models_BBBP_focal_loss/6lsvst0q: {'eval_mcc_metric': 0.7641952976557471, 'eval_loss': 0.07125464826822281, 'eval_model_preparation_time': 0.0137, 'eval_Accuracy': 0.8917525773195877, 'eval_AUC-ROC': 0.952708118630482, 'eval_Precision': 0.8923076923076924, 'eval_Recall': 0.943089430894309, 'eval_F1-score': 0.9169960474308301, 'eval_runtime': 0.4726, 'eval_samples_per_second': 410.471, 'eval_steps_per_second': 27.506}\n",
      "Evaluating model: ./models_BBBP_focal_loss/7gw4703h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2091449/2440043454.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP focal loss ./models_BBBP_focal_loss/7gw4703h: {'eval_mcc_metric': 0.7330462738834338, 'eval_loss': 0.07228748500347137, 'eval_model_preparation_time': 0.013, 'eval_Accuracy': 0.8711340206185567, 'eval_AUC-ROC': 0.9480132829497309, 'eval_Precision': 0.9298245614035088, 'eval_Recall': 0.8617886178861789, 'eval_F1-score': 0.8945147679324894, 'eval_runtime': 0.481, 'eval_samples_per_second': 403.305, 'eval_steps_per_second': 27.026}\n",
      "Evaluating model: ./models_BBBP_focal_loss/zvxvvpai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2091449/2440043454.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP focal loss ./models_BBBP_focal_loss/zvxvvpai: {'eval_mcc_metric': 0.7759615538661074, 'eval_loss': 0.07295171171426773, 'eval_model_preparation_time': 0.0131, 'eval_Accuracy': 0.8969072164948454, 'eval_AUC-ROC': 0.9653040192373755, 'eval_Precision': 0.905511811023622, 'eval_Recall': 0.9349593495934959, 'eval_F1-score': 0.92, 'eval_runtime': 0.4772, 'eval_samples_per_second': 406.572, 'eval_steps_per_second': 27.245}\n",
      "Evaluating model: ./models_BBBP_focal_loss/u6qyowv9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2091449/2440043454.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP focal loss ./models_BBBP_focal_loss/u6qyowv9: {'eval_mcc_metric': 0.7875047469022934, 'eval_loss': 0.06442337483167648, 'eval_model_preparation_time': 0.0133, 'eval_Accuracy': 0.9020618556701031, 'eval_AUC-ROC': 0.9607236917439597, 'eval_Precision': 0.9126984126984127, 'eval_Recall': 0.9349593495934959, 'eval_F1-score': 0.9236947791164659, 'eval_runtime': 0.4789, 'eval_samples_per_second': 405.102, 'eval_steps_per_second': 27.146}\n",
      "Evaluating model: ./models_BBBP_focal_loss/mbek35f7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2091449/2440043454.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2091449/2440043454.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP focal loss ./models_BBBP_focal_loss/mbek35f7: {'eval_mcc_metric': 0.7534564646973192, 'eval_loss': 0.06791112571954727, 'eval_model_preparation_time': 0.0131, 'eval_Accuracy': 0.8865979381443299, 'eval_AUC-ROC': 0.9646169701133631, 'eval_Precision': 0.8976377952755905, 'eval_Recall': 0.926829268292683, 'eval_F1-score': 0.912, 'eval_runtime': 0.4778, 'eval_samples_per_second': 406.037, 'eval_steps_per_second': 27.209}\n",
      "Evaluating model: ./models_BBBP_focal_loss/bxga4pr1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP focal loss ./models_BBBP_focal_loss/bxga4pr1: {'eval_mcc_metric': 0.7414360272005406, 'eval_loss': 0.07517533749341965, 'eval_model_preparation_time': 0.0156, 'eval_Accuracy': 0.8814432989690721, 'eval_AUC-ROC': 0.955570823313867, 'eval_Precision': 0.8846153846153846, 'eval_Recall': 0.9349593495934959, 'eval_F1-score': 0.9090909090909091, 'eval_runtime': 0.574, 'eval_samples_per_second': 337.978, 'eval_steps_per_second': 22.648}\n",
      "Evaluating model: ./models_BBBP_focal_loss/mnh92oo9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2091449/2440043454.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP focal loss ./models_BBBP_focal_loss/mnh92oo9: {'eval_mcc_metric': 0.7641952976557471, 'eval_loss': 0.06537608057260513, 'eval_model_preparation_time': 0.0155, 'eval_Accuracy': 0.8917525773195877, 'eval_AUC-ROC': 0.9616397572426428, 'eval_Precision': 0.8923076923076924, 'eval_Recall': 0.943089430894309, 'eval_F1-score': 0.9169960474308301, 'eval_runtime': 0.5809, 'eval_samples_per_second': 333.95, 'eval_steps_per_second': 22.378}\n",
      "Evaluating model: ./models_BBBP_focal_loss/zoq9c9ir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2091449/2440043454.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP focal loss ./models_BBBP_focal_loss/zoq9c9ir: {'eval_mcc_metric': 0.6959174862901277, 'eval_loss': 0.07610391825437546, 'eval_model_preparation_time': 0.0155, 'eval_Accuracy': 0.8608247422680413, 'eval_AUC-ROC': 0.9449215618916753, 'eval_Precision': 0.8692307692307693, 'eval_Recall': 0.9186991869918699, 'eval_F1-score': 0.8932806324110671, 'eval_runtime': 0.5813, 'eval_samples_per_second': 333.728, 'eval_steps_per_second': 22.363}\n",
      "Evaluating model: ./models_BBBP_focal_loss/appu9sb4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2091449/2440043454.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP focal loss ./models_BBBP_focal_loss/appu9sb4: {'eval_mcc_metric': 0.7299512279923026, 'eval_loss': 0.07019228488206863, 'eval_model_preparation_time': 0.0156, 'eval_Accuracy': 0.8762886597938144, 'eval_AUC-ROC': 0.954425741440513, 'eval_Precision': 0.8778625954198473, 'eval_Recall': 0.9349593495934959, 'eval_F1-score': 0.905511811023622, 'eval_runtime': 0.5822, 'eval_samples_per_second': 333.216, 'eval_steps_per_second': 22.329}\n"
     ]
    }
   ],
   "source": [
    "# List all checkpoints inside models directory\n",
    "\n",
    "models_dir = \"./models_BBBP_focal_loss\"\n",
    "model_checkpoints = [\n",
    "    os.path.join(models_dir, ckpt) \n",
    "    for ckpt in os.listdir(models_dir) \n",
    "    if os.path.isdir(os.path.join(models_dir, ckpt))  # Ensure it's a directory\n",
    "]\n",
    "print(model_checkpoints)\n",
    "# Evaluate each saved model\n",
    "\n",
    "for model_path in model_checkpoints:\n",
    "    \n",
    "    print(f\"Evaluating model: {model_path}\")\n",
    "\n",
    "    # Load the fine-tuned LoRA adapter model\n",
    "    \n",
    "    adapter_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    adapter_model.eval()  # Set to evaluation mode\n",
    "\n",
    "    # Initialize Trainer for evaluation\n",
    "    trainer = FocalLossTrainer(\n",
    "        model=adapter_model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Run evaluation\n",
    "    test_results = trainer.evaluate()\n",
    "    \n",
    "    print(f\"Test Results for BBBP focal loss {model_path}: {test_results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f73f0fb",
   "metadata": {},
   "source": [
    "### Best model for focal loss: models_BBBP_focal_loss/zvxvvpai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53b3bb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./models_BBBP/9vkzvef4', './models_BBBP/ngi394z7', './models_BBBP/mx5n8q53', './models_BBBP/5xmpcs4o', './models_BBBP/5tf2qbgz', './models_BBBP/8xuh5hwj', './models_BBBP/repizpfk', './models_BBBP/7dgp2e3w', './models_BBBP/sot1c3xx', './models_BBBP/ebn9ruz1', './models_BBBP/fk9mffo4', './models_BBBP/q4lnica8', './models_BBBP/q2xirung', './models_BBBP/50iwmcx3', './models_BBBP/gxheqjrf', './models_BBBP/2g93zcww', './models_BBBP/ste6wfed', './models_BBBP/4wgyglva', './models_BBBP/8fpgrske', './models_BBBP/kex36n5y']\n",
      "Evaluating model: ./models_BBBP/9vkzvef4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/9vkzvef4: {'eval_mcc_metric': -0.3805373150193529, 'eval_loss': 0.957651674747467, 'eval_model_preparation_time': 0.0125, 'eval_Accuracy': 0.27319587628865977, 'eval_AUC-ROC': 0.15092179090804994, 'eval_Precision': 0.25, 'eval_Recall': 0.07317073170731707, 'eval_F1-score': 0.11320754716981132, 'eval_runtime': 0.4955, 'eval_samples_per_second': 391.521, 'eval_steps_per_second': 26.236}\n",
      "Evaluating model: ./models_BBBP/ngi394z7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/ngi394z7: {'eval_mcc_metric': 0.723782267124832, 'eval_loss': 0.23008368909358978, 'eval_model_preparation_time': 0.0128, 'eval_Accuracy': 0.865979381443299, 'eval_AUC-ROC': 0.9596931180579411, 'eval_Precision': 0.9292035398230089, 'eval_Recall': 0.8536585365853658, 'eval_F1-score': 0.8898305084745762, 'eval_runtime': 0.4916, 'eval_samples_per_second': 394.667, 'eval_steps_per_second': 26.447}\n",
      "Evaluating model: ./models_BBBP/mx5n8q53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/mx5n8q53: {'eval_mcc_metric': 0.7454811946201522, 'eval_loss': 0.2184453010559082, 'eval_model_preparation_time': 0.0128, 'eval_Accuracy': 0.8762886597938144, 'eval_AUC-ROC': 0.9607236917439597, 'eval_Precision': 0.9380530973451328, 'eval_Recall': 0.8617886178861789, 'eval_F1-score': 0.8983050847457628, 'eval_runtime': 0.4917, 'eval_samples_per_second': 394.541, 'eval_steps_per_second': 26.438}\n",
      "Evaluating model: ./models_BBBP/5xmpcs4o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/5xmpcs4o: {'eval_mcc_metric': 0.7146317484209869, 'eval_loss': 0.23606035113334656, 'eval_model_preparation_time': 0.0127, 'eval_Accuracy': 0.8608247422680413, 'eval_AUC-ROC': 0.9514485285697927, 'eval_Precision': 0.9285714285714286, 'eval_Recall': 0.8455284552845529, 'eval_F1-score': 0.8851063829787233, 'eval_runtime': 0.4868, 'eval_samples_per_second': 398.484, 'eval_steps_per_second': 26.703}\n",
      "Evaluating model: ./models_BBBP/5tf2qbgz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/5tf2qbgz: {'eval_mcc_metric': 0.7330462738834338, 'eval_loss': 0.2229623943567276, 'eval_model_preparation_time': 0.0129, 'eval_Accuracy': 0.8711340206185567, 'eval_AUC-ROC': 0.9551127905645254, 'eval_Precision': 0.9298245614035088, 'eval_Recall': 0.8617886178861789, 'eval_F1-score': 0.8945147679324894, 'eval_runtime': 0.4869, 'eval_samples_per_second': 398.46, 'eval_steps_per_second': 26.701}\n",
      "Evaluating model: ./models_BBBP/8xuh5hwj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/8xuh5hwj: {'eval_mcc_metric': 0.7146317484209869, 'eval_loss': 0.23071998357772827, 'eval_model_preparation_time': 0.0128, 'eval_Accuracy': 0.8608247422680413, 'eval_AUC-ROC': 0.9596931180579411, 'eval_Precision': 0.9285714285714286, 'eval_Recall': 0.8455284552845529, 'eval_F1-score': 0.8851063829787233, 'eval_runtime': 0.4877, 'eval_samples_per_second': 397.796, 'eval_steps_per_second': 26.656}\n",
      "Evaluating model: ./models_BBBP/repizpfk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/repizpfk: {'eval_mcc_metric': 0.6285452326977308, 'eval_loss': 0.28119876980781555, 'eval_model_preparation_time': 0.0128, 'eval_Accuracy': 0.8092783505154639, 'eval_AUC-ROC': 0.9347303332188251, 'eval_Precision': 0.9215686274509803, 'eval_Recall': 0.7642276422764228, 'eval_F1-score': 0.8355555555555556, 'eval_runtime': 0.4805, 'eval_samples_per_second': 403.784, 'eval_steps_per_second': 27.058}\n",
      "Evaluating model: ./models_BBBP/7dgp2e3w\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/7dgp2e3w: {'eval_mcc_metric': 0.6966551358109493, 'eval_loss': 0.23464028537273407, 'eval_model_preparation_time': 0.0128, 'eval_Accuracy': 0.8505154639175257, 'eval_AUC-ROC': 0.9508759876331156, 'eval_Precision': 0.9272727272727272, 'eval_Recall': 0.8292682926829268, 'eval_F1-score': 0.8755364806866953, 'eval_runtime': 0.4866, 'eval_samples_per_second': 398.707, 'eval_steps_per_second': 26.717}\n",
      "Evaluating model: ./models_BBBP/sot1c3xx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/sot1c3xx: {'eval_mcc_metric': 0.7704751344800757, 'eval_loss': 0.22046661376953125, 'eval_model_preparation_time': 0.0128, 'eval_Accuracy': 0.8865979381443299, 'eval_AUC-ROC': 0.9611817244933012, 'eval_Precision': 0.954954954954955, 'eval_Recall': 0.8617886178861789, 'eval_F1-score': 0.905982905982906, 'eval_runtime': 0.4867, 'eval_samples_per_second': 398.631, 'eval_steps_per_second': 26.712}\n",
      "Evaluating model: ./models_BBBP/ebn9ruz1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/ebn9ruz1: {'eval_mcc_metric': 0.6367615579291525, 'eval_loss': 0.2501973509788513, 'eval_model_preparation_time': 0.0127, 'eval_Accuracy': 0.8144329896907216, 'eval_AUC-ROC': 0.9506469712584449, 'eval_Precision': 0.9223300970873787, 'eval_Recall': 0.7723577235772358, 'eval_F1-score': 0.8407079646017699, 'eval_runtime': 0.4858, 'eval_samples_per_second': 399.338, 'eval_steps_per_second': 26.76}\n",
      "Evaluating model: ./models_BBBP/fk9mffo4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/fk9mffo4: {'eval_mcc_metric': 0.7704751344800757, 'eval_loss': 0.21848148107528687, 'eval_model_preparation_time': 0.0127, 'eval_Accuracy': 0.8865979381443299, 'eval_AUC-ROC': 0.9628993473033323, 'eval_Precision': 0.954954954954955, 'eval_Recall': 0.8617886178861789, 'eval_F1-score': 0.905982905982906, 'eval_runtime': 0.4818, 'eval_samples_per_second': 402.663, 'eval_steps_per_second': 26.983}\n",
      "Evaluating model: ./models_BBBP/q4lnica8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/q4lnica8: {'eval_mcc_metric': 0.7765225738324971, 'eval_loss': 0.21523205935955048, 'eval_model_preparation_time': 0.0129, 'eval_Accuracy': 0.8917525773195877, 'eval_AUC-ROC': 0.9641589373640215, 'eval_Precision': 0.9473684210526315, 'eval_Recall': 0.8780487804878049, 'eval_F1-score': 0.9113924050632911, 'eval_runtime': 0.4973, 'eval_samples_per_second': 390.106, 'eval_steps_per_second': 26.141}\n",
      "Evaluating model: ./models_BBBP/q2xirung\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/q2xirung: {'eval_mcc_metric': 0.7272188253817875, 'eval_loss': 0.22433921694755554, 'eval_model_preparation_time': 0.0128, 'eval_Accuracy': 0.865979381443299, 'eval_AUC-ROC': 0.9595786098706057, 'eval_Precision': 0.9369369369369369, 'eval_Recall': 0.8455284552845529, 'eval_F1-score': 0.8888888888888888, 'eval_runtime': 0.4826, 'eval_samples_per_second': 402.014, 'eval_steps_per_second': 26.939}\n",
      "Evaluating model: ./models_BBBP/50iwmcx3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/50iwmcx3: {'eval_mcc_metric': 0.7834337703851408, 'eval_loss': 0.21752232313156128, 'eval_model_preparation_time': 0.0127, 'eval_Accuracy': 0.8969072164948454, 'eval_AUC-ROC': 0.960838199931295, 'eval_Precision': 0.9401709401709402, 'eval_Recall': 0.8943089430894309, 'eval_F1-score': 0.9166666666666666, 'eval_runtime': 0.4895, 'eval_samples_per_second': 396.307, 'eval_steps_per_second': 26.557}\n",
      "Evaluating model: ./models_BBBP/gxheqjrf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/gxheqjrf: {'eval_mcc_metric': 0.6450566560194042, 'eval_loss': 0.2560390532016754, 'eval_model_preparation_time': 0.0128, 'eval_Accuracy': 0.8195876288659794, 'eval_AUC-ROC': 0.9462956601396999, 'eval_Precision': 0.9230769230769231, 'eval_Recall': 0.7804878048780488, 'eval_F1-score': 0.8458149779735683, 'eval_runtime': 0.4947, 'eval_samples_per_second': 392.165, 'eval_steps_per_second': 26.279}\n",
      "Evaluating model: ./models_BBBP/2g93zcww\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/2g93zcww: {'eval_mcc_metric': 0.7093887596065858, 'eval_loss': 0.2195080816745758, 'eval_model_preparation_time': 0.0129, 'eval_Accuracy': 0.8556701030927835, 'eval_AUC-ROC': 0.9569449215618917, 'eval_Precision': 0.9357798165137615, 'eval_Recall': 0.8292682926829268, 'eval_F1-score': 0.8793103448275862, 'eval_runtime': 0.4865, 'eval_samples_per_second': 398.806, 'eval_steps_per_second': 26.724}\n",
      "Evaluating model: ./models_BBBP/ste6wfed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/ste6wfed: {'eval_mcc_metric': 0.7888790496107927, 'eval_loss': 0.2089870125055313, 'eval_model_preparation_time': 0.0129, 'eval_Accuracy': 0.8969072164948454, 'eval_AUC-ROC': 0.966334592923394, 'eval_Precision': 0.9557522123893806, 'eval_Recall': 0.8780487804878049, 'eval_F1-score': 0.9152542372881356, 'eval_runtime': 0.489, 'eval_samples_per_second': 396.751, 'eval_steps_per_second': 26.586}\n",
      "Evaluating model: ./models_BBBP/4wgyglva\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/4wgyglva: {'eval_mcc_metric': 0.7330462738834338, 'eval_loss': 0.23195064067840576, 'eval_model_preparation_time': 0.0128, 'eval_Accuracy': 0.8711340206185567, 'eval_AUC-ROC': 0.9590060689339287, 'eval_Precision': 0.9298245614035088, 'eval_Recall': 0.8617886178861789, 'eval_F1-score': 0.8945147679324894, 'eval_runtime': 0.4915, 'eval_samples_per_second': 394.681, 'eval_steps_per_second': 26.448}\n",
      "Evaluating model: ./models_BBBP/8fpgrske\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/8fpgrske: {'eval_mcc_metric': 0.764207955971172, 'eval_loss': 0.22854012250900269, 'eval_model_preparation_time': 0.0128, 'eval_Accuracy': 0.8865979381443299, 'eval_AUC-ROC': 0.9609527081186304, 'eval_Precision': 0.9391304347826087, 'eval_Recall': 0.8780487804878049, 'eval_F1-score': 0.907563025210084, 'eval_runtime': 0.4873, 'eval_samples_per_second': 398.113, 'eval_steps_per_second': 26.678}\n",
      "Evaluating model: ./models_BBBP/kex36n5y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.dense2.lora_A.default.weight', 'base_model.model.classifier.original_module.dense2.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.dense2.lora_A.default.weight', 'base_model.model.classifier.dense2.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_2082839/2044656183.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results for BBBP ./models_BBBP/kex36n5y: {'eval_mcc_metric': 0.6919611956034514, 'eval_loss': 0.23126566410064697, 'eval_model_preparation_time': 0.0129, 'eval_Accuracy': 0.845360824742268, 'eval_AUC-ROC': 0.9560288560632084, 'eval_Precision': 0.9345794392523364, 'eval_Recall': 0.8130081300813008, 'eval_F1-score': 0.8695652173913043, 'eval_runtime': 0.4946, 'eval_samples_per_second': 392.267, 'eval_steps_per_second': 26.286}\n"
     ]
    }
   ],
   "source": [
    "# List all checkpoints inside models directory\n",
    "\n",
    "models_dir = \"./models_BBBP\"\n",
    "model_checkpoints = [\n",
    "    os.path.join(models_dir, ckpt) \n",
    "    for ckpt in os.listdir(models_dir) \n",
    "    if os.path.isdir(os.path.join(models_dir, ckpt))  # Ensure it's a directory\n",
    "]\n",
    "print(model_checkpoints)\n",
    "# Evaluate each saved model\n",
    "\n",
    "for model_path in model_checkpoints:\n",
    "    \n",
    "    print(f\"Evaluating model: {model_path}\")\n",
    "\n",
    "    # Load the fine-tuned LoRA adapter model\n",
    "    \n",
    "    adapter_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    adapter_model.eval()  # Set to evaluation mode\n",
    "\n",
    "    # Initialize Trainer for evaluation\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=adapter_model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Run evaluation\n",
    "    test_results = trainer.evaluate()\n",
    "    \n",
    "    print(f\"Test Results for BBBP {model_path}: {test_results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dcb6da",
   "metadata": {},
   "source": [
    "### Best model for wieghted Loss trainer: models_BBBP/ste6wfed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe7cf0b",
   "metadata": {},
   "source": [
    "## Focal Loss Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb696af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544b8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ee4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Molformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
