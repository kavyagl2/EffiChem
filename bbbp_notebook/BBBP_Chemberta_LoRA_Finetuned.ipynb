{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ba02ee",
   "metadata": {},
   "source": [
    "## Finetuning Chemberta for BBBP Property Prediction Task using LoRA Technique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f213a8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "import evaluate\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4063dd",
   "metadata": {},
   "source": [
    "### Loading the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc48c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_bbbp=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/bbbp/train.csv')\n",
    "val_bbbp=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/bbbp/valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8674ff",
   "metadata": {},
   "source": [
    "## Chemberta Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b2087",
   "metadata": {},
   "source": [
    "## Model 77M MLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44386a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-77M-MLM\",\n",
    "    num_labels=5,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5cfec",
   "metadata": {},
   "source": [
    "## Model 10M MLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e98c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MLM and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-10M-MLM\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-10M-MLM\",\n",
    "    num_labels=5,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b3a8e9",
   "metadata": {},
   "source": [
    "## Model 10M MTR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d7e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-10M-MTR\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-10M-MTR\",\n",
    "    num_labels=5,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c6a602",
   "metadata": {},
   "source": [
    "## Model 77M MTR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d19523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-77M-MTR\",\n",
    "    num_labels=5,\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991fe7e7",
   "metadata": {},
   "source": [
    "## Model 5M MTR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3722d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## Model 5M MTR Model\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-5M-MTR\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-5M-MTR\",\n",
    "    num_labels=5,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503b99d",
   "metadata": {},
   "source": [
    "Preparing Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73d63d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list= train_bbbp['smiles'].tolist()\n",
    "smiles_val=val_bbbp['smiles'].tolist()\n",
    "train_tokenized=tokenizer(smiles_list)\n",
    "val_tokenized=tokenizer(smiles_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0301312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "val_dataset = Dataset.from_dict(val_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c875ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_bbbp['p_np'].tolist() # Assuming tasks start from column 1\n",
    "val_labels = val_bbbp['p_np'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65b1bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "val_dataset = val_dataset.add_column(\"labels\", val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f250d9b",
   "metadata": {},
   "source": [
    "### Compute Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f55d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    probabilities= softmax(logits, axis=1)\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "        \n",
    "    return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities,multi_class=\"ovr\"),  # AUC-ROC requires probabilities\n",
    "            \"Precision\": precision_score(labels, predictions,average=\"macro\"),\n",
    "            \"Recall\": recall_score(labels, predictions,average=\"macro\"),\n",
    "            \"F1-score\": f1_score(labels, predictions,average=\"macro\")\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66390e0d",
   "metadata": {},
   "source": [
    "### define LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b740a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",  # Sequence classification task\n",
    "    r=8,  # Rank of LoRA matrices\n",
    "    lora_alpha=16,  # Scaling factor double of rank( from the rule of thumb)\n",
    "    target_modules='all-linear',\n",
    "    lora_dropout=0.1,  \n",
    "    #init_lora_weights=\"gaussian\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c2e57",
   "metadata": {},
   "source": [
    "### Define Custom Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473066b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#focal loss computation\n",
    "\n",
    "def focal_loss_multiclass(inputs, targets, alpha=1, gamma=2):\n",
    "    log_prob = F.log_softmax(inputs, dim=-1)\n",
    "    prob = torch.exp(log_prob)  # Convert log probabilities back to normal probabilities\n",
    "\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[-1])\n",
    "    pt = torch.sum(prob * targets_one_hot, dim=-1)  # Get probability of the true class\n",
    "\n",
    "    focal_loss = -alpha * (1 - pt) ** gamma * torch.sum(log_prob * targets_one_hot, dim=-1)\n",
    "    \n",
    "    return focal_loss.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2cf525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = focal_loss_multiclass(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c83c8",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning using WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36a9e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ffded68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import evaluate\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from evaluate import load\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef\n",
    "\n",
    "#Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4fdeb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load():\n",
    "    train_bbbp=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/bbbp/train.csv')\n",
    "    val_bbbp=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/bbbp/valid.csv')\n",
    "\n",
    "    return train_bbbp, val_bbbp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5947fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data_process,tokenizer):\n",
    "\n",
    "    smiles_list = data_process['smiles'].tolist()\n",
    "    tokenized=tokenizer(smiles_list)\n",
    "    \n",
    "    \n",
    "    dataset = Dataset.from_dict(tokenized)\n",
    "    \n",
    "\n",
    "    labels = data_process['p_np'].tolist() # Assuming tasks start from column 1\n",
    "    \n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "    \n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a7d7a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "def lora_config(r,lora_alpha,dropout):\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",  # Sequence classification task\n",
    "        r=r,  # Rank of LoRA matrices\n",
    "        lora_alpha=lora_alpha,  # Scaling factor double of rank( from the rule of thumb)\n",
    "        target_modules='all-linear',\n",
    "        lora_dropout=dropout  # Dropout rate\n",
    "        #init_lora_weights=\"gaussian\"\n",
    "    )\n",
    "\n",
    "    return lora_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39759f7f",
   "metadata": {},
   "source": [
    "### Weighted Loss Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e4eedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class_weights= [1-(train_dataset['labels'].count(0)/len(train_dataset['labels'])),\n",
    "                           1-(train_dataset['labels'].count(1)/len(train_dataset['labels']))]\n",
    "\n",
    "class_weights = torch.from_numpy(np.array(class_weights)).float().to(\"cuda\")\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Extract labels\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # compute custom loss (suppose one has 2 labels with different weights)\n",
    "        loss_func = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_func(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ea26c3",
   "metadata": {},
   "source": [
    "### Focal Loss Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c90e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#focal loss computation\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def focal_loss_multiclass(inputs, targets, alpha=1, gamma=2):\n",
    "    log_prob = F.log_softmax(inputs, dim=-1)\n",
    "    prob = torch.exp(log_prob)  # Convert log probabilities back to normal probabilities\n",
    "\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[-1])\n",
    "    pt = torch.sum(prob * targets_one_hot, dim=-1)  # Get probability of the true class\n",
    "\n",
    "    focal_loss = -alpha * (1 - pt) ** gamma * torch.sum(log_prob * targets_one_hot, dim=-1)\n",
    "    \n",
    "    return focal_loss.mean() \n",
    "\n",
    "\n",
    "class FocalLossTrainer(Trainer):\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = focal_loss_multiclass(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16544de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score,matthews_corrcoef\n",
    "\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "mcc_metric= load(\"matthews_correlation\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    probabilities = softmax(logits, axis=1)[:, 1]  # Get probabilities for class 1\n",
    "    predictions = np.argmax(logits, axis=1)  # Choose the most likely class\n",
    "    \n",
    "\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        \"eval_mcc_metric\": mcc,\n",
    "        \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"AUC-ROC\": roc_auc_score(labels, probabilities),  # AUC-ROC requires probabilities\n",
    "        \"Precision\": precision_score(labels, predictions),\n",
    "        \"Recall\": recall_score(labels, predictions),\n",
    "        \"F1-score\": f1_score(labels, predictions)\n",
    "    } \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16693de7",
   "metadata": {},
   "source": [
    "### Weighted Loss Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "330cb437",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize W&B with sweep\n",
    "def run_training():\n",
    "    run = wandb.init(project=\"BBBP chemberta Hyperparameter Tuning\")\n",
    "    config = run.config   \n",
    "\n",
    "    # Define unique save path for each W&B run\n",
    "    save_dir = f\"./models_BBBP_chemberta_77M_MLM_FL/{wandb.run.id}\"  # Unique directory for each run\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-77M-MLM\",                                 # Define any other model here\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",\n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Load data\n",
    "    train_data, val_data = data_load()\n",
    "    training_data = data_prep(train_data, tokenizer)\n",
    "    validation_data = data_prep(val_data, tokenizer)\n",
    "\n",
    "\n",
    "    # Apply LoRA\n",
    "    peft_config = lora_config(config.r, config.lora_alpha, config.dropout)\n",
    "    lora_model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    lora_model.print_trainable_parameters()\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=config.lr,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=20,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",  # Save model at each epoch\n",
    "        logging_dir=f\"./logs_BBBP/{wandb.run.id}\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        report_to=\"wandb\",\n",
    "        save_total_limit=10,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_mcc_metric\"\n",
    "    )\n",
    "\n",
    "    accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = softmax(logits, axis=1)[:, 1]  \n",
    "        predictions = np.argmax(logits, axis=1)  \n",
    "        mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "        return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities),\n",
    "            \"Precision\": precision_score(labels, predictions),\n",
    "            \"Recall\": recall_score(labels, predictions),\n",
    "            \"F1-score\": f1_score(labels, predictions)\n",
    "        }\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = WeightedLossTrainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=training_data,\n",
    "        eval_dataset=validation_data,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save model and tokenizer for this run\n",
    "    trainer.save_model(save_dir)\n",
    "    \n",
    "    print(f\"Model saved to {save_dir}\")\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb13d943",
   "metadata": {},
   "source": [
    "### Focal loss trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "996428a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize W&B with sweep\n",
    "def run_training():\n",
    "    run = wandb.init(project=\"BBBP focal loss Hyperparameter Tuning\")\n",
    "    config = run.config   \n",
    "\n",
    "    # Define unique save path for each W&B run\n",
    "    save_dir = f\"./models_BBBP_chemberta_5M_MTR_FL/{wandb.run.id}\"  # Unique directory for each run\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Load model directly\n",
    "\n",
    "\n",
    "    # Load model directly\n",
    "\n",
    "\n",
    "    ## Model 5M MTR Model\n",
    "\n",
    "\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-5M-MTR\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"DeepChem/ChemBERTa-5M-MTR\",\n",
    "        num_labels=2,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Load data\n",
    "    train_data, val_data = data_load()\n",
    "    training_data = data_prep(train_data, tokenizer)\n",
    "    validation_data = data_prep(val_data, tokenizer)\n",
    "\n",
    "\n",
    "    # Apply LoRA\n",
    "    peft_config = lora_config(config.r, config.lora_alpha, config.dropout)\n",
    "    lora_model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    lora_model.print_trainable_parameters()\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=config.lr,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=20,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",  # Save model at each epoch\n",
    "        logging_dir=f\"./logs_BBBP/{wandb.run.id}\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        report_to=\"wandb\",\n",
    "        save_total_limit=10,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_mcc_metric\"\n",
    "    )\n",
    "\n",
    "    accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = softmax(logits, axis=1)[:, 1]  \n",
    "        predictions = np.argmax(logits, axis=1)  \n",
    "        mcc = matthews_corrcoef(labels, predictions)\n",
    "\n",
    "        return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities),\n",
    "            \"Precision\": precision_score(labels, predictions),\n",
    "            \"Recall\": recall_score(labels, predictions),\n",
    "            \"F1-score\": f1_score(labels, predictions)\n",
    "        }\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = FocalLossTrainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=training_data,\n",
    "        eval_dataset=validation_data,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save model and tokenizer for this run\n",
    "    trainer.save_model(save_dir)\n",
    "    \n",
    "    print(f\"Model saved to {save_dir}\")\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9cde5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: c918zhrd\n",
      "Sweep URL: https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4rr2blnx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.9076650745205e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mharodharsha21\u001b[0m (\u001b[33mharodharsha21-iit-ropar\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'BBBP focal loss Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/wandb/run-20250409_131240-4rr2blnx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/4rr2blnx' target=\"_blank\">floral-sweep-1</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/4rr2blnx' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/4rr2blnx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,137,922 || all params: 4,639,988 || trainable%: 24.5242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1816725/2257317317.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-09 13:12:47,913] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='988' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 988/1040 00:48 < 00:02, 20.14 it/s, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.136749</td>\n",
       "      <td>0.257908</td>\n",
       "      <td>0.736585</td>\n",
       "      <td>0.887011</td>\n",
       "      <td>0.730964</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>0.118527</td>\n",
       "      <td>0.257908</td>\n",
       "      <td>0.736585</td>\n",
       "      <td>0.911609</td>\n",
       "      <td>0.730964</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>0.109615</td>\n",
       "      <td>0.372377</td>\n",
       "      <td>0.765854</td>\n",
       "      <td>0.921034</td>\n",
       "      <td>0.756614</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.856287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.092700</td>\n",
       "      <td>0.104693</td>\n",
       "      <td>0.422912</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.924713</td>\n",
       "      <td>0.768817</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.864048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.092700</td>\n",
       "      <td>0.100884</td>\n",
       "      <td>0.499383</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.926092</td>\n",
       "      <td>0.790055</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.877301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>0.098804</td>\n",
       "      <td>0.495823</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.926207</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.876543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>0.097573</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.925172</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.071100</td>\n",
       "      <td>0.097125</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.924828</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.071100</td>\n",
       "      <td>0.096356</td>\n",
       "      <td>0.538740</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.925747</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.884735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.096410</td>\n",
       "      <td>0.566391</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.925057</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.890282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.097237</td>\n",
       "      <td>0.579974</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.924943</td>\n",
       "      <td>0.820809</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.893082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.097460</td>\n",
       "      <td>0.593411</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.925402</td>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.895899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.098113</td>\n",
       "      <td>0.593411</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.924943</td>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.895899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.098486</td>\n",
       "      <td>0.606714</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.924828</td>\n",
       "      <td>0.830409</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.898734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.098691</td>\n",
       "      <td>0.606714</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.925057</td>\n",
       "      <td>0.830409</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.898734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.099418</td>\n",
       "      <td>0.606714</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.924598</td>\n",
       "      <td>0.830409</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.898734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.061500</td>\n",
       "      <td>0.099334</td>\n",
       "      <td>0.606714</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.924828</td>\n",
       "      <td>0.830409</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.898734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.099722</td>\n",
       "      <td>0.606714</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.925287</td>\n",
       "      <td>0.830409</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.898734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.099769</td>\n",
       "      <td>0.606714</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.925172</td>\n",
       "      <td>0.830409</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.898734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_BBBP_chemberta_5M_MTR_FL/4rr2blnx\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▇████████████████</td></tr><tr><td>eval/Accuracy</td><td>▁▁▃▄▅▅▆▆▆▇▇████████</td></tr><tr><td>eval/F1-score</td><td>▁▁▃▄▅▅▆▆▆▇▇████████</td></tr><tr><td>eval/Precision</td><td>▁▁▃▄▅▅▆▆▆▇▇████████</td></tr><tr><td>eval/Recall</td><td>██▅▅▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂</td></tr><tr><td>eval/mcc_metric</td><td>▁▁▃▄▆▆▆▆▇▇▇████████</td></tr><tr><td>eval/runtime</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▁████▇████████▇█▇██</td></tr><tr><td>eval/steps_per_second</td><td>▁████▇████████▇█▇██</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▄▅▄▂▂▁▇▂</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.92517</td></tr><tr><td>eval/Accuracy</td><td>0.8439</td></tr><tr><td>eval/F1-score</td><td>0.89873</td></tr><tr><td>eval/Precision</td><td>0.83041</td></tr><tr><td>eval/Recall</td><td>0.97931</td></tr><tr><td>eval/loss</td><td>0.09977</td></tr><tr><td>eval/mcc_metric</td><td>0.60671</td></tr><tr><td>eval/runtime</td><td>0.1324</td></tr><tr><td>eval/samples_per_second</td><td>1548.307</td></tr><tr><td>eval/steps_per_second</td><td>52.869</td></tr><tr><td>total_flos</td><td>83217215345208.0</td></tr><tr><td>train/epoch</td><td>19</td></tr><tr><td>train/global_step</td><td>988</td></tr><tr><td>train/grad_norm</td><td>0.15728</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0614</td></tr><tr><td>train_loss</td><td>0.07668</td></tr><tr><td>train_runtime</td><td>49.4478</td></tr><tr><td>train_samples_per_second</td><td>662.921</td></tr><tr><td>train_steps_per_second</td><td>21.032</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">floral-sweep-1</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/4rr2blnx' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/4rr2blnx</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250409_131240-4rr2blnx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h9wlrd8f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.5952356868457578e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'BBBP focal loss Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/wandb/run-20250409_131353-h9wlrd8f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/h9wlrd8f' target=\"_blank\">glorious-sweep-2</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/h9wlrd8f' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/h9wlrd8f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 395,938 || all params: 3,842,612 || trainable%: 10.3039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1816725/2257317317.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='572' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 572/1040 00:27 < 00:22, 20.50 it/s, Epoch 11/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.121122</td>\n",
       "      <td>0.233262</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.934598</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.839650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.119800</td>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.378375</td>\n",
       "      <td>0.765854</td>\n",
       "      <td>0.939195</td>\n",
       "      <td>0.753927</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.119800</td>\n",
       "      <td>0.095153</td>\n",
       "      <td>0.528040</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.940115</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.882716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.076800</td>\n",
       "      <td>0.089010</td>\n",
       "      <td>0.579974</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.939310</td>\n",
       "      <td>0.820809</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.893082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.076800</td>\n",
       "      <td>0.087346</td>\n",
       "      <td>0.632959</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.939080</td>\n",
       "      <td>0.840237</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.904459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.087756</td>\n",
       "      <td>0.645920</td>\n",
       "      <td>0.858537</td>\n",
       "      <td>0.938621</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.907348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.090885</td>\n",
       "      <td>0.632959</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.936322</td>\n",
       "      <td>0.840237</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.904459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.092017</td>\n",
       "      <td>0.618043</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.937471</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.900958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.091141</td>\n",
       "      <td>0.616929</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.938276</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.900322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.092587</td>\n",
       "      <td>0.603605</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.938621</td>\n",
       "      <td>0.838323</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.897436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.057300</td>\n",
       "      <td>0.095992</td>\n",
       "      <td>0.603605</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.937356</td>\n",
       "      <td>0.838323</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.897436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_BBBP_chemberta_5M_MTR_FL/h9wlrd8f\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▇█▇▇▆▃▅▆▆▄</td></tr><tr><td>eval/Accuracy</td><td>▁▃▆▇███▇▇▇▇</td></tr><tr><td>eval/F1-score</td><td>▁▃▅▇███▇▇▇▇</td></tr><tr><td>eval/Precision</td><td>▁▃▅▇███████</td></tr><tr><td>eval/Recall</td><td>██▆▅▅▅▅▃▁▁▁</td></tr><tr><td>eval/loss</td><td>█▅▃▁▁▁▂▂▂▂▃</td></tr><tr><td>eval/mcc_metric</td><td>▁▃▆▇█████▇▇</td></tr><tr><td>eval/runtime</td><td>▂▁▁▂▁▁▁▂▁█▁</td></tr><tr><td>eval/samples_per_second</td><td>▇██▇███▇█▁█</td></tr><tr><td>eval/steps_per_second</td><td>▇██▇███▇█▁█</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▃▄▄▅▅▆▆▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▃▄▄▅▅▆▆▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▂▁█▅▃</td></tr><tr><td>train/learning_rate</td><td>█▆▅▃▁</td></tr><tr><td>train/loss</td><td>█▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.93736</td></tr><tr><td>eval/Accuracy</td><td>0.8439</td></tr><tr><td>eval/F1-score</td><td>0.89744</td></tr><tr><td>eval/Precision</td><td>0.83832</td></tr><tr><td>eval/Recall</td><td>0.96552</td></tr><tr><td>eval/loss</td><td>0.09599</td></tr><tr><td>eval/mcc_metric</td><td>0.6036</td></tr><tr><td>eval/runtime</td><td>0.1305</td></tr><tr><td>eval/samples_per_second</td><td>1570.329</td></tr><tr><td>eval/steps_per_second</td><td>53.621</td></tr><tr><td>total_flos</td><td>39160001804784.0</td></tr><tr><td>train/epoch</td><td>11</td></tr><tr><td>train/global_step</td><td>572</td></tr><tr><td>train/grad_norm</td><td>0.36477</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.0573</td></tr><tr><td>train_loss</td><td>0.07371</td></tr><tr><td>train_runtime</td><td>27.8621</td></tr><tr><td>train_samples_per_second</td><td>1176.508</td></tr><tr><td>train_steps_per_second</td><td>37.327</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glorious-sweep-2</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/h9wlrd8f' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/h9wlrd8f</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250409_131353-h9wlrd8f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: syqmo6uu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.1720367087103236e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'BBBP focal loss Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/wandb/run-20250409_131429-syqmo6uu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/syqmo6uu' target=\"_blank\">deep-sweep-3</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/syqmo6uu' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/syqmo6uu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,127,234 || all params: 5,703,156 || trainable%: 37.2992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1816725/2257317317.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='884' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 884/1040 00:46 < 00:08, 18.81 it/s, Epoch 17/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.272118</td>\n",
       "      <td>0.741463</td>\n",
       "      <td>0.855057</td>\n",
       "      <td>0.742105</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.841791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.142200</td>\n",
       "      <td>0.126476</td>\n",
       "      <td>0.322595</td>\n",
       "      <td>0.751220</td>\n",
       "      <td>0.886897</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.849558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.142200</td>\n",
       "      <td>0.116302</td>\n",
       "      <td>0.335873</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.902184</td>\n",
       "      <td>0.748691</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.851190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>0.110515</td>\n",
       "      <td>0.372377</td>\n",
       "      <td>0.765854</td>\n",
       "      <td>0.912989</td>\n",
       "      <td>0.756614</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.856287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>0.106543</td>\n",
       "      <td>0.368660</td>\n",
       "      <td>0.765854</td>\n",
       "      <td>0.920460</td>\n",
       "      <td>0.759358</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.855422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.090500</td>\n",
       "      <td>0.103725</td>\n",
       "      <td>0.419110</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.921954</td>\n",
       "      <td>0.771739</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.863222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.090500</td>\n",
       "      <td>0.101470</td>\n",
       "      <td>0.435092</td>\n",
       "      <td>0.785366</td>\n",
       "      <td>0.922299</td>\n",
       "      <td>0.775956</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.865854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.099999</td>\n",
       "      <td>0.466037</td>\n",
       "      <td>0.795122</td>\n",
       "      <td>0.924368</td>\n",
       "      <td>0.784530</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.871166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.098179</td>\n",
       "      <td>0.481062</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.924598</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.873846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.097173</td>\n",
       "      <td>0.510343</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.925057</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.879257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.096877</td>\n",
       "      <td>0.538740</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.925402</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.884735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.096419</td>\n",
       "      <td>0.566391</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.925862</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.890282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.096413</td>\n",
       "      <td>0.566391</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.925747</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.890282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>0.096240</td>\n",
       "      <td>0.566391</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.926437</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.890282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.070800</td>\n",
       "      <td>0.096056</td>\n",
       "      <td>0.566391</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.926207</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.890282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.067000</td>\n",
       "      <td>0.096286</td>\n",
       "      <td>0.566391</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.926437</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.890282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.067000</td>\n",
       "      <td>0.096087</td>\n",
       "      <td>0.566391</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.926207</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.890282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_BBBP_chemberta_5M_MTR_FL/syqmo6uu\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▄▆▇▇████████████</td></tr><tr><td>eval/Accuracy</td><td>▁▂▂▃▃▄▅▅▆▆▇██████</td></tr><tr><td>eval/F1-score</td><td>▁▂▂▃▃▄▄▅▆▆▇██████</td></tr><tr><td>eval/Precision</td><td>▁▁▂▂▃▄▄▅▅▆▇██████</td></tr><tr><td>eval/Recall</td><td>▁█▆▆▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>eval/loss</td><td>█▅▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/mcc_metric</td><td>▁▂▃▃▃▄▅▆▆▇▇██████</td></tr><tr><td>eval/runtime</td><td>▅▆▁▁▃█▂▂▂▂▆▃▂▄▄▃▃</td></tr><tr><td>eval/samples_per_second</td><td>▄▃██▆▁▇▇▇▇▃▆▇▅▅▆▆</td></tr><tr><td>eval/steps_per_second</td><td>▄▃██▆▁▇▇▇▇▃▆▇▅▅▆▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▆▄▃▂▃▁▅</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.92621</td></tr><tr><td>eval/Accuracy</td><td>0.82927</td></tr><tr><td>eval/F1-score</td><td>0.89028</td></tr><tr><td>eval/Precision</td><td>0.81609</td></tr><tr><td>eval/Recall</td><td>0.97931</td></tr><tr><td>eval/loss</td><td>0.09609</td></tr><tr><td>eval/mcc_metric</td><td>0.56639</td></tr><tr><td>eval/runtime</td><td>0.1344</td></tr><tr><td>eval/samples_per_second</td><td>1525.413</td></tr><tr><td>eval/steps_per_second</td><td>52.087</td></tr><tr><td>total_flos</td><td>93419908489656.0</td></tr><tr><td>train/epoch</td><td>17</td></tr><tr><td>train/global_step</td><td>884</td></tr><tr><td>train/grad_norm</td><td>0.23906</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.067</td></tr><tr><td>train_loss</td><td>0.08574</td></tr><tr><td>train_runtime</td><td>46.9654</td></tr><tr><td>train_samples_per_second</td><td>697.96</td></tr><tr><td>train_steps_per_second</td><td>22.144</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deep-sweep-3</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/syqmo6uu' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/syqmo6uu</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250409_131429-syqmo6uu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kwtwjebf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.9439392673500315e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'BBBP focal loss Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/wandb/run-20250409_131527-kwtwjebf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/kwtwjebf' target=\"_blank\">frosty-sweep-4</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/kwtwjebf' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/kwtwjebf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,127,234 || all params: 5,703,156 || trainable%: 37.2992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1816725/2257317317.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='728' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 728/1040 00:37 < 00:16, 19.16 it/s, Epoch 14/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.129228</td>\n",
       "      <td>0.257908</td>\n",
       "      <td>0.736585</td>\n",
       "      <td>0.854943</td>\n",
       "      <td>0.730964</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.123300</td>\n",
       "      <td>0.115030</td>\n",
       "      <td>0.372377</td>\n",
       "      <td>0.765854</td>\n",
       "      <td>0.891494</td>\n",
       "      <td>0.756614</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.856287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.123300</td>\n",
       "      <td>0.108735</td>\n",
       "      <td>0.406543</td>\n",
       "      <td>0.775610</td>\n",
       "      <td>0.907701</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.861446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.104466</td>\n",
       "      <td>0.484693</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.914828</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.874618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>0.510343</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.917471</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.879257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.075900</td>\n",
       "      <td>0.100475</td>\n",
       "      <td>0.538740</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.919310</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.884735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.075900</td>\n",
       "      <td>0.100906</td>\n",
       "      <td>0.552651</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.917356</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.101330</td>\n",
       "      <td>0.593411</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.917471</td>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.895899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.101042</td>\n",
       "      <td>0.606714</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.918276</td>\n",
       "      <td>0.830409</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.898734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.102401</td>\n",
       "      <td>0.591400</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.918276</td>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.895238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.104487</td>\n",
       "      <td>0.591400</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.917931</td>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.895238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.060600</td>\n",
       "      <td>0.104823</td>\n",
       "      <td>0.591400</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.918276</td>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.895238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.060600</td>\n",
       "      <td>0.105836</td>\n",
       "      <td>0.604781</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.917701</td>\n",
       "      <td>0.834320</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.898089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>0.106588</td>\n",
       "      <td>0.590167</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.918046</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.894569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_BBBP_chemberta_5M_MTR_FL/kwtwjebf\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▇███████████</td></tr><tr><td>eval/Accuracy</td><td>▁▃▄▅▆▆▇███████</td></tr><tr><td>eval/F1-score</td><td>▁▃▃▅▆▆▇██████▇</td></tr><tr><td>eval/Precision</td><td>▁▃▃▅▆▆▆▇██████</td></tr><tr><td>eval/Recall</td><td>█▆▆▆▅▅▅▅▅▃▃▃▃▁</td></tr><tr><td>eval/loss</td><td>█▅▃▂▁▁▁▁▁▁▂▂▂▂</td></tr><tr><td>eval/mcc_metric</td><td>▁▃▄▆▆▇▇███████</td></tr><tr><td>eval/runtime</td><td>▁▃▃▃▄▃▃▂▄▄▄▄▃█</td></tr><tr><td>eval/samples_per_second</td><td>█▇▆▆▅▆▆▇▅▅▅▅▆▁</td></tr><tr><td>eval/steps_per_second</td><td>█▇▆▆▅▆▆▇▅▅▅▅▆▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▇▃█▄▁▁▂</td></tr><tr><td>train/learning_rate</td><td>█▇▆▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.91805</td></tr><tr><td>eval/Accuracy</td><td>0.83902</td></tr><tr><td>eval/F1-score</td><td>0.89457</td></tr><tr><td>eval/Precision</td><td>0.83333</td></tr><tr><td>eval/Recall</td><td>0.96552</td></tr><tr><td>eval/loss</td><td>0.10659</td></tr><tr><td>eval/mcc_metric</td><td>0.59017</td></tr><tr><td>eval/runtime</td><td>0.1378</td></tr><tr><td>eval/samples_per_second</td><td>1487.409</td></tr><tr><td>eval/steps_per_second</td><td>50.79</td></tr><tr><td>total_flos</td><td>76942168290792.0</td></tr><tr><td>train/epoch</td><td>14</td></tr><tr><td>train/global_step</td><td>728</td></tr><tr><td>train/grad_norm</td><td>0.16522</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.0611</td></tr><tr><td>train_loss</td><td>0.07567</td></tr><tr><td>train_runtime</td><td>37.9607</td></tr><tr><td>train_samples_per_second</td><td>863.525</td></tr><tr><td>train_steps_per_second</td><td>27.397</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-sweep-4</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/kwtwjebf' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/kwtwjebf</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250409_131527-kwtwjebf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8u9ogv87 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.7246328744624805e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'BBBP focal loss Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/wandb/run-20250409_131613-8u9ogv87</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8u9ogv87' target=\"_blank\">electric-sweep-5</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8u9ogv87' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8u9ogv87</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,127,234 || all params: 5,703,156 || trainable%: 37.2992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1816725/2257317317.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='728' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 728/1040 00:40 < 00:17, 17.87 it/s, Epoch 14/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.133277</td>\n",
       "      <td>0.341963</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.904483</td>\n",
       "      <td>0.746114</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.852071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.112653</td>\n",
       "      <td>0.360518</td>\n",
       "      <td>0.760976</td>\n",
       "      <td>0.918276</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.854599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.105364</td>\n",
       "      <td>0.372377</td>\n",
       "      <td>0.765854</td>\n",
       "      <td>0.925287</td>\n",
       "      <td>0.756614</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.856287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.084200</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.481062</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.926322</td>\n",
       "      <td>0.788889</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.873846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.084200</td>\n",
       "      <td>0.096451</td>\n",
       "      <td>0.538740</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.926782</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.884735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.095502</td>\n",
       "      <td>0.566391</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.927126</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.890282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.096137</td>\n",
       "      <td>0.593411</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.926782</td>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.895899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>0.097048</td>\n",
       "      <td>0.606714</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.927471</td>\n",
       "      <td>0.830409</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.898734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>0.097256</td>\n",
       "      <td>0.632959</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.928046</td>\n",
       "      <td>0.840237</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.904459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.060800</td>\n",
       "      <td>0.099073</td>\n",
       "      <td>0.632959</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.928276</td>\n",
       "      <td>0.840237</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.904459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.060800</td>\n",
       "      <td>0.101004</td>\n",
       "      <td>0.632959</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.927011</td>\n",
       "      <td>0.840237</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.904459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.101445</td>\n",
       "      <td>0.632959</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.927586</td>\n",
       "      <td>0.840237</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.904459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.101702</td>\n",
       "      <td>0.619893</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.926092</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.901587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.102917</td>\n",
       "      <td>0.618043</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.926092</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.900958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_BBBP_chemberta_5M_MTR_FL/8u9ogv87\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▇▇████████▇▇</td></tr><tr><td>eval/Accuracy</td><td>▁▁▂▄▆▆▇▇██████</td></tr><tr><td>eval/F1-score</td><td>▁▁▂▄▅▆▇▇██████</td></tr><tr><td>eval/Precision</td><td>▁▁▂▄▆▆▇▇██████</td></tr><tr><td>eval/Recall</td><td>██▆▃▃▃▃▃▃▃▃▃▃▁</td></tr><tr><td>eval/loss</td><td>█▄▃▂▁▁▁▁▁▂▂▂▂▂</td></tr><tr><td>eval/mcc_metric</td><td>▁▁▂▄▆▆▇▇██████</td></tr><tr><td>eval/runtime</td><td>▁█▁▅▂▁▂▁▃▁▂▂▃▃</td></tr><tr><td>eval/samples_per_second</td><td>█▁█▄▇█▇█▆█▇▆▆▆</td></tr><tr><td>eval/steps_per_second</td><td>█▁█▄▇█▇█▆█▇▆▆▆</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▃▅▆▃▁▅</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▃▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.92609</td></tr><tr><td>eval/Accuracy</td><td>0.84878</td></tr><tr><td>eval/F1-score</td><td>0.90096</td></tr><tr><td>eval/Precision</td><td>0.83929</td></tr><tr><td>eval/Recall</td><td>0.97241</td></tr><tr><td>eval/loss</td><td>0.10292</td></tr><tr><td>eval/mcc_metric</td><td>0.61804</td></tr><tr><td>eval/runtime</td><td>0.1346</td></tr><tr><td>eval/samples_per_second</td><td>1522.911</td></tr><tr><td>eval/steps_per_second</td><td>52.002</td></tr><tr><td>total_flos</td><td>76942168290792.0</td></tr><tr><td>train/epoch</td><td>14</td></tr><tr><td>train/global_step</td><td>728</td></tr><tr><td>train/grad_norm</td><td>0.17928</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.0589</td></tr><tr><td>train_loss</td><td>0.07539</td></tr><tr><td>train_runtime</td><td>40.6805</td></tr><tr><td>train_samples_per_second</td><td>805.792</td></tr><tr><td>train_steps_per_second</td><td>25.565</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">electric-sweep-5</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8u9ogv87' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/8u9ogv87</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250409_131613-8u9ogv87/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m9mf2oqr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.973668800289758e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'BBBP focal loss Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/wandb/run-20250409_131706-m9mf2oqr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/m9mf2oqr' target=\"_blank\">winter-sweep-6</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/m9mf2oqr' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/m9mf2oqr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,127,234 || all params: 5,703,156 || trainable%: 37.2992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1816725/2257317317.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='676' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 676/1040 00:34 < 00:18, 19.37 it/s, Epoch 13/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.129247</td>\n",
       "      <td>0.322595</td>\n",
       "      <td>0.751220</td>\n",
       "      <td>0.908161</td>\n",
       "      <td>0.742268</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.849558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.128700</td>\n",
       "      <td>0.110080</td>\n",
       "      <td>0.395625</td>\n",
       "      <td>0.770732</td>\n",
       "      <td>0.920805</td>\n",
       "      <td>0.757895</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.859701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.128700</td>\n",
       "      <td>0.103243</td>\n",
       "      <td>0.454467</td>\n",
       "      <td>0.790244</td>\n",
       "      <td>0.926437</td>\n",
       "      <td>0.777174</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.869301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.098180</td>\n",
       "      <td>0.495823</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.876543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.095573</td>\n",
       "      <td>0.552651</td>\n",
       "      <td>0.824390</td>\n",
       "      <td>0.927816</td>\n",
       "      <td>0.811429</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.071400</td>\n",
       "      <td>0.095375</td>\n",
       "      <td>0.579974</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.928506</td>\n",
       "      <td>0.820809</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.893082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.071400</td>\n",
       "      <td>0.097030</td>\n",
       "      <td>0.593411</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.927471</td>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.895899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>0.097654</td>\n",
       "      <td>0.632959</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.928276</td>\n",
       "      <td>0.840237</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.904459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.061600</td>\n",
       "      <td>0.097290</td>\n",
       "      <td>0.618043</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.929885</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.900958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.099436</td>\n",
       "      <td>0.631194</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.929655</td>\n",
       "      <td>0.844311</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.903846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.102693</td>\n",
       "      <td>0.618043</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.928391</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.900958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.101988</td>\n",
       "      <td>0.616929</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.928966</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.900322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.103059</td>\n",
       "      <td>0.576015</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.927471</td>\n",
       "      <td>0.832335</td>\n",
       "      <td>0.958621</td>\n",
       "      <td>0.891026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_BBBP_chemberta_5M_MTR_FL/m9mf2oqr\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▇▇▇█▇▇████▇</td></tr><tr><td>eval/Accuracy</td><td>▁▂▄▅▆▇▇█████▇</td></tr><tr><td>eval/F1-score</td><td>▁▂▄▄▆▇▇████▇▆</td></tr><tr><td>eval/Precision</td><td>▁▂▃▅▆▆▇█████▇</td></tr><tr><td>eval/Recall</td><td>██▇▅▅▅▅▅▄▄▄▂▁</td></tr><tr><td>eval/loss</td><td>█▄▃▂▁▁▁▁▁▂▃▂▃</td></tr><tr><td>eval/mcc_metric</td><td>▁▃▄▅▆▇▇█████▇</td></tr><tr><td>eval/runtime</td><td>▁▁▁▂▂▁▃▂▂▂▂▂█</td></tr><tr><td>eval/samples_per_second</td><td>███▇▇█▆▇▇▇▇▇▁</td></tr><tr><td>eval/steps_per_second</td><td>███▇▇█▆▇▇▇▇▇▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>█▂█▆▁▂</td></tr><tr><td>train/learning_rate</td><td>█▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>█▃▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.92747</td></tr><tr><td>eval/Accuracy</td><td>0.83415</td></tr><tr><td>eval/F1-score</td><td>0.89103</td></tr><tr><td>eval/Precision</td><td>0.83234</td></tr><tr><td>eval/Recall</td><td>0.95862</td></tr><tr><td>eval/loss</td><td>0.10306</td></tr><tr><td>eval/mcc_metric</td><td>0.57602</td></tr><tr><td>eval/runtime</td><td>0.1386</td></tr><tr><td>eval/samples_per_second</td><td>1479.05</td></tr><tr><td>eval/steps_per_second</td><td>50.504</td></tr><tr><td>total_flos</td><td>71423953610184.0</td></tr><tr><td>train/epoch</td><td>13</td></tr><tr><td>train/global_step</td><td>676</td></tr><tr><td>train/grad_norm</td><td>0.15332</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.0572</td></tr><tr><td>train_loss</td><td>0.07464</td></tr><tr><td>train_runtime</td><td>34.8521</td></tr><tr><td>train_samples_per_second</td><td>940.545</td></tr><tr><td>train_steps_per_second</td><td>29.84</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-sweep-6</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/m9mf2oqr' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/m9mf2oqr</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250409_131706-m9mf2oqr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l2kgceha with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.449164770695549e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'BBBP focal loss Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/wandb/run-20250409_131753-l2kgceha</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/l2kgceha' target=\"_blank\">mild-sweep-7</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/l2kgceha' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/l2kgceha</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 643,266 || all params: 4,108,404 || trainable%: 15.6573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1816725/2257317317.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='624' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 624/1040 00:30 < 00:20, 20.72 it/s, Epoch 12/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.132363</td>\n",
       "      <td>0.064278</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.875287</td>\n",
       "      <td>0.711443</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.826590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.114240</td>\n",
       "      <td>0.176297</td>\n",
       "      <td>0.721951</td>\n",
       "      <td>0.901379</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.834783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.107114</td>\n",
       "      <td>0.406543</td>\n",
       "      <td>0.775610</td>\n",
       "      <td>0.911379</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.861446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.100766</td>\n",
       "      <td>0.513825</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.915747</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.097958</td>\n",
       "      <td>0.538740</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.918391</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.884735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.098073</td>\n",
       "      <td>0.593411</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.921149</td>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.895899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.099990</td>\n",
       "      <td>0.632959</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.923103</td>\n",
       "      <td>0.840237</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.904459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.100858</td>\n",
       "      <td>0.631194</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.924598</td>\n",
       "      <td>0.844311</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.903846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.100417</td>\n",
       "      <td>0.616465</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.925977</td>\n",
       "      <td>0.847561</td>\n",
       "      <td>0.958621</td>\n",
       "      <td>0.899676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.102883</td>\n",
       "      <td>0.590247</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.926207</td>\n",
       "      <td>0.845679</td>\n",
       "      <td>0.944828</td>\n",
       "      <td>0.892508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>0.105391</td>\n",
       "      <td>0.603173</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.846626</td>\n",
       "      <td>0.951724</td>\n",
       "      <td>0.896104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.104680</td>\n",
       "      <td>0.603173</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.927241</td>\n",
       "      <td>0.846626</td>\n",
       "      <td>0.951724</td>\n",
       "      <td>0.896104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_BBBP_chemberta_5M_MTR_FL/l2kgceha\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▆▆▇▇▇█████</td></tr><tr><td>eval/Accuracy</td><td>▁▂▄▆▆▇███▇██</td></tr><tr><td>eval/F1-score</td><td>▁▂▄▆▆▇███▇▇▇</td></tr><tr><td>eval/Precision</td><td>▁▁▄▅▆▇██████</td></tr><tr><td>eval/Recall</td><td>▇█▇▇▆▆▆▅▃▁▂▂</td></tr><tr><td>eval/loss</td><td>█▄▃▂▁▁▁▂▂▂▃▂</td></tr><tr><td>eval/mcc_metric</td><td>▁▂▅▇▇████▇██</td></tr><tr><td>eval/runtime</td><td>█▃▄▂▂▄▁▄▂▃▃▂</td></tr><tr><td>eval/samples_per_second</td><td>▁▆▅▇▇▅█▅▇▆▆▇</td></tr><tr><td>eval/steps_per_second</td><td>▁▆▅▇▇▅█▅▇▆▆▇</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇███</td></tr><tr><td>train/grad_norm</td><td>▆▁█▄▄▅</td></tr><tr><td>train/learning_rate</td><td>█▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>█▄▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.92724</td></tr><tr><td>eval/Accuracy</td><td>0.8439</td></tr><tr><td>eval/F1-score</td><td>0.8961</td></tr><tr><td>eval/Precision</td><td>0.84663</td></tr><tr><td>eval/Recall</td><td>0.95172</td></tr><tr><td>eval/loss</td><td>0.10468</td></tr><tr><td>eval/mcc_metric</td><td>0.60317</td></tr><tr><td>eval/runtime</td><td>0.1316</td></tr><tr><td>eval/samples_per_second</td><td>1557.278</td></tr><tr><td>eval/steps_per_second</td><td>53.175</td></tr><tr><td>total_flos</td><td>46087928188200.0</td></tr><tr><td>train/epoch</td><td>12</td></tr><tr><td>train/global_step</td><td>624</td></tr><tr><td>train/grad_norm</td><td>0.27813</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.0565</td></tr><tr><td>train_loss</td><td>0.0758</td></tr><tr><td>train_runtime</td><td>30.0824</td></tr><tr><td>train_samples_per_second</td><td>1089.674</td></tr><tr><td>train_steps_per_second</td><td>34.572</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mild-sweep-7</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/l2kgceha' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/l2kgceha</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250409_131753-l2kgceha/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i1ivpbk3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.7529215617587703e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'BBBP focal loss Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/wandb/run-20250409_131834-i1ivpbk3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/i1ivpbk3' target=\"_blank\">swift-sweep-8</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/i1ivpbk3' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/i1ivpbk3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,127,234 || all params: 5,703,156 || trainable%: 37.2992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1816725/2257317317.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='624' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 624/1040 00:32 < 00:21, 19.22 it/s, Epoch 12/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.123461</td>\n",
       "      <td>0.141789</td>\n",
       "      <td>0.717073</td>\n",
       "      <td>0.903908</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.993103</td>\n",
       "      <td>0.832370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.118900</td>\n",
       "      <td>0.109060</td>\n",
       "      <td>0.354456</td>\n",
       "      <td>0.760976</td>\n",
       "      <td>0.916322</td>\n",
       "      <td>0.752632</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.853731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.118900</td>\n",
       "      <td>0.099718</td>\n",
       "      <td>0.524643</td>\n",
       "      <td>0.814634</td>\n",
       "      <td>0.919655</td>\n",
       "      <td>0.802260</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.095008</td>\n",
       "      <td>0.566391</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.921839</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.890282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.094694</td>\n",
       "      <td>0.606714</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.923218</td>\n",
       "      <td>0.830409</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.898734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.095799</td>\n",
       "      <td>0.604781</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.922874</td>\n",
       "      <td>0.834320</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.898089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.099468</td>\n",
       "      <td>0.618043</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.921264</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.900958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.100397</td>\n",
       "      <td>0.618043</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.920575</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.900958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.101185</td>\n",
       "      <td>0.616929</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.921494</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.900322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.104144</td>\n",
       "      <td>0.616929</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.921839</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.900322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.107718</td>\n",
       "      <td>0.589613</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.921609</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.958621</td>\n",
       "      <td>0.893891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.106326</td>\n",
       "      <td>0.589613</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.921954</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.958621</td>\n",
       "      <td>0.893891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_BBBP_chemberta_5M_MTR_FL/i1ivpbk3\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▇▇██▇▇▇▇▇█</td></tr><tr><td>eval/Accuracy</td><td>▁▃▆▇██████▇▇</td></tr><tr><td>eval/F1-score</td><td>▁▃▆▇██████▇▇</td></tr><tr><td>eval/Precision</td><td>▁▃▆▆▇███████</td></tr><tr><td>eval/Recall</td><td>█▇▅▅▅▄▄▄▂▂▁▁</td></tr><tr><td>eval/loss</td><td>█▄▂▁▁▁▂▂▃▃▄▄</td></tr><tr><td>eval/mcc_metric</td><td>▁▄▇▇████████</td></tr><tr><td>eval/runtime</td><td>▁▁▁▂▁▁▂▂▂█▂▂</td></tr><tr><td>eval/samples_per_second</td><td>██▇▇██▇▇▇▁▇▇</td></tr><tr><td>eval/steps_per_second</td><td>██▇▇██▇▇▇▁▇▇</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▁▇▆▁▃</td></tr><tr><td>train/learning_rate</td><td>█▇▅▄▂▁</td></tr><tr><td>train/loss</td><td>█▃▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.92195</td></tr><tr><td>eval/Accuracy</td><td>0.83902</td></tr><tr><td>eval/F1-score</td><td>0.89389</td></tr><tr><td>eval/Precision</td><td>0.83735</td></tr><tr><td>eval/Recall</td><td>0.95862</td></tr><tr><td>eval/loss</td><td>0.10633</td></tr><tr><td>eval/mcc_metric</td><td>0.58961</td></tr><tr><td>eval/runtime</td><td>0.1337</td></tr><tr><td>eval/samples_per_second</td><td>1533.244</td></tr><tr><td>eval/steps_per_second</td><td>52.355</td></tr><tr><td>total_flos</td><td>66061192294440.0</td></tr><tr><td>train/epoch</td><td>12</td></tr><tr><td>train/global_step</td><td>624</td></tr><tr><td>train/grad_norm</td><td>0.18878</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.0554</td></tr><tr><td>train_loss</td><td>0.07229</td></tr><tr><td>train_runtime</td><td>32.4173</td></tr><tr><td>train_samples_per_second</td><td>1011.187</td></tr><tr><td>train_steps_per_second</td><td>32.082</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swift-sweep-8</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/i1ivpbk3' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/i1ivpbk3</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250409_131834-i1ivpbk3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ftemtdss with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.6230969530912157e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'BBBP focal loss Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/wandb/run-20250409_131916-ftemtdss</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/ftemtdss' target=\"_blank\">legendary-sweep-9</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/ftemtdss' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/ftemtdss</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,127,234 || all params: 5,703,156 || trainable%: 37.2992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1816725/2257317317.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='832' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 832/1040 00:42 < 00:10, 19.34 it/s, Epoch 16/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.130386</td>\n",
       "      <td>0.174226</td>\n",
       "      <td>0.721951</td>\n",
       "      <td>0.896897</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.833819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>0.113728</td>\n",
       "      <td>0.275018</td>\n",
       "      <td>0.741463</td>\n",
       "      <td>0.913448</td>\n",
       "      <td>0.737113</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.843658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.127100</td>\n",
       "      <td>0.106086</td>\n",
       "      <td>0.422912</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.918736</td>\n",
       "      <td>0.768817</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.864048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.085900</td>\n",
       "      <td>0.100825</td>\n",
       "      <td>0.495823</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.921264</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.876543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.085900</td>\n",
       "      <td>0.097102</td>\n",
       "      <td>0.538740</td>\n",
       "      <td>0.819512</td>\n",
       "      <td>0.922529</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.884735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.095716</td>\n",
       "      <td>0.566391</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.924138</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.890282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.096027</td>\n",
       "      <td>0.566391</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.922184</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.890282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>0.096422</td>\n",
       "      <td>0.619893</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.921494</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.901587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>0.096447</td>\n",
       "      <td>0.619893</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.922184</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.901587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>0.097802</td>\n",
       "      <td>0.618043</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.922299</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.900958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>0.100163</td>\n",
       "      <td>0.632959</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.921839</td>\n",
       "      <td>0.840237</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.904459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>0.100514</td>\n",
       "      <td>0.631194</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.922069</td>\n",
       "      <td>0.844311</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.903846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>0.101593</td>\n",
       "      <td>0.631194</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.920920</td>\n",
       "      <td>0.844311</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.903846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.102364</td>\n",
       "      <td>0.631194</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.920575</td>\n",
       "      <td>0.844311</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.903846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.102876</td>\n",
       "      <td>0.631194</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.920115</td>\n",
       "      <td>0.844311</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.903846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>0.104379</td>\n",
       "      <td>0.631194</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.919770</td>\n",
       "      <td>0.844311</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.903846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_BBBP_chemberta_5M_MTR_FL/ftemtdss\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▅▇▇██▇▇▇█▇▇▇▇▇▇</td></tr><tr><td>eval/Accuracy</td><td>▁▂▄▅▆▇▇█████████</td></tr><tr><td>eval/F1-score</td><td>▁▂▄▅▆▇▇█████████</td></tr><tr><td>eval/Precision</td><td>▁▂▄▅▆▆▆▇▇███████</td></tr><tr><td>eval/Recall</td><td>███▅▅▅▅▅▅▁▅▁▁▁▁▁</td></tr><tr><td>eval/loss</td><td>█▅▃▂▁▁▁▁▁▁▂▂▂▂▂▃</td></tr><tr><td>eval/mcc_metric</td><td>▁▃▅▆▇▇▇█████████</td></tr><tr><td>eval/runtime</td><td>▁▁▃▁▁█▃▁▃▂▃▄▃▃▄▃</td></tr><tr><td>eval/samples_per_second</td><td>██▆██▁▆█▆▇▅▅▆▆▅▆</td></tr><tr><td>eval/steps_per_second</td><td>██▆██▁▆█▆▇▅▅▆▆▅▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>█▁▅▅▁▂▂█</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.91977</td></tr><tr><td>eval/Accuracy</td><td>0.85366</td></tr><tr><td>eval/F1-score</td><td>0.90385</td></tr><tr><td>eval/Precision</td><td>0.84431</td></tr><tr><td>eval/Recall</td><td>0.97241</td></tr><tr><td>eval/loss</td><td>0.10438</td></tr><tr><td>eval/mcc_metric</td><td>0.63119</td></tr><tr><td>eval/runtime</td><td>0.1339</td></tr><tr><td>eval/samples_per_second</td><td>1530.937</td></tr><tr><td>eval/steps_per_second</td><td>52.276</td></tr><tr><td>total_flos</td><td>87888560025168.0</td></tr><tr><td>train/epoch</td><td>16</td></tr><tr><td>train/global_step</td><td>832</td></tr><tr><td>train/grad_norm</td><td>0.23525</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0575</td></tr><tr><td>train_loss</td><td>0.07402</td></tr><tr><td>train_runtime</td><td>42.9773</td></tr><tr><td>train_samples_per_second</td><td>762.729</td></tr><tr><td>train_steps_per_second</td><td>24.199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">legendary-sweep-9</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/ftemtdss' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/ftemtdss</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250409_131916-ftemtdss/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kvx5oe5b with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlora_alpha: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.3848211420962031e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: ['adamw']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tr: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'BBBP focal loss Hyperparameter Tuning' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/qnap_home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/wandb/run-20250409_132008-kvx5oe5b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/kvx5oe5b' target=\"_blank\">divine-sweep-10</a></strong> to <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/sweeps/c918zhrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/kvx5oe5b' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/kvx5oe5b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 643,266 || all params: 4,108,404 || trainable%: 15.6573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1816725/2257317317.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='936' max='1040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 936/1040 00:45 < 00:05, 20.54 it/s, Epoch 18/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Metric</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc-roc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.131618</td>\n",
       "      <td>0.227441</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.914483</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.837758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.115527</td>\n",
       "      <td>0.296294</td>\n",
       "      <td>0.746341</td>\n",
       "      <td>0.925862</td>\n",
       "      <td>0.740933</td>\n",
       "      <td>0.986207</td>\n",
       "      <td>0.846154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.107759</td>\n",
       "      <td>0.332410</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.930230</td>\n",
       "      <td>0.751323</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.850299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.090500</td>\n",
       "      <td>0.103009</td>\n",
       "      <td>0.385941</td>\n",
       "      <td>0.770732</td>\n",
       "      <td>0.931264</td>\n",
       "      <td>0.763441</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.858006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.090500</td>\n",
       "      <td>0.099131</td>\n",
       "      <td>0.466037</td>\n",
       "      <td>0.795122</td>\n",
       "      <td>0.931494</td>\n",
       "      <td>0.784530</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.871166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.096897</td>\n",
       "      <td>0.510343</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.879257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.095968</td>\n",
       "      <td>0.510343</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.797753</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.879257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.095546</td>\n",
       "      <td>0.566391</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.929540</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.890282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.094824</td>\n",
       "      <td>0.593411</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.929885</td>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.979310</td>\n",
       "      <td>0.895899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.095199</td>\n",
       "      <td>0.604781</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.929195</td>\n",
       "      <td>0.834320</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.898089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.096654</td>\n",
       "      <td>0.604781</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.928736</td>\n",
       "      <td>0.834320</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.898089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.096717</td>\n",
       "      <td>0.604781</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.928736</td>\n",
       "      <td>0.834320</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.898089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.097406</td>\n",
       "      <td>0.618043</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.928621</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.900958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.098267</td>\n",
       "      <td>0.618043</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.928736</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.900958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.098722</td>\n",
       "      <td>0.618043</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.927816</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.900958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.099868</td>\n",
       "      <td>0.618043</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.927701</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.972414</td>\n",
       "      <td>0.900958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.099683</td>\n",
       "      <td>0.603605</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.927586</td>\n",
       "      <td>0.838323</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.897436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.100131</td>\n",
       "      <td>0.603605</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.927126</td>\n",
       "      <td>0.838323</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.897436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_BBBP_chemberta_5M_MTR_FL/kvx5oe5b\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>▁▆▇███▇▇▇▇▇▇▇▇▆▆▆▆</td></tr><tr><td>eval/Accuracy</td><td>▁▂▂▃▅▆▆▇▇█████████</td></tr><tr><td>eval/F1-score</td><td>▁▂▂▃▅▆▆▇▇█████████</td></tr><tr><td>eval/Precision</td><td>▁▂▂▃▄▅▅▆▇█████████</td></tr><tr><td>eval/Recall</td><td>▆█▆▆▆▆▆▆▆▃▃▃▃▃▃▃▁▁</td></tr><tr><td>eval/loss</td><td>█▅▃▃▂▁▁▁▁▁▁▁▁▂▂▂▂▂</td></tr><tr><td>eval/mcc_metric</td><td>▁▂▃▄▅▆▆▇██████████</td></tr><tr><td>eval/runtime</td><td>▁▃▂█▁▂▄▂▂▃▃▂▂▄▃▅▄▄</td></tr><tr><td>eval/samples_per_second</td><td>█▆▇▁█▇▅▇▇▆▆▇▇▅▆▄▅▅</td></tr><tr><td>eval/steps_per_second</td><td>█▆▇▁█▇▅▇▇▆▆▇▇▅▆▄▅▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▆▁█▃▂▂▁▇▂</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▄▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/AUC-ROC</td><td>0.92713</td></tr><tr><td>eval/Accuracy</td><td>0.8439</td></tr><tr><td>eval/F1-score</td><td>0.89744</td></tr><tr><td>eval/Precision</td><td>0.83832</td></tr><tr><td>eval/Recall</td><td>0.96552</td></tr><tr><td>eval/loss</td><td>0.10013</td></tr><tr><td>eval/mcc_metric</td><td>0.6036</td></tr><tr><td>eval/runtime</td><td>0.1319</td></tr><tr><td>eval/samples_per_second</td><td>1554.187</td></tr><tr><td>eval/steps_per_second</td><td>53.07</td></tr><tr><td>total_flos</td><td>68890114440720.0</td></tr><tr><td>train/epoch</td><td>18</td></tr><tr><td>train/global_step</td><td>936</td></tr><tr><td>train/grad_norm</td><td>0.21712</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0594</td></tr><tr><td>train_loss</td><td>0.0751</td></tr><tr><td>train_runtime</td><td>45.5273</td></tr><tr><td>train_samples_per_second</td><td>720.008</td></tr><tr><td>train_steps_per_second</td><td>22.843</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-sweep-10</strong> at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/kvx5oe5b' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface/runs/kvx5oe5b</a><br> View project at: <a href='https://wandb.ai/harodharsha21-iit-ropar/huggingface' target=\"_blank\">https://wandb.ai/harodharsha21-iit-ropar/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250409_132008-kvx5oe5b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_runtime': 51.083988233, '_step': 27, '_timestamp': 1744197660.0279956, '_wandb': {'runtime': 51}, 'eval/AUC-ROC': 0.9271264367816092, 'eval/Accuracy': 0.8439024390243902, 'eval/F1-score': 0.8974358974358975, 'eval/Precision': 0.8383233532934131, 'eval/Recall': 0.9655172413793104, 'eval/loss': 0.10013064742088318, 'eval/mcc_metric': 0.603604942410772, 'eval/runtime': 0.1319, 'eval/samples_per_second': 1554.187, 'eval/steps_per_second': 53.07, 'total_flos': 68890114440720, 'train/epoch': 18, 'train/global_step': 936, 'train/grad_norm': 0.2171177119016647, 'train/learning_rate': 1.8641823066679656e-06, 'train/loss': 0.0594, 'train_loss': 0.07510012744838356, 'train_runtime': 45.5273, 'train_samples_per_second': 720.008, 'train_steps_per_second': 22.843}\n",
      "Best hyperparameters: {'r': 128, 'lr': 1.1720367087103236e-05, 'bf16': False, 'fp16': False, 'fsdp': [], 'seed': 42, 'tf32': None, 'debug': [], 'optim': 'adamw_torch', 'top_k': 50, 'top_p': 1, 'is_gpu': True, 'prefix': None, 'do_eval': True, 'dropout': 0.2, 'no_cuda': False, 'tp_size': 0, 'use_cpu': False, 'do_train': False, 'id2label': {'0': 'LABEL_0', '1': 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'norm_std': [2.9210526350021033, 1.5294133532822063, 2.9209947673330334, 0.21956154740898992, 0.22097666681598951, 160.4856642380458, 151.38170855657367, 160.3304390667665, 60.484857692625106, 0.181038611279414, 0, 0, 0, 0, 0.24851193112366385, 0.317494124851492, 0.37175815103599535, 0.6098706561111424, 539.8195290502504, 8.140940922894863, 6.600767667198695, 6.700942921964325, 5.536318526756788, 4.020569431789569, 4.316039675035455, 3.229701298304296, 4.058753110098356, 2.399274478688092, 4.590084765547685, 1.8657465201411236, 8.197075845395899, 1.3989800795766576, 8.727770321711972, 4.719034225006412, 3.6844834579923407, 66.65125255607474, 11.022808176926915, 9.88512023443511, 5.895101555004671, 6.0315631910071374, 4.465786134186721, 8.73293454096314, 7.292192943139112, 5.798809757257198, 5.458840154330179, 5.34562222799046, 28.624753237838465, 22.7685485030176, 13.735506972569182, 12.75558914023291, 12.647297666063738, 16.73803715869515, 1.3236865505015507, 8.012917117258175, 6.328266302270954, 30.80439768300023, 14.510669158473307, 33.76748799216324, 0, 8.851153866015428, 8.222102882220607, 7.329351085680612, 4.87773057457412, 10.796349487508555, 24.55359833254403, 10.33295824604808, 8.986884190324291, 26.77991276665104, 29.521288543995215, 4.077418430037268, 11.23487898363004, 0, 50.277243284807206, 19.12173183245714, 9.819697177666312, 1.4201437981599128, 12.511435257208836, 14.212538029397628, 16.973978925056553, 19.21649041911615, 15.092240504961104, 19.889237093009676, 25.80872442073538, 9.254317550453823, 19.013243564373347, 3.6841568734614953, 17.690679185577395, 10.27595457263499, 3.3283202642652645, 2.8773795244438474, 9.228734822190496, 5.106296483962912, 4.008127533955226, 2.3345092198667503, 0.23958883840178577, 11.48532061063049, 2.0042680181777808, 3.411142707197923, 0.7103265443180337, 0.8009597262862117, 1.0630493791282618, 1.249503799091361, 0.8592211073826755, 1.4909738617970665, 2.8049912821495706, 1.5692082041123123, 3.718886071238216, 4.918753910447648, 0.6213838320183964, 0.6971589290933399, 0.9385507839118636, 1.7370945619837506, 2.7759468746763334, 43.91556441471313, 0.2929625321198007, 0.6742399816263887, 0.6447563579731193, 0.26136083143708466, 0.1703202147866646, 1.3696411924562566, 0.3394696140137124, 0.26977939457438505, 0.3350074869447194, 0.3408584597974497, 1.2690580420372088, 1.2684116362885036, 0.1297126917051003, 0.06304965563156611, 0.17914965229828922, 1.485673805113914, 1.1656052934139842, 0.5018632205797633, 0.15576643470973517, 0.2883562378800223, 0.3774901929558512, 0.3394696140137124, 0.07983606764988928, 0.1030741645577756, 0.11692041889415362, 1.0010868912132271, 0.7705779932112281, 1.157481598590082, 0.13507534533122212, 0.8359812306885952, 0.7600865243553028, 0.04757124327808961, 0.07183232513905516, 0.03513570421263404, 1.239225396368063, 0.015097985029438592, 1.3364349277900949, 0.013378265133341392, 0.032663541616103894, 0.060970137226002974, 0.44400840883756576, 1.159532265122051, 0.198246590935912, 0.1491817288215558, 1.28126795861232, 0.143114919141507, 0.11579880303510388, 0.25012811724209466, 0.1830406121462275, 0.03504726333553974, 0.015295758691880374, 0.3034514997274073, 0.2749689545601939, 0.04859983910409953, 0.09878498419533764, 0.5707110234042025, 0.17028898672063034, 0.24456026600763192, 0.21322057789532145, 0.1917343827305721, 0.13591391704896466, 0.03519702423260403, 0.1108018278371122, 0.0680510883818226, 0.5264724473438641, 0.2602735481879015, 0.25847912916802446, 0.10886360159063148, 0.1002693464072736, 0.35113436163289397, 0.2260341350934195, 0.16874580630684471, 0, 0.4146998571400424, 0.5347143492505464, 0.3137422508894841, 0.27962501103110715, 0.1547563582555832, 0.08130444916739461, 0.08949068223889126, 0.225304925348536, 0.014421012861987593, 0.2736413019822887, 2.253629375384596, 0.22817317920167496], 'run_name': './models_BBBP_chemberta_5M_MTR_FL/syqmo6uu', 'use_ipex': False, 'adafactor': False, 'data_seed': None, 'deepspeed': None, 'do_sample': False, 'hub_token': '<HUB_TOKEN>', 'log_level': 'passive', 'max_steps': -1, 'norm_mean': [11.199569164274653, -0.9728601944583676, 11.199595401578872, 0.1914454376660732, 0.608589373135307, 365.064017672, 342.24912812000014, 364.6033136038417, 134.06547, 0.004249, 0, 0, 0, 0, 1.1861084842221647, 1.890967178564785, 2.519587985439997, 2.0112818114267816, 795.5621221754437, 18.14439203724506, 14.536240385432391, 15.215140271072489, 12.068994414289726, 8.453657900068215, 9.114162139055054, 6.434168605708085, 7.215103879809845, 4.436200487997215, 5.109730699855831, 3.055231525907226, 3.6252747118486264, -2.202564923376624, 18.195385007867852, 7.970699358994477, 4.5379164631837545, 150.95250337667272, 13.184208966483704, 8.814008658052902, 3.81918390789873, 3.4969386790830774, 2.9222201316693712, 2.644444123964607, 6.408740449956927, 4.95314480536345, 2.6263770771853108, 2.4113616526384853, 26.24052195128434, 37.102909834641714, 19.89943953042712, 16.353848799228413, 15.638332143998122, 21.706094849865753, 0.28727529762970366, 8.054432014422119, 3.2648099385428853, 32.629006626588726, 16.26551059790217, 47.70605007162041, 0, 5.325837027308287, 9.698460925314944, 5.573601891254677, 2.581492771453006, 7.312496194388467, 33.07539073817076, 10.718462271839512, 6.99277406210818, 31.684923475431933, 36.92162447084414, 1.2074202610211655, 5.110701506051421, 0, 71.04050338999998, 9.57750975344203, 10.066085526965992, 0.07691213090851719, 13.38923196114951, 16.862422387837878, 21.382953923695233, 15.651918121909311, 14.440634953378058, 19.13130604146014, 22.114944705243296, 8.183429061888226, 13.699768012021506, 2.1212691930096144, 17.474216494453906, 7.846769617492272, 2.6683841482907034, 0.11868201225906092, 9.064881467380092, 2.659801877718109, 4.055917032498944, 0.259848432909807, 0.413963629624058, 25.186704, 1.79722, 5.353545, 0.272499, 0.562898, 0.835397, 1.236854, 0.729917, 1.966771, 4.216321, 1.414081, 6.486208, 5.688314, 0.205632, 0.409204, 0.614836, 2.802168, 2.7549044689500004, 97.31541557350002, 0.069051, 0.151924, 0.130758, 0.06279, 0.027038, 0.999062, 0.096951, 0.042862, 0.096089, 0.100163, 1.033857, 1.034286, 0.016206, 0.00357, 0.016776, 1.488795, 0.915699, 0.232236, 0.012241, 0.074885, 0.131561, 0.096951, 0.004026, 0.009835, 0.011646, 0.250196, 0.131237, 0.768633, 0.015927, 0.539599, 0.451885, 0.001726, 0.003335, 0.001218, 1.236474, 0.000226, 0.555529, 0.000149, 0.001046, 0.002578, 0.126995, 0.732216, 0.037978, 0.019179, 0.720141, 0.018951, 0.013025, 0.059523, 0.027553, 0.000831, 0.0002, 0.073914, 0.061694, 0.002249, 0.007716, 0.236426, 0.0287, 0.05231, 0.041425, 0.033421, 0.017275, 0.001082, 0.011915, 0.004249, 0.196769, 0.039316, 0.038686, 0.00409, 0.003615, 0.116124, 0.051192, 0.025177, 0, 0.161908, 0.315775, 0.087229, 0.079586, 0.023227, 0.005966, 0.007901, 0.050376, 0.000186, 0.065723, 0.380193, 0.051566], 'num_beams': 1, 'optimizer': ['adamw'], 'ray_scope': 'last', 'report_to': ['wandb'], 'typical_p': 1, 'use_cache': True, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'do_predict': False, 'eval_delay': 0, 'eval_steps': None, 'hidden_act': 'gelu', 'is_decoder': False, 'local_rank': 0, 'lora_alpha': 32, 'max_length': 20, 'min_length': 0, 'model_type': 'roberta', 'optim_args': None, 'output_dir': './models_BBBP_chemberta_5M_MTR_FL/syqmo6uu', 'past_index': -1, 'save_steps': 500, 'vocab_size': 600, 'ddp_backend': None, 'ddp_timeout': 1800, 'fsdp_config': {'xla': False, 'xla_fsdp_v2': False, 'min_num_params': 0, 'xla_fsdp_grad_ckpt': False}, 'hidden_size': 384, 'label_names': None, 'logging_dir': './logs_BBBP/syqmo6uu', 'peft_config': {'default': {'r': 128, 'bias': 'none', 'revision': None, 'use_dora': False, 'lora_bias': False, 'peft_type': 'LORA', 'task_type': 'SEQ_CLS', 'eva_config': None, 'lora_alpha': 32, 'use_rslora': False, 'auto_mapping': None, 'lora_dropout': 0.2, 'megatron_core': 'megatron.core', 'fan_in_fan_out': False, 'inference_mode': False, 'layers_pattern': None, 'runtime_config': {'ephemeral_gpu_offload': False}, 'target_modules': ['value', 'out_proj', 'dense', 'key', 'query'], 'exclude_modules': None, 'megatron_config': None, 'modules_to_save': ['classifier', 'score'], 'init_lora_weights': True, 'layer_replication': None, 'layers_to_transform': None, 'base_model_name_or_path': 'DeepChem/ChemBERTa-5M-MTR'}}, 'push_to_hub': False, 'return_dict': True, 'temperature': 1, 'torch_dtype': 'float32', 'torchdynamo': None, 'torchscript': False, 'adam_epsilon': 1e-08, 'bos_token_id': 0, 'disable_tqdm': False, 'eos_token_id': 2, 'fp16_backend': 'auto', 'hub_model_id': None, 'hub_strategy': 'every_save', 'pad_token_id': 1, 'problem_type': None, 'sep_token_id': None, 'use_bfloat16': False, 'warmup_ratio': 0, 'warmup_steps': 0, 'weight_decay': 0.01, '_name_or_path': 'DeepChem/ChemBERTa-5M-MTR', 'architectures': ['RobertaForRegression'], 'bad_words_ids': None, 'eval_on_start': False, 'eval_strategy': 'epoch', 'jit_mode_eval': False, 'learning_rate': 1.1720367087103236e-05, 'logging_steps': 100, 'max_grad_norm': 1, 'mp_parameters': '', 'output_scores': False, 'save_strategy': 'epoch', 'split_batches': None, 'torch_compile': False, 'tpu_num_cores': None, 'bf16_full_eval': False, 'early_stopping': False, 'fp16_full_eval': False, 'fp16_opt_level': 'O1', 'layer_norm_eps': 1e-12, 'length_penalty': 1, 'tf_legacy_loss': False, 'use_mps_device': False, 'finetuning_task': None, 'group_by_length': False, 'hub_always_push': False, 'num_beam_groups': 1, 'save_only_model': False, 'suppress_tokens': None, 'tokenizer_class': None, 'type_vocab_size': 1, 'dispatch_batches': None, 'full_determinism': False, 'hub_private_repo': None, 'ignore_data_skip': False, 'log_on_each_node': True, 'logging_strategy': 'steps', 'num_train_epochs': 20, 'save_safetensors': True, 'save_total_limit': 10, 'use_liger_kernel': False, 'ddp_bucket_cap_mb': None, 'diversity_penalty': 0, 'greater_is_better': True, 'initializer_range': 0.02, 'intermediate_size': 464, 'log_level_replica': 'warning', 'lr_scheduler_type': 'linear', 'num_hidden_layers': 3, 'output_attentions': False, 'push_to_hub_token': '<PUSH_TO_HUB_TOKEN>', 'save_on_each_node': False, 'tpu_metrics_debug': False, 'accelerator_config': {'even_batches': True, 'non_blocking': False, 'split_batches': False, 'dispatch_batches': None, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None}, 'batch_eval_metrics': False, 'classifier_dropout': None, 'is_encoder_decoder': False, 'length_column_name': 'length', 'logging_first_step': False, 'repetition_penalty': 1, 'torch_compile_mode': None, 'add_cross_attention': False, 'evaluation_strategy': 'epoch', 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'fsdp_min_num_params': 0, 'hidden_dropout_prob': 0.144, 'include_for_metrics': [], 'neftune_noise_alpha': None, 'num_attention_heads': 12, 'skip_memory_metrics': True, 'tie_encoder_decoder': False, 'tie_word_embeddings': True, 'auto_find_batch_size': False, 'dataloader_drop_last': False, 'model/num_parameters': 5703156, 'no_repeat_ngram_size': 0, 'num_return_sequences': 1, 'optim_target_modules': None, 'output_hidden_states': False, 'overwrite_output_dir': False, 'prediction_loss_only': False, 'push_to_hub_model_id': None, 'task_specific_params': None, 'transformers_version': '4.50.0', 'begin_suppress_tokens': None, 'dataloader_pin_memory': True, 'ddp_broadcast_buffers': None, 'metric_for_best_model': 'eval_mcc_metric', 'remove_invalid_values': False, 'remove_unused_columns': True, 'torch_compile_backend': None, 'dataloader_num_workers': 0, 'decoder_start_token_id': None, 'eval_do_concat_batches': True, 'eval_use_gather_object': False, 'gradient_checkpointing': False, 'half_precision_backend': 'auto', 'label_smoothing_factor': 0, 'load_best_model_at_end': True, 'logging_nan_inf_filter': True, 'resume_from_checkpoint': None, 'chunk_size_feed_forward': 0, 'eval_accumulation_steps': None, 'max_position_embeddings': 515, 'per_gpu_eval_batch_size': None, 'position_embedding_type': 'absolute', 'return_dict_in_generate': False, 'torch_empty_cache_steps': None, 'per_gpu_train_batch_size': None, 'push_to_hub_organization': None, 'include_tokens_per_second': False, 'dataloader_prefetch_factor': None, 'ddp_find_unused_parameters': None, 'include_inputs_for_metrics': False, 'per_device_eval_batch_size': 32, 'use_legacy_prediction_loop': False, 'cross_attention_hidden_size': None, 'gradient_accumulation_steps': 1, 'per_device_train_batch_size': 32, '_attn_implementation_autoset': True, 'attention_probs_dropout_prob': 0.109, 'encoder_no_repeat_ngram_size': 0, 'average_tokens_across_devices': False, 'dataloader_persistent_workers': False, 'gradient_checkpointing_kwargs': None, 'include_num_input_tokens_seen': False, 'exponential_decay_length_penalty': None, 'fsdp_transformer_layer_cls_to_wrap': None, 'restore_callback_states_from_checkpoint': False}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    sweep_config = {\n",
    "    \"name\": \"BBBP Chemberta Hyperparameter Tuning\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\n",
    "        \"goal\": \"maximize\", \n",
    "        \"name\": \"eval/mcc_metric\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 1e-5,  \n",
    "            \"max\": 2e-5\n",
    "        },\n",
    "        \"r\": {\n",
    "            \"values\": [8,16,32,64, 128]\n",
    "        },\n",
    "        \"lora_alpha\": {\n",
    "            \"values\": [8,16,32,64,128]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"values\": [0.0,0.1,0.2]\n",
    "        },\n",
    "        \n",
    "        \"optimizer\": {\n",
    "            \"value\": [\"adamw\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"huggingface\")\n",
    "    wandb.agent(sweep_id, function=run_training, count=10)\n",
    "\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(f\"huggingface/{sweep_id}\")\n",
    "    print(sweep.runs[0].summary_metrics)\n",
    "\n",
    "    runs_with_rmse = [run for run in sweep.runs if 'eval/mcc_metric' in run.summary_metrics]\n",
    "    if runs_with_rmse:\n",
    "        # Sort by rmse in descending order (maximize)\n",
    "        best_run = sorted(runs_with_rmse, key=lambda run: run.summary_metrics['eval/mcc_metric'])[0]\n",
    "    else:\n",
    "        raise ValueError(\"No runs found with 'eval/mcc_metric' metric.\")\n",
    "\n",
    "    best_hyperparameters = best_run.config\n",
    "    print(f\"Best hyperparameters: {best_hyperparameters}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b00034e",
   "metadata": {},
   "source": [
    "Evaluate on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32580646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Ensure deterministic behavior in PyTorch computations\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8c9c0b",
   "metadata": {},
   "source": [
    "## 77M MTR Model Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5a8828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-5M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-5M-MTR\",\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True,\n",
    "    \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-5M-MTR\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7bd3d8",
   "metadata": {},
   "source": [
    "## 77M MTR Weighted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07a40569",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 77M MTR Weighted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49b646fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data=pd.read_csv('/home/raghvendra2/Molformer_Finetuning/BBBP_Prediction_Task/bbbp/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "993458a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "smiles_test = test_data['smiles'].tolist()\n",
    "\n",
    "test_tokenized =tokenizer(smiles_test)\n",
    "\n",
    "test_dataset = Dataset.from_dict(test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32c00d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_data['p_np'].tolist() \n",
    "\n",
    "\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56c8f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "accuracy_metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probabilities = softmax(logits, axis=1)[:, 1]  # Get probabilities for class 1\n",
    "        predictions = np.argmax(logits, axis=1)  # Choose the most likely class\n",
    "        mcc = matthews_corrcoef(labels, predictions)\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"eval_mcc_metric\": mcc,\n",
    "            \"Accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "            \"AUC-ROC\": roc_auc_score(labels, probabilities),  # AUC-ROC requires probabilities\n",
    "            \"Precision\": precision_score(labels, predictions),\n",
    "            \"Recall\": recall_score(labels, predictions),\n",
    "            \"F1-score\": f1_score(labels, predictions)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cc90789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"./test_results_clintox_wandb\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    report_to=\"none\",  # Disable logging to W&B for test\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71767d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Valid nested checkpoints found: ['./models_BBBP_chemberta_5M_MTR_FL/syqmo6uu/checkpoint-884', './models_BBBP_chemberta_5M_MTR_FL/syqmo6uu/checkpoint-468', './models_BBBP_chemberta_5M_MTR_FL/syqmo6uu/checkpoint-728', './models_BBBP_chemberta_5M_MTR_FL/syqmo6uu/checkpoint-572', './models_BBBP_chemberta_5M_MTR_FL/syqmo6uu/checkpoint-676', './models_BBBP_chemberta_5M_MTR_FL/syqmo6uu/checkpoint-832', './models_BBBP_chemberta_5M_MTR_FL/syqmo6uu/checkpoint-624', './models_BBBP_chemberta_5M_MTR_FL/syqmo6uu/checkpoint-780', './models_BBBP_chemberta_5M_MTR_FL/syqmo6uu/checkpoint-416', './models_BBBP_chemberta_5M_MTR_FL/syqmo6uu/checkpoint-520', './models_BBBP_chemberta_5M_MTR_FL/ftemtdss/checkpoint-468', './models_BBBP_chemberta_5M_MTR_FL/ftemtdss/checkpoint-728', './models_BBBP_chemberta_5M_MTR_FL/ftemtdss/checkpoint-572', './models_BBBP_chemberta_5M_MTR_FL/ftemtdss/checkpoint-676', './models_BBBP_chemberta_5M_MTR_FL/ftemtdss/checkpoint-832', './models_BBBP_chemberta_5M_MTR_FL/ftemtdss/checkpoint-624', './models_BBBP_chemberta_5M_MTR_FL/ftemtdss/checkpoint-780', './models_BBBP_chemberta_5M_MTR_FL/ftemtdss/checkpoint-364', './models_BBBP_chemberta_5M_MTR_FL/ftemtdss/checkpoint-416', './models_BBBP_chemberta_5M_MTR_FL/ftemtdss/checkpoint-520', './models_BBBP_chemberta_5M_MTR_FL/8u9ogv87/checkpoint-468', './models_BBBP_chemberta_5M_MTR_FL/8u9ogv87/checkpoint-728', './models_BBBP_chemberta_5M_MTR_FL/8u9ogv87/checkpoint-572', './models_BBBP_chemberta_5M_MTR_FL/8u9ogv87/checkpoint-676', './models_BBBP_chemberta_5M_MTR_FL/8u9ogv87/checkpoint-624', './models_BBBP_chemberta_5M_MTR_FL/8u9ogv87/checkpoint-312', './models_BBBP_chemberta_5M_MTR_FL/8u9ogv87/checkpoint-364', './models_BBBP_chemberta_5M_MTR_FL/8u9ogv87/checkpoint-416', './models_BBBP_chemberta_5M_MTR_FL/8u9ogv87/checkpoint-260', './models_BBBP_chemberta_5M_MTR_FL/8u9ogv87/checkpoint-520', './models_BBBP_chemberta_5M_MTR_FL/4rr2blnx/checkpoint-884', './models_BBBP_chemberta_5M_MTR_FL/4rr2blnx/checkpoint-728', './models_BBBP_chemberta_5M_MTR_FL/4rr2blnx/checkpoint-572', './models_BBBP_chemberta_5M_MTR_FL/4rr2blnx/checkpoint-676', './models_BBBP_chemberta_5M_MTR_FL/4rr2blnx/checkpoint-832', './models_BBBP_chemberta_5M_MTR_FL/4rr2blnx/checkpoint-624', './models_BBBP_chemberta_5M_MTR_FL/4rr2blnx/checkpoint-780', './models_BBBP_chemberta_5M_MTR_FL/4rr2blnx/checkpoint-988', './models_BBBP_chemberta_5M_MTR_FL/4rr2blnx/checkpoint-936', './models_BBBP_chemberta_5M_MTR_FL/4rr2blnx/checkpoint-520', './models_BBBP_chemberta_5M_MTR_FL/l2kgceha/checkpoint-468', './models_BBBP_chemberta_5M_MTR_FL/l2kgceha/checkpoint-572', './models_BBBP_chemberta_5M_MTR_FL/l2kgceha/checkpoint-208', './models_BBBP_chemberta_5M_MTR_FL/l2kgceha/checkpoint-624', './models_BBBP_chemberta_5M_MTR_FL/l2kgceha/checkpoint-312', './models_BBBP_chemberta_5M_MTR_FL/l2kgceha/checkpoint-364', './models_BBBP_chemberta_5M_MTR_FL/l2kgceha/checkpoint-416', './models_BBBP_chemberta_5M_MTR_FL/l2kgceha/checkpoint-260', './models_BBBP_chemberta_5M_MTR_FL/l2kgceha/checkpoint-156', './models_BBBP_chemberta_5M_MTR_FL/l2kgceha/checkpoint-520', './models_BBBP_chemberta_5M_MTR_FL/kvx5oe5b/checkpoint-884', './models_BBBP_chemberta_5M_MTR_FL/kvx5oe5b/checkpoint-468', './models_BBBP_chemberta_5M_MTR_FL/kvx5oe5b/checkpoint-728', './models_BBBP_chemberta_5M_MTR_FL/kvx5oe5b/checkpoint-572', './models_BBBP_chemberta_5M_MTR_FL/kvx5oe5b/checkpoint-676', './models_BBBP_chemberta_5M_MTR_FL/kvx5oe5b/checkpoint-832', './models_BBBP_chemberta_5M_MTR_FL/kvx5oe5b/checkpoint-624', './models_BBBP_chemberta_5M_MTR_FL/kvx5oe5b/checkpoint-780', './models_BBBP_chemberta_5M_MTR_FL/kvx5oe5b/checkpoint-936', './models_BBBP_chemberta_5M_MTR_FL/kvx5oe5b/checkpoint-520', './models_BBBP_chemberta_5M_MTR_FL/h9wlrd8f/checkpoint-468', './models_BBBP_chemberta_5M_MTR_FL/h9wlrd8f/checkpoint-104', './models_BBBP_chemberta_5M_MTR_FL/h9wlrd8f/checkpoint-572', './models_BBBP_chemberta_5M_MTR_FL/h9wlrd8f/checkpoint-208', './models_BBBP_chemberta_5M_MTR_FL/h9wlrd8f/checkpoint-312', './models_BBBP_chemberta_5M_MTR_FL/h9wlrd8f/checkpoint-364', './models_BBBP_chemberta_5M_MTR_FL/h9wlrd8f/checkpoint-416', './models_BBBP_chemberta_5M_MTR_FL/h9wlrd8f/checkpoint-260', './models_BBBP_chemberta_5M_MTR_FL/h9wlrd8f/checkpoint-156', './models_BBBP_chemberta_5M_MTR_FL/h9wlrd8f/checkpoint-520', './models_BBBP_chemberta_5M_MTR_FL/m9mf2oqr/checkpoint-468', './models_BBBP_chemberta_5M_MTR_FL/m9mf2oqr/checkpoint-572', './models_BBBP_chemberta_5M_MTR_FL/m9mf2oqr/checkpoint-676', './models_BBBP_chemberta_5M_MTR_FL/m9mf2oqr/checkpoint-208', './models_BBBP_chemberta_5M_MTR_FL/m9mf2oqr/checkpoint-624', './models_BBBP_chemberta_5M_MTR_FL/m9mf2oqr/checkpoint-312', './models_BBBP_chemberta_5M_MTR_FL/m9mf2oqr/checkpoint-364', './models_BBBP_chemberta_5M_MTR_FL/m9mf2oqr/checkpoint-416', './models_BBBP_chemberta_5M_MTR_FL/m9mf2oqr/checkpoint-260', './models_BBBP_chemberta_5M_MTR_FL/m9mf2oqr/checkpoint-520', './models_BBBP_chemberta_5M_MTR_FL/i1ivpbk3/checkpoint-468', './models_BBBP_chemberta_5M_MTR_FL/i1ivpbk3/checkpoint-572', './models_BBBP_chemberta_5M_MTR_FL/i1ivpbk3/checkpoint-208', './models_BBBP_chemberta_5M_MTR_FL/i1ivpbk3/checkpoint-624', './models_BBBP_chemberta_5M_MTR_FL/i1ivpbk3/checkpoint-312', './models_BBBP_chemberta_5M_MTR_FL/i1ivpbk3/checkpoint-364', './models_BBBP_chemberta_5M_MTR_FL/i1ivpbk3/checkpoint-416', './models_BBBP_chemberta_5M_MTR_FL/i1ivpbk3/checkpoint-260', './models_BBBP_chemberta_5M_MTR_FL/i1ivpbk3/checkpoint-156', './models_BBBP_chemberta_5M_MTR_FL/i1ivpbk3/checkpoint-520', './models_BBBP_chemberta_5M_MTR_FL/kwtwjebf/checkpoint-468', './models_BBBP_chemberta_5M_MTR_FL/kwtwjebf/checkpoint-728', './models_BBBP_chemberta_5M_MTR_FL/kwtwjebf/checkpoint-572', './models_BBBP_chemberta_5M_MTR_FL/kwtwjebf/checkpoint-676', './models_BBBP_chemberta_5M_MTR_FL/kwtwjebf/checkpoint-624', './models_BBBP_chemberta_5M_MTR_FL/kwtwjebf/checkpoint-312', './models_BBBP_chemberta_5M_MTR_FL/kwtwjebf/checkpoint-364', './models_BBBP_chemberta_5M_MTR_FL/kwtwjebf/checkpoint-416', './models_BBBP_chemberta_5M_MTR_FL/kwtwjebf/checkpoint-260', './models_BBBP_chemberta_5M_MTR_FL/kwtwjebf/checkpoint-520']\n",
      "\n",
      "🔍 Evaluating model: syqmo6uu/checkpoint-884\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5/13 00:00 < 00:00, 97.10 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping syqmo6uu/checkpoint-884\n",
      "\n",
      "🔍 Evaluating model: syqmo6uu/checkpoint-468\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping syqmo6uu/checkpoint-468\n",
      "\n",
      "🔍 Evaluating model: syqmo6uu/checkpoint-728\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping syqmo6uu/checkpoint-728\n",
      "\n",
      "🔍 Evaluating model: syqmo6uu/checkpoint-572\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping syqmo6uu/checkpoint-572\n",
      "\n",
      "🔍 Evaluating model: syqmo6uu/checkpoint-676\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping syqmo6uu/checkpoint-676\n",
      "\n",
      "🔍 Evaluating model: syqmo6uu/checkpoint-832\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping syqmo6uu/checkpoint-832\n",
      "\n",
      "🔍 Evaluating model: syqmo6uu/checkpoint-624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping syqmo6uu/checkpoint-624\n",
      "\n",
      "🔍 Evaluating model: syqmo6uu/checkpoint-780\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping syqmo6uu/checkpoint-780\n",
      "\n",
      "🔍 Evaluating model: syqmo6uu/checkpoint-416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping syqmo6uu/checkpoint-416\n",
      "\n",
      "🔍 Evaluating model: syqmo6uu/checkpoint-520\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping syqmo6uu/checkpoint-520\n",
      "\n",
      "🔍 Evaluating model: ftemtdss/checkpoint-468\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping ftemtdss/checkpoint-468\n",
      "\n",
      "🔍 Evaluating model: ftemtdss/checkpoint-728\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping ftemtdss/checkpoint-728\n",
      "\n",
      "🔍 Evaluating model: ftemtdss/checkpoint-572\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping ftemtdss/checkpoint-572\n",
      "\n",
      "🔍 Evaluating model: ftemtdss/checkpoint-676\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping ftemtdss/checkpoint-676\n",
      "\n",
      "🔍 Evaluating model: ftemtdss/checkpoint-832\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping ftemtdss/checkpoint-832\n",
      "\n",
      "🔍 Evaluating model: ftemtdss/checkpoint-624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping ftemtdss/checkpoint-624\n",
      "\n",
      "🔍 Evaluating model: ftemtdss/checkpoint-780\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping ftemtdss/checkpoint-780\n",
      "\n",
      "🔍 Evaluating model: ftemtdss/checkpoint-364\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping ftemtdss/checkpoint-364\n",
      "\n",
      "🔍 Evaluating model: ftemtdss/checkpoint-416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping ftemtdss/checkpoint-416\n",
      "\n",
      "🔍 Evaluating model: ftemtdss/checkpoint-520\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping ftemtdss/checkpoint-520\n",
      "\n",
      "🔍 Evaluating model: 8u9ogv87/checkpoint-468\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8u9ogv87/checkpoint-468\n",
      "\n",
      "🔍 Evaluating model: 8u9ogv87/checkpoint-728\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8u9ogv87/checkpoint-728\n",
      "\n",
      "🔍 Evaluating model: 8u9ogv87/checkpoint-572\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8u9ogv87/checkpoint-572\n",
      "\n",
      "🔍 Evaluating model: 8u9ogv87/checkpoint-676\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8u9ogv87/checkpoint-676\n",
      "\n",
      "🔍 Evaluating model: 8u9ogv87/checkpoint-624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8u9ogv87/checkpoint-624\n",
      "\n",
      "🔍 Evaluating model: 8u9ogv87/checkpoint-312\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8u9ogv87/checkpoint-312\n",
      "\n",
      "🔍 Evaluating model: 8u9ogv87/checkpoint-364\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8u9ogv87/checkpoint-364\n",
      "\n",
      "🔍 Evaluating model: 8u9ogv87/checkpoint-416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8u9ogv87/checkpoint-416\n",
      "\n",
      "🔍 Evaluating model: 8u9ogv87/checkpoint-260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8u9ogv87/checkpoint-260\n",
      "\n",
      "🔍 Evaluating model: 8u9ogv87/checkpoint-520\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 8u9ogv87/checkpoint-520\n",
      "\n",
      "🔍 Evaluating model: 4rr2blnx/checkpoint-884\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 4rr2blnx/checkpoint-884\n",
      "\n",
      "🔍 Evaluating model: 4rr2blnx/checkpoint-728\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 4rr2blnx/checkpoint-728\n",
      "\n",
      "🔍 Evaluating model: 4rr2blnx/checkpoint-572\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 4rr2blnx/checkpoint-572\n",
      "\n",
      "🔍 Evaluating model: 4rr2blnx/checkpoint-676\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 4rr2blnx/checkpoint-676\n",
      "\n",
      "🔍 Evaluating model: 4rr2blnx/checkpoint-832\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 4rr2blnx/checkpoint-832\n",
      "\n",
      "🔍 Evaluating model: 4rr2blnx/checkpoint-624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 4rr2blnx/checkpoint-624\n",
      "\n",
      "🔍 Evaluating model: 4rr2blnx/checkpoint-780\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 4rr2blnx/checkpoint-780\n",
      "\n",
      "🔍 Evaluating model: 4rr2blnx/checkpoint-988\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 4rr2blnx/checkpoint-988\n",
      "\n",
      "🔍 Evaluating model: 4rr2blnx/checkpoint-936\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 4rr2blnx/checkpoint-936\n",
      "\n",
      "🔍 Evaluating model: 4rr2blnx/checkpoint-520\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping 4rr2blnx/checkpoint-520\n",
      "\n",
      "🔍 Evaluating model: l2kgceha/checkpoint-468\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.90 for l2kgceha/checkpoint-468\n",
      "📌 Test Results: {'eval_mcc_metric': 0.6854804887548676, 'eval_loss': 0.07313016802072525, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8556701030927835, 'eval_AUC-ROC': 0.960838199931295, 'eval_Precision': 0.841726618705036, 'eval_Recall': 0.9512195121951219, 'eval_F1-score': 0.8931297709923665, 'eval_runtime': 0.1676, 'eval_samples_per_second': 1157.639, 'eval_steps_per_second': 77.574}\n",
      "\n",
      "🔍 Evaluating model: l2kgceha/checkpoint-572\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.90 for l2kgceha/checkpoint-572\n",
      "📌 Test Results: {'eval_mcc_metric': 0.6967618094157277, 'eval_loss': 0.0733150914311409, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8608247422680413, 'eval_AUC-ROC': 0.9606091835566243, 'eval_Precision': 0.8478260869565217, 'eval_Recall': 0.9512195121951219, 'eval_F1-score': 0.896551724137931, 'eval_runtime': 0.1664, 'eval_samples_per_second': 1166.13, 'eval_steps_per_second': 78.143}\n",
      "\n",
      "🔍 Evaluating model: l2kgceha/checkpoint-208\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping l2kgceha/checkpoint-208\n",
      "\n",
      "🔍 Evaluating model: l2kgceha/checkpoint-624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.90 for l2kgceha/checkpoint-624\n",
      "📌 Test Results: {'eval_mcc_metric': 0.6967618094157277, 'eval_loss': 0.0720517635345459, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8608247422680413, 'eval_AUC-ROC': 0.9620977899919845, 'eval_Precision': 0.8478260869565217, 'eval_Recall': 0.9512195121951219, 'eval_F1-score': 0.896551724137931, 'eval_runtime': 0.1699, 'eval_samples_per_second': 1141.757, 'eval_steps_per_second': 76.51}\n",
      "\n",
      "🔍 Evaluating model: l2kgceha/checkpoint-312\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping l2kgceha/checkpoint-312\n",
      "\n",
      "🔍 Evaluating model: l2kgceha/checkpoint-364\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping l2kgceha/checkpoint-364\n",
      "\n",
      "🔍 Evaluating model: l2kgceha/checkpoint-416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping l2kgceha/checkpoint-416\n",
      "\n",
      "🔍 Evaluating model: l2kgceha/checkpoint-260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping l2kgceha/checkpoint-260\n",
      "\n",
      "🔍 Evaluating model: l2kgceha/checkpoint-156\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping l2kgceha/checkpoint-156\n",
      "\n",
      "🔍 Evaluating model: l2kgceha/checkpoint-520\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.90 for l2kgceha/checkpoint-520\n",
      "📌 Test Results: {'eval_mcc_metric': 0.6967618094157277, 'eval_loss': 0.07268020510673523, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8608247422680413, 'eval_AUC-ROC': 0.9616397572426427, 'eval_Precision': 0.8478260869565217, 'eval_Recall': 0.9512195121951219, 'eval_F1-score': 0.896551724137931, 'eval_runtime': 0.1654, 'eval_samples_per_second': 1172.846, 'eval_steps_per_second': 78.593}\n",
      "\n",
      "🔍 Evaluating model: kvx5oe5b/checkpoint-884\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kvx5oe5b/checkpoint-884\n",
      "\n",
      "🔍 Evaluating model: kvx5oe5b/checkpoint-468\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kvx5oe5b/checkpoint-468\n",
      "\n",
      "🔍 Evaluating model: kvx5oe5b/checkpoint-728\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kvx5oe5b/checkpoint-728\n",
      "\n",
      "🔍 Evaluating model: kvx5oe5b/checkpoint-572\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kvx5oe5b/checkpoint-572\n",
      "\n",
      "🔍 Evaluating model: kvx5oe5b/checkpoint-676\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kvx5oe5b/checkpoint-676\n",
      "\n",
      "🔍 Evaluating model: kvx5oe5b/checkpoint-832\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kvx5oe5b/checkpoint-832\n",
      "\n",
      "🔍 Evaluating model: kvx5oe5b/checkpoint-624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kvx5oe5b/checkpoint-624\n",
      "\n",
      "🔍 Evaluating model: kvx5oe5b/checkpoint-780\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kvx5oe5b/checkpoint-780\n",
      "\n",
      "🔍 Evaluating model: kvx5oe5b/checkpoint-936\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kvx5oe5b/checkpoint-936\n",
      "\n",
      "🔍 Evaluating model: kvx5oe5b/checkpoint-520\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kvx5oe5b/checkpoint-520\n",
      "\n",
      "🔍 Evaluating model: h9wlrd8f/checkpoint-468\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping h9wlrd8f/checkpoint-468\n",
      "\n",
      "🔍 Evaluating model: h9wlrd8f/checkpoint-104\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping h9wlrd8f/checkpoint-104\n",
      "\n",
      "🔍 Evaluating model: h9wlrd8f/checkpoint-572\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.90 for h9wlrd8f/checkpoint-572\n",
      "📌 Test Results: {'eval_mcc_metric': 0.7329660144890023, 'eval_loss': 0.07311765104532242, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8762886597938144, 'eval_AUC-ROC': 0.9604946753692889, 'eval_Precision': 0.8561151079136691, 'eval_Recall': 0.967479674796748, 'eval_F1-score': 0.9083969465648855, 'eval_runtime': 0.1663, 'eval_samples_per_second': 1166.22, 'eval_steps_per_second': 78.149}\n",
      "\n",
      "🔍 Evaluating model: h9wlrd8f/checkpoint-208\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping h9wlrd8f/checkpoint-208\n",
      "\n",
      "🔍 Evaluating model: h9wlrd8f/checkpoint-312\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping h9wlrd8f/checkpoint-312\n",
      "\n",
      "🔍 Evaluating model: h9wlrd8f/checkpoint-364\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping h9wlrd8f/checkpoint-364\n",
      "\n",
      "🔍 Evaluating model: h9wlrd8f/checkpoint-416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping h9wlrd8f/checkpoint-416\n",
      "\n",
      "🔍 Evaluating model: h9wlrd8f/checkpoint-260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping h9wlrd8f/checkpoint-260\n",
      "\n",
      "🔍 Evaluating model: h9wlrd8f/checkpoint-156\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping h9wlrd8f/checkpoint-156\n",
      "\n",
      "🔍 Evaluating model: h9wlrd8f/checkpoint-520\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.90 for h9wlrd8f/checkpoint-520\n",
      "📌 Test Results: {'eval_mcc_metric': 0.7329660144890023, 'eval_loss': 0.07192401587963104, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8762886597938144, 'eval_AUC-ROC': 0.9607236917439597, 'eval_Precision': 0.8561151079136691, 'eval_Recall': 0.967479674796748, 'eval_F1-score': 0.9083969465648855, 'eval_runtime': 0.1653, 'eval_samples_per_second': 1173.95, 'eval_steps_per_second': 78.667}\n",
      "\n",
      "🔍 Evaluating model: m9mf2oqr/checkpoint-468\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping m9mf2oqr/checkpoint-468\n",
      "\n",
      "🔍 Evaluating model: m9mf2oqr/checkpoint-572\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping m9mf2oqr/checkpoint-572\n",
      "\n",
      "🔍 Evaluating model: m9mf2oqr/checkpoint-676\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping m9mf2oqr/checkpoint-676\n",
      "\n",
      "🔍 Evaluating model: m9mf2oqr/checkpoint-208\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping m9mf2oqr/checkpoint-208\n",
      "\n",
      "🔍 Evaluating model: m9mf2oqr/checkpoint-624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping m9mf2oqr/checkpoint-624\n",
      "\n",
      "🔍 Evaluating model: m9mf2oqr/checkpoint-312\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping m9mf2oqr/checkpoint-312\n",
      "\n",
      "🔍 Evaluating model: m9mf2oqr/checkpoint-364\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping m9mf2oqr/checkpoint-364\n",
      "\n",
      "🔍 Evaluating model: m9mf2oqr/checkpoint-416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping m9mf2oqr/checkpoint-416\n",
      "\n",
      "🔍 Evaluating model: m9mf2oqr/checkpoint-260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping m9mf2oqr/checkpoint-260\n",
      "\n",
      "🔍 Evaluating model: m9mf2oqr/checkpoint-520\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping m9mf2oqr/checkpoint-520\n",
      "\n",
      "🔍 Evaluating model: i1ivpbk3/checkpoint-468\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping i1ivpbk3/checkpoint-468\n",
      "\n",
      "🔍 Evaluating model: i1ivpbk3/checkpoint-572\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping i1ivpbk3/checkpoint-572\n",
      "\n",
      "🔍 Evaluating model: i1ivpbk3/checkpoint-208\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping i1ivpbk3/checkpoint-208\n",
      "\n",
      "🔍 Evaluating model: i1ivpbk3/checkpoint-624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AUC_ROC > 0.90 for i1ivpbk3/checkpoint-624\n",
      "📌 Test Results: {'eval_mcc_metric': 0.6967618094157277, 'eval_loss': 0.07300040870904922, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8608247422680413, 'eval_AUC-ROC': 0.960265658994618, 'eval_Precision': 0.8478260869565217, 'eval_Recall': 0.9512195121951219, 'eval_F1-score': 0.896551724137931, 'eval_runtime': 0.1656, 'eval_samples_per_second': 1171.73, 'eval_steps_per_second': 78.518}\n",
      "\n",
      "🔍 Evaluating model: i1ivpbk3/checkpoint-312\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping i1ivpbk3/checkpoint-312\n",
      "\n",
      "🔍 Evaluating model: i1ivpbk3/checkpoint-364\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping i1ivpbk3/checkpoint-364\n",
      "\n",
      "🔍 Evaluating model: i1ivpbk3/checkpoint-416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping i1ivpbk3/checkpoint-416\n",
      "\n",
      "🔍 Evaluating model: i1ivpbk3/checkpoint-260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping i1ivpbk3/checkpoint-260\n",
      "\n",
      "🔍 Evaluating model: i1ivpbk3/checkpoint-156\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping i1ivpbk3/checkpoint-156\n",
      "\n",
      "🔍 Evaluating model: i1ivpbk3/checkpoint-520\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping i1ivpbk3/checkpoint-520\n",
      "\n",
      "🔍 Evaluating model: kwtwjebf/checkpoint-468\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kwtwjebf/checkpoint-468\n",
      "\n",
      "🔍 Evaluating model: kwtwjebf/checkpoint-728\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kwtwjebf/checkpoint-728\n",
      "\n",
      "🔍 Evaluating model: kwtwjebf/checkpoint-572\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kwtwjebf/checkpoint-572\n",
      "\n",
      "🔍 Evaluating model: kwtwjebf/checkpoint-676\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kwtwjebf/checkpoint-676\n",
      "\n",
      "🔍 Evaluating model: kwtwjebf/checkpoint-624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kwtwjebf/checkpoint-624\n",
      "\n",
      "🔍 Evaluating model: kwtwjebf/checkpoint-312\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kwtwjebf/checkpoint-312\n",
      "\n",
      "🔍 Evaluating model: kwtwjebf/checkpoint-364\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kwtwjebf/checkpoint-364\n",
      "\n",
      "🔍 Evaluating model: kwtwjebf/checkpoint-416\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kwtwjebf/checkpoint-416\n",
      "\n",
      "🔍 Evaluating model: kwtwjebf/checkpoint-260\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/qnap_home/raghvendra2/micromamba/envs/Molformer/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.classifier.original_module.dense.lora_A.default.weight', 'base_model.model.classifier.original_module.dense.lora_B.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_A.default.weight', 'base_model.model.classifier.original_module.out_proj.lora_B.default.weight', 'base_model.model.classifier.dense.lora_A.default.weight', 'base_model.model.classifier.dense.lora_B.default.weight', 'base_model.model.classifier.out_proj.lora_A.default.weight', 'base_model.model.classifier.out_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "/tmp/ipykernel_1816725/4183930858.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `FocalLossTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = FocalLossTrainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kwtwjebf/checkpoint-260\n",
      "\n",
      "🔍 Evaluating model: kwtwjebf/checkpoint-520\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Skipping kwtwjebf/checkpoint-520\n"
     ]
    }
   ],
   "source": [
    "# List all checkpoints inside models directory\n",
    "import os\n",
    "from peft import PeftModel\n",
    "\n",
    "models_dir = \"/clintox_models_10MTR\"\n",
    "\n",
    "def find_all_checkpoints(base_dir):\n",
    "    all_checkpoints = []\n",
    "    for folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for subfolder in os.listdir(folder_path):\n",
    "                subfolder_path = os.path.join(folder_path, subfolder)\n",
    "                if os.path.isdir(subfolder_path) and subfolder.startswith(\"checkpoint-\"):\n",
    "                    if os.path.exists(os.path.join(subfolder_path, \"adapter_config.json\")):\n",
    "                        all_checkpoints.append(subfolder_path)\n",
    "    return all_checkpoints\n",
    "\n",
    "valid_checkpoints = find_all_checkpoints(models_dir)\n",
    "print(\" Valid nested checkpoints found:\", valid_checkpoints)\n",
    "\n",
    "for checkpoint_path in valid_checkpoints:\n",
    "    checkpoint_name = os.path.basename(checkpoint_path)\n",
    "    parent_folder = os.path.basename(os.path.dirname(checkpoint_path))\n",
    "\n",
    "    print(f\"\\n Evaluating model: {parent_folder}/{checkpoint_name}\")\n",
    "\n",
    "    adapter_model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    adapter_model.eval()\n",
    "\n",
    "    trainer = FocalLossTrainer(\n",
    "        model=adapter_model,\n",
    "        args=eval_args,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "\n",
    "    test_results = trainer.evaluate()\n",
    "\n",
    "    auc_score = test_results[\"eval_AUC-ROC\"]\n",
    "    if auc_score > 0.96:\n",
    "        print(f\"✅ AUC_ROC > 0.90 for {parent_folder}/{checkpoint_name}\")\n",
    "        print(f\"📌 Test Results: {test_results}\")\n",
    "    else:\n",
    "        print(f\"❌ Skipping {parent_folder}/{checkpoint_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15acaf13",
   "metadata": {},
   "source": [
    "### Best results for 77M MTR: p4hh23r5/checkpoint-780  (Focal loss)\n",
    "\n",
    "trainable params: 395,938 || all params: 3,842,612 || trainable%: 10.3039\n",
    "\n",
    "{'eval_mcc_metric': 0.7080332487622075, 'eval_loss': 0.06555292755365372, 'eval_model_preparation_time': 0.0049, 'eval_Accuracy': 0.865979381443299, 'eval_AUC-ROC': 0.9654185274247108, 'eval_Precision': 0.8540145985401459, 'eval_Recall': 0.9512195121951219, 'eval_F1-score': 0.9, 'eval_runtime': 0.2057, 'eval_samples_per_second': 943.098, 'eval_steps_per_second': 63.197}\n",
    "\n",
    "### Best results for 77M MTR: 6mmywjr3/checkpoint-260( weighted loss)\n",
    "\n",
    "trainable params: 395,938 || all params: 3,842,612 || trainable%: 10.3039\n",
    "\n",
    "{'eval_mcc_metric': 0.7530924327044072, 'eval_loss': 0.4659155607223511, 'eval_model_preparation_time': 0.004, 'eval_Accuracy': 0.8865979381443299, 'eval_AUC-ROC': 0.9586625443719226, 'eval_Precision': 0.8796992481203008, 'eval_Recall': 0.9512195121951219, 'eval_F1-score': 0.9140625, 'eval_runtime': 0.1762, 'eval_samples_per_second': 1100.732, 'eval_steps_per_second': 73.76}\n",
    "\n",
    "\n",
    "### Best results for 77M MLM : 3inhdg73/checkpoint-988 (Focal Loss)\n",
    "\n",
    "trainable params: 643,266 || all params: 4,108,404 || trainable%: 15.6573\n",
    "\n",
    "📌 Test Results: {'eval_mcc_metric': 0.6713501414672036, 'eval_loss': 0.08056607097387314, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.845360824742268, 'eval_AUC-ROC': 0.9396541852742472, 'eval_Precision': 0.8907563025210085, 'eval_Recall': 0.8617886178861789, 'eval_F1-score': 0.8760330578512396, 'eval_runtime': 0.173, 'eval_samples_per_second': 1121.312, 'eval_steps_per_second': 75.139}\n",
    "\n",
    "### Best result for 10M MTR: qx65by2n/checkpoint-780\n",
    "\n",
    "trainable params: 643,266 || all params: 4,108,404 || trainable%: 15.6573\n",
    "\n",
    "📌 Test Results: {'eval_mcc_metric': 0.7080332487622075, 'eval_loss': 0.07051488757133484, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.865979381443299, 'eval_AUC-ROC': 0.9607236917439598, 'eval_Precision': 0.8540145985401459, 'eval_Recall': 0.9512195121951219, 'eval_F1-score': 0.9, 'eval_runtime': 0.1679, 'eval_samples_per_second': 1155.233, 'eval_steps_per_second': 77.412}\n",
    "\n",
    "### Best model for 10M MLM:  g0lp7aba/checkpoint-676\n",
    "\n",
    "trainable params: 1,137,922 || all params: 4,639,988 || trainable%: 24.5242\n",
    "\n",
    "📌 Test Results: {'eval_mcc_metric': 0.7438155398213232, 'eval_loss': 0.0648377314209938, 'eval_model_preparation_time': 0.0039, 'eval_Accuracy': 0.8814432989690721, 'eval_AUC-ROC': 0.9590060689339288, 'eval_Precision': 0.9032258064516129, 'eval_Recall': 0.9105691056910569, 'eval_F1-score': 0.9068825910931174, 'eval_runtime': 0.1688, 'eval_samples_per_second': 1149.189, 'eval_steps_per_second': 77.008}\n",
    "\n",
    "### Best Model for 5M MTR: l2kgceha/checkpoint-624\n",
    "\n",
    "trainable params: 643,266 || all params: 4,108,404 || trainable%: 15.6573\n",
    "\n",
    "\n",
    "📌 Test Results: {'eval_mcc_metric': 0.6967618094157277, 'eval_loss': 0.0720517635345459, 'eval_model_preparation_time': 0.0038, 'eval_Accuracy': 0.8608247422680413, 'eval_AUC-ROC': 0.9620977899919845, 'eval_Precision': 0.8478260869565217, 'eval_Recall': 0.9512195121951219, 'eval_F1-score': 0.896551724137931, 'eval_runtime': 0.1699, 'eval_samples_per_second': 1141.757, 'eval_steps_per_second': 76.51}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e997033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 395,938 || all params: 3,842,612 || trainable%: 10.3039\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"DeepChem/ChemBERTa-77M-MTR\",\n",
    "    num_labels=2,\n",
    "    problem_type=\"single_label_classification\",    \n",
    "    trust_remote_code=True,\n",
    "    \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\",trust_remote_code=True)\n",
    "\n",
    "# Apply LoRA\n",
    "peft_config = lora_config(16, 32, 0.1)\n",
    "lora_model = get_peft_model(base_model, peft_config)\n",
    "\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Molformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
